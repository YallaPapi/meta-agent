{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Initialize Python Project Structure",
        "description": "Set up the Python project with pyproject.toml, create the directory structure as specified in the PRD, and configure development dependencies.",
        "details": "Create the following structure:\n\n1. Create `pyproject.toml` with:\n   - Project metadata (name='metaagent', version='0.1.0')\n   - Python 3.10+ requirement\n   - Dependencies: click or typer, pyyaml, httpx, python-dotenv, jinja2\n   - Dev dependencies: pytest, pytest-cov, pytest-asyncio\n   - Entry point: metaagent = 'metaagent.cli:main'\n\n2. Create directory structure:\n   ```\n   src/metaagent/__init__.py\n   src/metaagent/cli.py (stub)\n   src/metaagent/orchestrator.py (stub)\n   src/metaagent/repomix.py (stub)\n   src/metaagent/prompts.py (stub)\n   src/metaagent/analysis.py (stub)\n   src/metaagent/plan_writer.py (stub)\n   src/metaagent/config.py (stub)\n   config/prompts.yaml (empty structure)\n   config/profiles.yaml (empty structure)\n   tests/__init__.py\n   tests/test_cli.py (stub)\n   tests/test_orchestrator.py (stub)\n   docs/\n   ```\n\n3. Update .gitignore for Python:\n   - __pycache__/, *.pyc, *.pyo\n   - .venv/, venv/, .env\n   - dist/, build/, *.egg-info/\n   - .pytest_cache/, .coverage\n\n4. Create README.md with basic project description and setup instructions.",
        "testStrategy": "Verify project structure exists with `ls -R`. Verify `uv pip install -e .` or `pip install -e .` succeeds. Verify `metaagent --help` runs without errors (will show help from stub CLI).",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Author modern pyproject.toml with src-layout and dependency groups",
            "description": "Create pyproject.toml file with project metadata, Python 3.10+ requirement, runtime and dev dependencies, and console_script entry point using modern standards.",
            "dependencies": [],
            "details": "Use [tool.uv] or [build-system] with hatchling; define [project] table with name='metaagent', version='0.1.0', requires-python='>=3.10'; dependencies=['click', 'pyyaml', 'httpx', 'python-dotenv', 'jinja2']; optional-dependencies.dev=['pytest', 'pytest-cov', 'pytest-asyncio']; [project.scripts] metaagent='metaagent.cli:main'; enable src-layout with packages=['src/metaagent'].",
            "status": "pending",
            "testStrategy": "Verify with `uv pip install -e .` or `pip install -e .` succeeds without errors; check `metaagent --help` displays CLI help from stub; validate TOML syntax and metadata with `uv project info`.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create src/, config/, tests/, and docs/ directory skeletons",
            "description": "Set up complete directory structure with all specified empty stub files as per PRD requirements.",
            "dependencies": [
              1
            ],
            "details": "Create directories: src/metaagent/, config/, tests/, docs/; add __init__.py files to src/metaagent/ and tests/; create stub Python files: cli.py, orchestrator.py, repomix.py, prompts.py, analysis.py, plan_writer.py, config.py; create empty YAML files: config/prompts.yaml, config/profiles.yaml.",
            "status": "pending",
            "testStrategy": "Run `ls -R src config tests docs` to verify exact structure matches spec; ensure all stub files exist and contain pass or basic if __name__ == '__main__' guards.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Set up .gitignore for Python projects and common tooling",
            "description": "Create comprehensive .gitignore covering Python caches, virtualenvs, build artifacts, and testing caches.",
            "dependencies": [
              1
            ],
            "details": "Include patterns: __pycache__/, *.pyc, *.pyo, *.pyd; .venv/, venv/, ENV/, env/; .env, .env.*; dist/, build/, *.egg-info/, *.whl; .pytest_cache/, .coverage, .coverage.*; recommend adding .uv/, uv.lock if using uv tooling.",
            "status": "pending",
            "testStrategy": "Verify git ignores files by creating test files matching patterns and running `git status --ignored`; ensure common ignore files like .env are properly excluded.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create minimal README.md with install, usage, and dev notes",
            "description": "Write basic project README including project description, installation instructions, basic usage, and development setup.",
            "dependencies": [
              1,
              2
            ],
            "details": "Include sections: # MetaAgent (AI-powered repo refinement); ## Installation (`uv venv; uv pip install -e .` or pip); ## Usage (`metaagent --help`); ## Development (pytest, pre-commit if added); ## Structure overview; link to PRD/docs.",
            "status": "pending",
            "testStrategy": "Verify README renders correctly in GitHub/Markdown viewer; check all commands in install/usage sections execute successfully after project setup.",
            "parentId": "undefined"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down Task 1 (Initialize Python Project Structure) into 4 concrete subtasks covering: (1) authoring a modern pyproject.toml with src-layout, dependency groups for dev, and console_script entry point; (2) creating the specified src/, config/, tests/, and docs/ directory/files skeletons; (3) setting up .gitignore following Python and common tooling patterns; (4) creating a minimal but correct README.md with install, usage, and development notes. For each subtask, specify clear acceptance criteria and any tooling conventions (e.g., uv, pytest).",
        "updatedAt": "2025-12-14T01:59:42.237Z"
      },
      {
        "id": "2",
        "title": "Implement Configuration Management",
        "description": "Create the config.py module to handle environment variables, application settings, and provide a centralized configuration interface.",
        "details": "Implement `src/metaagent/config.py`:\n\n```python\nimport os\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\n\n@dataclass\nclass Config:\n    perplexity_api_key: str | None\n    anthropic_api_key: str | None\n    log_level: str\n    timeout: int\n    max_tokens: int\n    repo_path: Path\n    prd_path: Path\n    config_dir: Path\n    output_dir: Path\n\n    @classmethod\n    def from_env(cls, repo_path: Path | None = None) -> 'Config':\n        load_dotenv()\n        repo = repo_path or Path.cwd()\n        return cls(\n            perplexity_api_key=os.getenv('PERPLEXITY_API_KEY'),\n            anthropic_api_key=os.getenv('ANTHROPIC_API_KEY'),\n            log_level=os.getenv('METAAGENT_LOG_LEVEL', 'INFO'),\n            timeout=int(os.getenv('METAAGENT_TIMEOUT', '120')),\n            max_tokens=int(os.getenv('METAAGENT_MAX_TOKENS', '100000')),\n            repo_path=repo,\n            prd_path=repo / 'docs' / 'prd.md',\n            config_dir=Path(__file__).parent.parent.parent / 'config',\n            output_dir=repo / 'docs'\n        )\n\n    def validate(self) -> list[str]:\n        \"\"\"Return list of validation errors, empty if valid.\"\"\"\n        errors = []\n        if not self.repo_path.exists():\n            errors.append(f'Repository path does not exist: {self.repo_path}')\n        if not self.config_dir.exists():\n            errors.append(f'Config directory does not exist: {self.config_dir}')\n        return errors\n```\n\nKey features:\n- Load from environment variables with defaults\n- Dataclass for type safety and immutability\n- Path resolution for repo, PRD, config, and output directories\n- Validation method for required paths\n- Support for both installed package and development mode paths",
        "testStrategy": "Unit tests in `tests/test_config.py`:\n1. Test Config.from_env() with mocked environment variables\n2. Test default values when env vars not set\n3. Test validate() returns errors for missing paths\n4. Test validate() returns empty list for valid paths\n5. Test path resolution works correctly",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Config dataclass and from_env() factory with dotenv loading and robust defaults",
            "description": "Create the Config dataclass and the from_env() classmethod to load configuration from environment variables using python-dotenv, including sensible defaults and safe type conversion.",
            "dependencies": [],
            "details": "- Define the Config dataclass exactly as specified (API keys, log_level, timeout, max_tokens, repo_path, prd_path, config_dir, output_dir) with appropriate type hints.\n- Implement from_env() to call load_dotenv() first so that .env files are respected.\n- Accept an optional repo_path argument; if not provided, default to Path.cwd().\n- Read PERPLEXITY_API_KEY and ANTHROPIC_API_KEY from the environment; allow them to be missing (set to None) without raising.\n- Read METAAGENT_LOG_LEVEL, METAAGENT_TIMEOUT, METAAGENT_MAX_TOKENS with robust defaulting and type conversion (e.g., fall back to safe defaults if env values are malformed integers).\n- Compute prd_path as repo_path / 'docs' / 'prd.md'.\n- Compute config_dir relative to the installed package layout (e.g., Path(__file__).parent.parent.parent / 'config') so it works in both editable and installed modes.\n- Compute output_dir as repo_path / 'docs', creating only paths in later code, not here.\n- Edge cases to consider and later test: missing env vars (ensure defaults/None used), invalid integer values for timeout/max_tokens (decide on fallback behavior, e.g., catch ValueError and revert to defaults rather than crash), unusual CWD when repo_path is not passed.\n- Do not perform filesystem existence checks here; leave that to validate().",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement validate() with repo/config/output/PRD path checks and dev vs installed behavior",
            "description": "Implement the validate() method to check existence and correctness of key paths (repo_path, config_dir, output_dir, prd_path), handling differences between development and installed package modes.",
            "dependencies": [
              1
            ],
            "details": "- Extend validate() to return a list of human-readable error strings describing configuration problems; keep empty list on success.\n- Check that repo_path exists and is a directory; if not, add an error.\n- Check that config_dir exists and is a directory; if not, add an error describing that configuration assets are missing.\n- Check that output_dir exists or, if your design requires it, that its parent exists; decide whether absence of output_dir is an error (e.g., might be created later) or only a warning (still represented as an error string if you choose to enforce creation upfront).\n- Check that prd_path exists and is a file; add an error if the PRD is missing so downstream components (orchestrator, plan writer) can fail early.\n- For dev vs installed modes, base behavior only on the resolved config_dir path: for example, treat a config_dir located inside the source tree (e.g., under src/metaagent/../config) as dev mode, and a site-packages-like path as installed mode; ensure validate() behaves consistently in both, without hard-coding environment-specific assumptions.\n- Ensure validate() never raises; all issues must be represented as error messages so callers can decide how to handle them.\n- Edge cases and expectations: non-existent repo_path passed explicitly; repo_path that exists but lacks docs/ or prd.md; config_dir missing because package assets not installed; output_dir pointing to a file instead of a directory; paths that are symlinks (still treated as existing).",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Wire Config into a minimal caller (stub CLI or script) to exercise usage patterns",
            "description": "Create a small entrypoint (e.g., a stub CLI command or script) that constructs Config.from_env(), runs validate(), and reports configuration status, to verify real-world usage patterns.",
            "dependencies": [
              1,
              2
            ],
            "details": "- Implement a minimal callable entrypoint, such as a function main() or a tiny Typer/Rich-based CLI stub under src/metaagent, that imports and uses Config.\n- In the entrypoint, call Config.from_env() with an optional repo_path argument (e.g., from CLI flag or default Path.cwd()).\n- Immediately call config.validate() and, if errors are returned, print them in a user-friendly way and exit with a non-zero status code.\n- On success (no validation errors), print or log key configuration values (e.g., repo_path, prd_path, config_dir, output_dir, timeout, log_level) without exposing sensitive API keys.\n- Ensure the entrypoint demonstrates how higher-level components (orchestrator, CLI) will interact with Config, including typical error-handling flow.\n- Edge cases and expectations: behavior when repo_path points to a non-existent directory (entrypoint should show validation errors, not crash); behavior with missing PRD or config_dir (clear error output); running from different working directories to confirm repo_path resolution is intuitive.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create focused pytest unit tests for Config using monkeypatch and tmp_path",
            "description": "Add pytest unit tests covering environment-variable handling, default values, path resolution, and validate() behavior across various filesystem scenarios using monkeypatch and tmp_path.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "- Implement tests in tests/test_config.py structured around the provided test strategy.\n- Use monkeypatch to control os.environ for Config.from_env() tests, including setting PERPLEXITY_API_KEY, ANTHROPIC_API_KEY, METAAGENT_LOG_LEVEL, METAAGENT_TIMEOUT, and METAAGENT_MAX_TOKENS, and clearing them to test defaults.\n- Test edge cases of env parsing: missing API keys (must yield None), missing optional settings (must use defaults), and invalid integer strings for timeout/max_tokens (must fall back safely instead of raising).\n- Use tmp_path to create ephemeral directory structures representing a fake repo with docs/, prd.md, and optional config/ directory, passing these paths into Config instances for validation tests.\n- Write tests where repo_path does not exist, config_dir does not exist, output_dir points to a file instead of a directory, and prd.md is missing; assert that validate() returns clear error messages for each condition.\n- Add a test to confirm that path resolution from from_env() (repo_path, prd_path, config_dir, output_dir) matches expectations when run from different working directories or with an explicit repo_path.\n- Optionally add an integration-style test that uses the minimal caller entrypoint to confirm end-to-end behavior (construct, validate, and report), asserting correct exit codes and output using pytest’s capsys or CliRunner if using Typer.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down Task 2 (Implement Configuration Management) into 4 subtasks: (1) implement Config dataclass and from_env factory using python-dotenv and robust type conversion with defaults; (2) implement validate() including repo/config/output/PRD path checks and behavior in dev vs installed modes; (3) wire Config into a minimal caller (e.g., stub CLI) to verify usage patterns; (4) create focused pytest unit tests using monkeypatch/tmp_path for environment and filesystem scenarios. For each subtask, call out edge cases (missing env vars, non-existent paths) and test expectations.",
        "updatedAt": "2025-12-14T01:59:48.825Z"
      },
      {
        "id": "3",
        "title": "Implement Prompt and Profile Loading",
        "description": "Create the prompts.py module to load and render prompt templates from YAML configuration, and load profile definitions that map stages to prompts.",
        "details": "Implement `src/metaagent/prompts.py`:\n\n```python\nfrom pathlib import Path\nfrom dataclasses import dataclass\nimport yaml\nfrom jinja2 import Template\n\n@dataclass\nclass Prompt:\n    id: str\n    goal: str\n    stage: str\n    template: str\n\n    def render(self, prd: str, code_context: str, history: str, current_stage: str) -> str:\n        \"\"\"Render template with provided variables.\"\"\"\n        tmpl = Template(self.template)\n        return tmpl.render(\n            prd=prd,\n            code_context=code_context,\n            history=history,\n            current_stage=current_stage\n        )\n\n@dataclass\nclass Profile:\n    name: str\n    description: str\n    stages: list[str]\n\nclass PromptLibrary:\n    def __init__(self, config_dir: Path):\n        self.config_dir = config_dir\n        self._prompts: dict[str, Prompt] = {}\n        self._profiles: dict[str, Profile] = {}\n        self._load()\n\n    def _load(self):\n        # Load prompts.yaml\n        prompts_file = self.config_dir / 'prompts.yaml'\n        if prompts_file.exists():\n            with open(prompts_file) as f:\n                data = yaml.safe_load(f)\n            for pid, pdata in data.get('prompts', {}).items():\n                self._prompts[pid] = Prompt(\n                    id=pdata['id'],\n                    goal=pdata['goal'],\n                    stage=pdata['stage'],\n                    template=pdata['template']\n                )\n\n        # Load profiles.yaml\n        profiles_file = self.config_dir / 'profiles.yaml'\n        if profiles_file.exists():\n            with open(profiles_file) as f:\n                data = yaml.safe_load(f)\n            for pname, pdata in data.get('profiles', {}).items():\n                self._profiles[pname] = Profile(\n                    name=pdata['name'],\n                    description=pdata['description'],\n                    stages=pdata['stages']\n                )\n\n    def get_prompt(self, prompt_id: str) -> Prompt | None:\n        return self._prompts.get(prompt_id)\n\n    def get_profile(self, profile_name: str) -> Profile | None:\n        return self._profiles.get(profile_name)\n\n    def list_profiles(self) -> list[str]:\n        return list(self._profiles.keys())\n\n    def get_prompts_for_profile(self, profile_name: str) -> list[Prompt]:\n        profile = self.get_profile(profile_name)\n        if not profile:\n            return []\n        return [self._prompts[s] for s in profile.stages if s in self._prompts]\n```\n\nPopulate `config/prompts.yaml` and `config/profiles.yaml` with the exact content from PRD Section 7.1 and 7.2.",
        "testStrategy": "Unit tests in `tests/test_prompts.py`:\n1. Test loading prompts from YAML file\n2. Test loading profiles from YAML file\n3. Test Prompt.render() with template variables\n4. Test get_prompts_for_profile() returns correct ordered list\n5. Test handling of missing files gracefully\n6. Test list_profiles() returns all profile names",
        "priority": "high",
        "dependencies": [
          "1",
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Prompt and Profile dataclasses and PromptLibrary public interface",
            "description": "Create the Prompt and Profile dataclasses and sketch the PromptLibrary API surface that will manage loading and accessing prompts and profiles.",
            "dependencies": [
              3
            ],
            "details": "Implement Prompt and Profile as @dataclass structures with the fields specified in the PRD (id, goal, stage, template for Prompt; name, description, stages for Profile). Define the PromptLibrary __init__(config_dir: Path) signature and internal dictionaries for prompts and profiles. Stub out _load(), get_prompt(), get_profile(), list_profiles(), and get_prompts_for_profile() with type hints and docstrings but no logic yet, ensuring the interface is stable for later use by orchestrator and CLI.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement YAML loading with safe_load, validation, and missing-file behavior",
            "description": "Add YAML loading logic to PromptLibrary that reads prompts.yaml and profiles.yaml, using yaml.safe_load with graceful handling of missing files and malformed content.",
            "dependencies": [
              1
            ],
            "details": "In PromptLibrary._load(), implement reading config_dir / 'prompts.yaml' and config_dir / 'profiles.yaml' using context managers and yaml.safe_load. If a file does not exist, skip loading without raising, so construction of PromptLibrary has minimal side effects and is deterministic. Add minimal schema validation (e.g., ensure top-level keys 'prompts' and 'profiles' are dicts, required fields exist) and either log or raise clear exceptions for invalid structures while keeping side effects confined to object state. Avoid any global state; all loaded data should be stored only in self._prompts and self._profiles so tests can inject a temporary config_dir with fixture YAML files.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Prompt.render() using Jinja2 with fixed variable contract and error guardrails",
            "description": "Complete the Prompt.render() method so it renders templates with a fixed set of variables via Jinja2, handling template errors safely.",
            "dependencies": [
              1,
              2
            ],
            "details": "Use jinja2.Template to compile self.template and render it with a fixed context including prd, code_context, history, and current_stage. Decide on behavior for template syntax or rendering errors (e.g., catch TemplateError and either re-raise a custom exception or return a fallback string) to prevent hard crashes in callers like the orchestrator. Keep rendering pure and side-effect free: no file I/O or logging inside render, so unit tests can call it deterministically. Document the variable contract in the render docstring so YAML templates in prompts.yaml can rely on a stable set of fields.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement profile and prompt lookup helpers with ordering guarantees",
            "description": "Fill in PromptLibrary helper methods for querying prompts and profiles, ensuring get_prompts_for_profile preserves the profile-defined stage order and handles missing mappings robustly.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement get_prompt(prompt_id) and get_profile(profile_name) as simple dictionary lookups returning None when not found. Implement list_profiles() to return profile names in deterministic order (e.g., sorted or insertion order, documented explicitly). Implement get_prompts_for_profile(profile_name) to resolve the profile, then map profile.stages entries to Prompt instances, skipping unknown prompt IDs safely while preserving the original stage order. Keep logic purely in-memory so tests can construct PromptLibrary against temporary YAML directories without global side effects.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write unit tests for YAML loading, rendering, helpers, and error cases",
            "description": "Create tests in tests/test_prompts.py that cover YAML fixtures, prompt rendering, helper behaviors, missing files, and invalid data, while keeping loading side effects isolated.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Add pytest-based tests that use temporary directories to host prompts.yaml and profiles.yaml fixtures derived from PRD sections 7.1 and 7.2. Test that PromptLibrary loads valid YAML into correct Prompt and Profile objects, that Prompt.render() correctly interpolates all supported variables, and that get_prompts_for_profile() returns prompts in the expected stage order. Add tests for behavior when prompts.yaml and/or profiles.yaml are missing (PromptLibrary still constructs and methods return empty/None appropriately) and when YAML contains invalid structures or missing required fields. Ensure each test constructs its own PromptLibrary instance pointing at an isolated temp config_dir so there are no cross-test side effects or reliance on global filesystem state.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down Task 3 (Implement Prompt and Profile Loading) into 5 subtasks: (1) define Prompt and Profile dataclasses plus PromptLibrary interface; (2) implement YAML loading with safe_load, error handling, and missing-file behavior; (3) implement Prompt.render() using Jinja2 with a fixed variable contract and guardrails for template errors; (4) implement profile/query helpers (get_prompt, get_profile, list_profiles, get_prompts_for_profile) with ordering guarantees; (5) write unit tests covering YAML fixtures, rendering, missing files, and invalid data cases. Explicitly note how to keep loading side effects contained for testability.",
        "updatedAt": "2025-12-14T01:59:55.377Z"
      },
      {
        "id": "4",
        "title": "Implement Repomix Integration",
        "description": "Create the repomix.py module to run Repomix CLI as a subprocess and return the packed codebase content.",
        "details": "Implement `src/metaagent/repomix.py`:\n\n```python\nimport subprocess\nimport tempfile\nfrom pathlib import Path\nfrom dataclasses import dataclass\n\n@dataclass\nclass RepomixResult:\n    success: bool\n    content: str\n    error: str | None = None\n\nclass RepomixRunner:\n    def __init__(self, timeout: int = 120):\n        self.timeout = timeout\n\n    def pack(self, repo_path: Path) -> RepomixResult:\n        \"\"\"\n        Run Repomix on the given repository and return packed content.\n        \n        Args:\n            repo_path: Path to the repository to pack\n            \n        Returns:\n            RepomixResult with success status and content or error\n        \"\"\"\n        with tempfile.NamedTemporaryFile(suffix='.txt', delete=False) as tmp:\n            output_file = Path(tmp.name)\n        \n        try:\n            result = subprocess.run(\n                ['npx', '-y', 'repomix', '--output', str(output_file)],\n                cwd=str(repo_path),\n                capture_output=True,\n                text=True,\n                timeout=self.timeout\n            )\n            \n            if result.returncode != 0:\n                return RepomixResult(\n                    success=False,\n                    content='',\n                    error=f'Repomix failed: {result.stderr}'\n                )\n            \n            content = output_file.read_text(encoding='utf-8')\n            return RepomixResult(success=True, content=content)\n            \n        except subprocess.TimeoutExpired:\n            return RepomixResult(\n                success=False,\n                content='',\n                error=f'Repomix timed out after {self.timeout}s'\n            )\n        except FileNotFoundError:\n            return RepomixResult(\n                success=False,\n                content='',\n                error='npx/repomix not found. Ensure Node.js is installed.'\n            )\n        finally:\n            output_file.unlink(missing_ok=True)\n\n    def truncate_content(self, content: str, max_tokens: int) -> str:\n        \"\"\"Truncate content to fit within token budget (rough char estimate).\"\"\"\n        # Rough estimate: 1 token ≈ 4 characters\n        max_chars = max_tokens * 4\n        if len(content) <= max_chars:\n            return content\n        return content[:max_chars] + '\\n\\n[Content truncated to fit token limit]'\n```\n\nKey considerations:\n- Use npx to run repomix without global installation\n- Capture output to temp file and read content\n- Handle timeout errors gracefully\n- Handle missing npx/node gracefully\n- Provide truncation utility for context budget",
        "testStrategy": "Unit tests in `tests/test_repomix.py`:\n1. Test successful pack with mock subprocess (mock subprocess.run)\n2. Test timeout handling\n3. Test error handling for failed subprocess\n4. Test FileNotFoundError handling\n5. Test truncate_content() with content under limit\n6. Test truncate_content() with content over limit\n7. Integration test with real Repomix on small test repo (optional, mark as slow)",
        "priority": "high",
        "dependencies": [
          "1",
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design RepomixResult dataclass and RepomixRunner API",
            "description": "Define the data class for results and the main class structure with init and method signatures.",
            "dependencies": [],
            "details": "Create RepomixResult dataclass with success, content, error fields. Define RepomixRunner __init__ with timeout param and pack() method signature accepting Path returning RepomixResult. Add truncate_content signature.",
            "status": "pending",
            "testStrategy": "Verify dataclass field types and defaults. Test __init__ sets timeout correctly. Test method signatures via inspect.signature.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement pack() method core logic with subprocess and temp file",
            "description": "Implement the main subprocess.run call using npx repomix with temp file output and cwd set to repo_path.",
            "dependencies": [
              1
            ],
            "details": "Use tempfile.NamedTemporaryFile for output. Run subprocess.run(['npx', '-y', 'repomix', '--output', str(output_file)], cwd=str(repo_path), capture_output=True, text=True, timeout=self.timeout). Read content on success. Ensure cleanup in finally block with unlink(missing_ok=True).",
            "status": "pending",
            "testStrategy": "Mock subprocess.run returning success (returncode=0). Verify temp file path passed correctly to --output. Verify cwd set to repo_path. Verify content read from temp file after success.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add comprehensive error handling branches",
            "description": "Handle subprocess failure, timeout, and missing npx/Node.js cases with descriptive error messages in RepomixResult.",
            "dependencies": [
              2
            ],
            "details": "Check result.returncode != 0 and return error with result.stderr. Catch subprocess.TimeoutExpired with timeout message. Catch FileNotFoundError with 'npx/repomix not found' message ensuring Node.js check.",
            "status": "pending",
            "testStrategy": "Mock subprocess.run with returncode=1 and stderr. Mock TimeoutExpired exception. Mock FileNotFoundError. Verify each returns RepomixResult(success=False, error=expected_message).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement truncate_content() with token estimation",
            "description": "Add utility method to truncate content based on max_tokens using 4 chars per token heuristic.",
            "dependencies": [
              1
            ],
            "details": "Calculate max_chars = max_tokens * 4. Return content unchanged if under limit. Otherwise truncate and append '[Content truncated to fit token limit]' suffix. Document the 1 token ≈ 4 chars approximation.",
            "status": "pending",
            "testStrategy": "Test content under limit returns unchanged. Test exact boundary (max_chars). Test over limit truncates correctly with suffix. Test empty string and max_tokens=0 edge cases.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write comprehensive unit tests with mocks for CI stability",
            "description": "Create test_repomix.py with pytest monkeypatch covering all success/failure paths without requiring Node.js.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Use monkeypatch to mock subprocess.run for success/failure/timeout cases. Mock Path.read_text() and tempfile.NamedTemporaryFile. Test truncate_content boundaries. Ensure tests pass in CI without Node/repomix installed by mocking all external calls.",
            "status": "pending",
            "testStrategy": "Coverage: success pack, failed subprocess, timeout, FileNotFoundError, truncate under/over limit. Verify temp file cleanup called. Use pytest tmp_path for filesystem isolation. Mock Path.unlink().",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down Task 4 (Implement Repomix Integration) into 5 subtasks: (1) design RepomixResult dataclass and RepomixRunner API; (2) implement pack() using subprocess.run with cwd, temp file handling, and robust cleanup in finally; (3) implement error handling branches for non-zero exit, TimeoutExpired, and FileNotFoundError with clear messages; (4) implement truncate_content() with a well-documented token-to-char heuristic and tests around boundaries; (5) write unit tests using monkeypatch to mock subprocess.run and filesystem interactions, covering success, failure, timeout, and missing binary cases. Include notes on making this stable in CI where Node/repomix may not be installed.",
        "updatedAt": "2025-12-14T02:33:18.775Z"
      },
      {
        "id": "5",
        "title": "Implement Analysis Engine with Mock Mode",
        "description": "Create the analysis.py module to wrap LLM API calls for analysis. Include a mock mode for testing and a clear extension point for future API integration.",
        "details": "Implement `src/metaagent/analysis.py`:\n\n```python\nimport json\nimport httpx\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Protocol\n\n@dataclass\nclass AnalysisResult:\n    summary: str\n    recommendations: list[str]\n    tasks: list[dict]  # Each task: {title, description, priority, files}\n\nclass AnalysisEngine(Protocol):\n    def analyze(self, prompt: str) -> AnalysisResult:\n        \"\"\"Run analysis with rendered prompt and return structured result.\"\"\"\n        ...\n\nclass MockAnalysisEngine:\n    \"\"\"Mock engine for testing without API calls.\"\"\"\n    \n    def analyze(self, prompt: str) -> AnalysisResult:\n        return AnalysisResult(\n            summary='Mock analysis completed. This is a placeholder result.',\n            recommendations=[\n                'Implement core functionality first',\n                'Add comprehensive error handling',\n                'Write unit tests for critical paths'\n            ],\n            tasks=[\n                {'title': 'Sample Task 1', 'description': 'Placeholder task', 'priority': 'high', 'files': []},\n                {'title': 'Sample Task 2', 'description': 'Another placeholder', 'priority': 'medium', 'files': []}\n            ]\n        )\n\nclass PerplexityAnalysisEngine:\n    \"\"\"Perplexity API integration for analysis.\"\"\"\n    \n    def __init__(self, api_key: str, timeout: int = 120):\n        self.api_key = api_key\n        self.timeout = timeout\n        self.base_url = 'https://api.perplexity.ai'\n    \n    def analyze(self, prompt: str) -> AnalysisResult:\n        \"\"\"Extension point for LLM analysis calls.\"\"\"\n        headers = {\n            'Authorization': f'Bearer {self.api_key}',\n            'Content-Type': 'application/json'\n        }\n        \n        payload = {\n            'model': 'llama-3.1-sonar-large-128k-online',\n            'messages': [\n                {'role': 'system', 'content': 'You are a code analysis expert. Always respond with valid JSON containing keys: summary, recommendations, tasks.'},\n                {'role': 'user', 'content': prompt}\n            ]\n        }\n        \n        with httpx.Client(timeout=self.timeout) as client:\n            response = client.post(\n                f'{self.base_url}/chat/completions',\n                headers=headers,\n                json=payload\n            )\n            response.raise_for_status()\n            data = response.json()\n        \n        content = data['choices'][0]['message']['content']\n        return self._parse_response(content)\n    \n    def _parse_response(self, content: str) -> AnalysisResult:\n        \"\"\"Parse JSON response from LLM.\"\"\"\n        try:\n            # Try to extract JSON from response\n            parsed = json.loads(content)\n        except json.JSONDecodeError:\n            # Fallback: try to find JSON block\n            import re\n            match = re.search(r'```json\\s*(.+?)\\s*```', content, re.DOTALL)\n            if match:\n                parsed = json.loads(match.group(1))\n            else:\n                return AnalysisResult(\n                    summary=content[:500],\n                    recommendations=[],\n                    tasks=[]\n                )\n        \n        return AnalysisResult(\n            summary=parsed.get('summary', ''),\n            recommendations=parsed.get('recommendations', []),\n            tasks=parsed.get('tasks', [])\n        )\n\ndef create_analysis_engine(api_key: str | None, use_mock: bool = False) -> AnalysisEngine:\n    \"\"\"Factory function to create appropriate analysis engine.\"\"\"\n    if use_mock or not api_key:\n        return MockAnalysisEngine()\n    return PerplexityAnalysisEngine(api_key)\n```",
        "testStrategy": "Unit tests in `tests/test_analysis.py`:\n1. Test MockAnalysisEngine.analyze() returns valid AnalysisResult\n2. Test create_analysis_engine() returns mock when use_mock=True\n3. Test create_analysis_engine() returns mock when api_key is None\n4. Test _parse_response() with valid JSON\n5. Test _parse_response() with JSON in code block\n6. Test _parse_response() with invalid JSON (fallback)\n7. Integration test with real Perplexity API (optional, mark as integration, skip in CI)",
        "priority": "high",
        "dependencies": [
          "1",
          "2",
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define AnalysisResult dataclass and AnalysisEngine protocol in analysis.py",
            "description": "Create the AnalysisResult dataclass and AnalysisEngine protocol to formalize structured analysis outputs and the engine interface.",
            "dependencies": [],
            "details": "Implement AnalysisResult with fields: summary: str, recommendations: list[str], tasks: list[dict] where each task dict includes at least title, description, priority, files. Define an AnalysisEngine Protocol with a single method analyze(self, prompt: str) -> AnalysisResult and a clear docstring describing its contract as the extension point for different LLM providers.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement MockAnalysisEngine with deterministic analyze() output",
            "description": "Create MockAnalysisEngine that implements AnalysisEngine and returns fixed, deterministic results for tests.",
            "dependencies": [
              1
            ],
            "details": "Add MockAnalysisEngine class with analyze(self, prompt: str) -> AnalysisResult returning a constant AnalysisResult instance matching the provided example values so tests can rely on stable outputs. Ensure it does not perform any network calls or depend on external state. Keep implementation simple and side-effect free so unit tests can assert exact summaries, recommendations, and tasks.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement PerplexityAnalysisEngine HTTP client with secure configuration",
            "description": "Create PerplexityAnalysisEngine that calls the Perplexity API using httpx with proper headers, payload, timeout, and error handling while avoiding API key leaks.",
            "dependencies": [
              1
            ],
            "details": "Implement __init__(self, api_key: str, timeout: int = 120) storing api_key, timeout, and base_url. Implement analyze(self, prompt: str) -> AnalysisResult using httpx.Client with configured timeout to POST to /chat/completions. Build headers including Authorization: Bearer <api_key> and Content-Type: application/json, and construct the payload with the specified model and system/user messages. Call response.raise_for_status() to surface HTTP errors, then parse JSON and extract the LLM message content. Never log or print the raw api_key, and avoid logging full request/response bodies that may contain sensitive data; if logging is needed, redact keys and truncate content.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement robust _parse_response() JSON extraction and graceful fallback",
            "description": "Add private _parse_response() helper on PerplexityAnalysisEngine to convert raw LLM content into AnalysisResult, handling invalid or wrapped JSON safely.",
            "dependencies": [
              3
            ],
            "details": "Implement _parse_response(self, content: str) -> AnalysisResult that first attempts json.loads(content). On json.JSONDecodeError, search for a ```json ... ``` fenced block via regex, attempt to json.loads on the captured block, and if that also fails, return an AnalysisResult with summary set to a truncated slice of the raw content (e.g., first 500 chars) and empty recommendations and tasks. Ensure missing keys in parsed JSON are handled via .get with sensible defaults. Do not execute or eval any content, and avoid logging full untrusted content; if logging parse failures, log only short snippets or generic error messages to reduce risk of leaking sensitive data.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement create_analysis_engine() factory for mock vs real engines",
            "description": "Create the create_analysis_engine() factory function to choose between MockAnalysisEngine and PerplexityAnalysisEngine based on api_key and use_mock flag.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement create_analysis_engine(api_key: str | None, use_mock: bool = False) -> AnalysisEngine such that it returns MockAnalysisEngine when use_mock is True or when api_key is falsy/None, and returns PerplexityAnalysisEngine(api_key) otherwise. Document this behavior clearly so callers (e.g., configuration/orchestrator) understand how to enable mock mode. Ensure the function does not log API keys and only logs high-level selection decisions if logging is added.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Write unit tests for analysis engines and JSON parsing with mocked httpx",
            "description": "Add comprehensive unit tests in tests/test_analysis.py covering mock engine, factory behavior, PerplexityAnalysisEngine HTTP interactions, and JSON parsing fallbacks without real network calls.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "Create tests verifying: (1) MockAnalysisEngine.analyze() returns an AnalysisResult with expected deterministic fields; (2) create_analysis_engine() returns MockAnalysisEngine when use_mock=True; (3) create_analysis_engine() returns MockAnalysisEngine when api_key is None or empty; (4) _parse_response() correctly parses valid JSON content; (5) _parse_response() extracts JSON from ```json fenced blocks; (6) _parse_response() falls back to truncated summary with empty lists on invalid JSON. Fully mock httpx.Client using monkeypatch or unittest.mock to simulate successful responses, HTTP error responses (ensuring raise_for_status propagates), and malformed model responses where choices/message/content is missing or non-JSON. Assert that no real HTTP requests are made and that error paths do not expose API keys or full sensitive payloads in exception messages or logs if any logging is present.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down Task 5 (Implement Analysis Engine with Mock Mode) into 6 subtasks: (1) define AnalysisResult dataclass and AnalysisEngine protocol; (2) implement MockAnalysisEngine with deterministic outputs for tests; (3) implement PerplexityAnalysisEngine HTTP client using httpx, including headers, payload shape, timeout, and error handling; (4) implement _parse_response() to robustly extract JSON from raw content or ```json``` blocks and degrade gracefully on invalid JSON; (5) implement create_analysis_engine() factory with clear rules for mock vs real engine; (6) write unit tests that fully mock httpx.Client to cover success, HTTP errors, malformed model responses, and JSON parse fallbacks, ensuring no real network access. Highlight security considerations around API keys and logging.",
        "updatedAt": "2025-12-14T02:00:08.782Z"
      },
      {
        "id": "6",
        "title": "Implement Plan Writer",
        "description": "Create the plan_writer.py module to generate the mvp_improvement_plan.md file from aggregated analysis results.",
        "details": "Implement `src/metaagent/plan_writer.py`:\n\n```python\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom .analysis import AnalysisResult\n\n@dataclass\nclass StageResult:\n    stage_name: str\n    prompt_id: str\n    result: AnalysisResult\n\nclass PlanWriter:\n    def __init__(self, output_dir: Path):\n        self.output_dir = output_dir\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n    \n    def write_plan(self, prd_content: str, stage_results: list[StageResult], profile_name: str) -> Path:\n        \"\"\"\n        Generate mvp_improvement_plan.md from analysis results.\n        \n        Args:\n            prd_content: Original PRD content\n            stage_results: Results from each analysis stage\n            profile_name: Name of the profile used\n            \n        Returns:\n            Path to the generated plan file\n        \"\"\"\n        output_path = self.output_dir / 'mvp_improvement_plan.md'\n        \n        lines = [\n            '# MVP Improvement Plan',\n            '',\n            f'**Generated:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}',\n            f'**Profile:** {profile_name}',\n            '',\n            '---',\n            '',\n            '## PRD Summary',\n            '',\n            self._extract_prd_summary(prd_content),\n            '',\n            '---',\n            ''\n        ]\n        \n        # Add stage summaries\n        lines.append('## Analysis Summaries')\n        lines.append('')\n        for sr in stage_results:\n            lines.append(f'### {sr.stage_name}')\n            lines.append('')\n            lines.append(sr.result.summary)\n            lines.append('')\n            if sr.result.recommendations:\n                lines.append('**Recommendations:**')\n                for rec in sr.result.recommendations:\n                    lines.append(f'- {rec}')\n                lines.append('')\n        \n        # Aggregate and prioritize tasks\n        lines.append('---')\n        lines.append('')\n        lines.append('## Prioritized Task List')\n        lines.append('')\n        lines.append('Complete tasks in order. Check off as completed.')\n        lines.append('')\n        \n        all_tasks = self._aggregate_tasks(stage_results)\n        for i, task in enumerate(all_tasks, 1):\n            priority_badge = self._priority_badge(task.get('priority', 'medium'))\n            lines.append(f'- [ ] **{i}. {task[\"title\"]}** {priority_badge}')\n            lines.append(f'  - {task[\"description\"]}')\n            if task.get('files'):\n                lines.append(f'  - Files: {\", \".join(task[\"files\"])}')\n            lines.append('')\n        \n        # Claude Code instruction block\n        lines.extend(self._claude_code_instructions())\n        \n        output_path.write_text('\\n'.join(lines), encoding='utf-8')\n        return output_path\n    \n    def _extract_prd_summary(self, prd_content: str) -> str:\n        \"\"\"Extract first 500 chars as summary.\"\"\"\n        lines = prd_content.strip().split('\\n')\n        summary_lines = []\n        char_count = 0\n        for line in lines:\n            if char_count + len(line) > 500:\n                break\n            summary_lines.append(line)\n            char_count += len(line)\n        return '\\n'.join(summary_lines) + '...'\n    \n    def _aggregate_tasks(self, stage_results: list[StageResult]) -> list[dict]:\n        \"\"\"Aggregate tasks from all stages, sorted by priority.\"\"\"\n        all_tasks = []\n        for sr in stage_results:\n            all_tasks.extend(sr.result.tasks)\n        \n        priority_order = {'high': 0, 'medium': 1, 'low': 2}\n        return sorted(all_tasks, key=lambda t: priority_order.get(t.get('priority', 'medium'), 1))\n    \n    def _priority_badge(self, priority: str) -> str:\n        badges = {'high': '🔴', 'medium': '🟡', 'low': '🟢'}\n        return badges.get(priority, '🟡')\n    \n    def _claude_code_instructions(self) -> list[str]:\n        return [\n            '---',\n            '',\n            '## Instructions for Claude Code',\n            '',\n            '1. Read this entire document before starting',\n            '2. Work through tasks in order, checking off each as completed',\n            '3. Run tests after each significant change',\n            '4. Commit changes incrementally with descriptive messages',\n            '5. If blocked, document the blocker and move to the next task',\n            ''\n        ]\n```",
        "testStrategy": "Unit tests in `tests/test_plan_writer.py`:\n1. Test write_plan() creates file at expected path\n2. Test output contains PRD summary section\n3. Test output contains stage summaries for all stages\n4. Test tasks are aggregated and sorted by priority\n5. Test Claude Code instruction block is included\n6. Test _priority_badge() returns correct emojis\n7. Test with empty stage_results list",
        "priority": "medium",
        "dependencies": [
          "1",
          "2",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define StageResult dataclass and PlanWriter constructor",
            "description": "Implement the basic class structure with StageResult dataclass and PlanWriter __init__ that creates output directory.",
            "dependencies": [],
            "details": "Create src/metaagent/plan_writer.py with imports (Path, dataclass, datetime, AnalysisResult), define StageResult with stage_name, prompt_id, result fields, and PlanWriter __init__ that sets self.output_dir and calls mkdir(parents=True, exist_ok=True).",
            "status": "pending",
            "testStrategy": "Test StageResult instantiation with sample data and PlanWriter __init__ creates directory using tmp_path fixture.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement write_plan() method with Markdown sections",
            "description": "Create the main write_plan method that assembles metadata, PRD summary, stage summaries, prioritized tasks, and Claude instructions into mvp_improvement_plan.md.",
            "dependencies": [
              1
            ],
            "details": "Implement write_plan(prd_content, stage_results, profile_name) that builds lines list with header, timestamp/profile metadata, PRD summary call, stage summaries loop, prioritized tasks section with _aggregate_tasks call, and _claude_code_instructions, then writes UTF-8 file.",
            "status": "pending",
            "testStrategy": "Test file creation at expected path, verify all sections present in output (header, metadata, PRD summary, stage summaries, tasks, instructions) using file content assertions.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement _extract_prd_summary with line-aware truncation",
            "description": "Create helper method to extract first ~500 characters of PRD content while preserving complete lines and adding ellipsis.",
            "dependencies": [
              1
            ],
            "details": "Implement _extract_prd_summary(prd_content) that splits by lines, accumulates until 500 char limit, joins lines, appends '...' for truncation. Handle empty/whitespace PRD gracefully.",
            "status": "pending",
            "testStrategy": "Test truncation at ~500 chars preserves lines, test short content returns full, test empty PRD returns empty string or minimal output.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement task aggregation and priority utilities",
            "description": "Add _aggregate_tasks to collect/sort tasks by priority and _priority_badge for emoji mapping.",
            "dependencies": [
              1
            ],
            "details": "Implement _aggregate_tasks(stage_results) that flattens all tasks and sorts by priority_order={'high':0,'medium':1,'low':2}, _priority_badge(priority) mapping to emojis (🔴🟡🟢). Ensure UTF-8 compatibility for emojis.",
            "status": "pending",
            "testStrategy": "Test task aggregation collects from multiple stages, verify priority sorting (high>medium>low), test badge emojis render correctly in UTF-8 output.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write comprehensive unit tests for PlanWriter",
            "description": "Create tests/test_plan_writer.py with pytest tests covering file creation, content sections, priority sorting, edge cases.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Use tmp_path fixture to test: file creation/path, all Markdown sections present, stage summaries rendered, tasks sorted by priority, empty tasks/stages handled, UTF-8 emoji encoding, PRD truncation. Mock AnalysisResult and StageResult.",
            "status": "pending",
            "testStrategy": "Run pytest with coverage: verify 100% pass rate, test empty inputs (no stages/no tasks), mixed priorities, long PRD truncation, Unicode emoji preservation in output file.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down Task 6 (Implement Plan Writer) into 5 subtasks: (1) define StageResult dataclass and PlanWriter constructor creating output_dir; (2) implement write_plan() to assemble sections (metadata, PRD summary, stage summaries, prioritized tasks, instructions) into Markdown; (3) implement _extract_prd_summary() with length limits and line-aware truncation; (4) implement _aggregate_tasks() and _priority_badge() with clear priority ordering and emoji mapping; (5) write unit tests using tmp_path to verify file creation, content sections, priority sorting, and behavior with empty tasks/stages. Note any i18n/encoding considerations for emojis and UTF-8 writes.",
        "updatedAt": "2025-12-14T02:00:13.300Z"
      },
      {
        "id": "7",
        "title": "Implement Orchestrator",
        "description": "Create the orchestrator.py module that coordinates the entire refinement workflow: loading config, running stages, calling analysis engine, and generating the plan.",
        "details": "Implement `src/metaagent/orchestrator.py`:\n\n```python\nimport logging\nfrom pathlib import Path\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\n\nfrom .config import Config\nfrom .prompts import PromptLibrary, Prompt\nfrom .repomix import RepomixRunner, RepomixResult\nfrom .analysis import create_analysis_engine, AnalysisResult\nfrom .plan_writer import PlanWriter, StageResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RunHistory:\n    \"\"\"Tracks analysis history for context in subsequent stages.\"\"\"\n    entries: list[dict] = field(default_factory=list)\n    \n    def add(self, stage: str, summary: str):\n        self.entries.append({\n            'stage': stage,\n            'timestamp': datetime.now().isoformat(),\n            'summary': summary\n        })\n    \n    def to_string(self) -> str:\n        if not self.entries:\n            return 'No previous analysis.'\n        lines = ['Previous analysis results:']\n        for entry in self.entries:\n            lines.append(f\"\\n## {entry['stage']} ({entry['timestamp']})\")\n            lines.append(entry['summary'])\n        return '\\n'.join(lines)\n\n@dataclass  \nclass RefinementResult:\n    success: bool\n    plan_path: Path | None\n    stage_results: list[StageResult]\n    errors: list[str]\n\nclass Orchestrator:\n    def __init__(self, config: Config, use_mock: bool = False):\n        self.config = config\n        self.use_mock = use_mock\n        self.prompt_library = PromptLibrary(config.config_dir)\n        self.repomix = RepomixRunner(timeout=config.timeout)\n        self.analysis_engine = create_analysis_engine(\n            config.perplexity_api_key,\n            use_mock=use_mock\n        )\n        self.plan_writer = PlanWriter(config.output_dir)\n        self.history = RunHistory()\n    \n    def refine(self, profile_name: str) -> RefinementResult:\n        \"\"\"\n        Run the full refinement workflow for the given profile.\n        \n        Args:\n            profile_name: Name of the profile to use\n            \n        Returns:\n            RefinementResult with success status and plan path\n        \"\"\"\n        errors = []\n        stage_results = []\n        \n        # Validate profile exists\n        profile = self.prompt_library.get_profile(profile_name)\n        if not profile:\n            available = self.prompt_library.list_profiles()\n            return RefinementResult(\n                success=False,\n                plan_path=None,\n                stage_results=[],\n                errors=[f'Profile \"{profile_name}\" not found. Available: {available}']\n            )\n        \n        logger.info(f'Starting refinement with profile: {profile_name}')\n        \n        # Load PRD\n        prd_content = self._load_prd()\n        if not prd_content:\n            return RefinementResult(\n                success=False,\n                plan_path=None,\n                stage_results=[],\n                errors=[f'PRD not found at {self.config.prd_path}']\n            )\n        \n        # Run Repomix\n        logger.info('Packing codebase with Repomix...')\n        repomix_result = self.repomix.pack(self.config.repo_path)\n        if not repomix_result.success:\n            errors.append(f'Repomix warning: {repomix_result.error}')\n            code_context = '[Repomix failed - limited code context available]'\n        else:\n            code_context = self.repomix.truncate_content(\n                repomix_result.content,\n                self.config.max_tokens\n            )\n        \n        # Run each stage\n        prompts = self.prompt_library.get_prompts_for_profile(profile_name)\n        for prompt in prompts:\n            logger.info(f'Running stage: {prompt.id}')\n            try:\n                result = self._run_stage(prompt, prd_content, code_context)\n                stage_results.append(StageResult(\n                    stage_name=prompt.id,\n                    prompt_id=prompt.id,\n                    result=result\n                ))\n                self.history.add(prompt.id, result.summary)\n            except Exception as e:\n                logger.error(f'Stage {prompt.id} failed: {e}')\n                errors.append(f'Stage {prompt.id} failed: {str(e)}')\n        \n        # Generate plan\n        if stage_results:\n            logger.info('Generating improvement plan...')\n            plan_path = self.plan_writer.write_plan(\n                prd_content,\n                stage_results,\n                profile_name\n            )\n            logger.info(f'Plan written to {plan_path}')\n        else:\n            plan_path = None\n            errors.append('No stages completed successfully')\n        \n        return RefinementResult(\n            success=len(errors) == 0,\n            plan_path=plan_path,\n            stage_results=stage_results,\n            errors=errors\n        )\n    \n    def _load_prd(self) -> str | None:\n        if self.config.prd_path.exists():\n            return self.config.prd_path.read_text(encoding='utf-8')\n        return None\n    \n    def _run_stage(self, prompt: Prompt, prd: str, code_context: str) -> AnalysisResult:\n        rendered = prompt.render(\n            prd=prd,\n            code_context=code_context,\n            history=self.history.to_string(),\n            current_stage=prompt.id\n        )\n        return self.analysis_engine.analyze(rendered)\n```",
        "testStrategy": "Unit tests in `tests/test_orchestrator.py`:\n1. Test refine() with valid profile returns success\n2. Test refine() with invalid profile returns error\n3. Test refine() with missing PRD returns error\n4. Test refine() continues after Repomix failure with warning\n5. Test stages are run in profile order\n6. Test history is accumulated across stages\n7. Test plan is generated after successful stages\n8. Use MockAnalysisEngine and mock Repomix for isolation",
        "priority": "high",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define RunHistory and RefinementResult dataclasses",
            "description": "Implement RunHistory class with add() and to_string() methods, and RefinementResult dataclass for workflow results.",
            "dependencies": [],
            "details": "Use @dataclass with field(default_factory=list) for RunHistory.entries. Implement timestamped history tracking and string formatting for prompt context. RefinementResult holds success flag, plan_path, stage_results list, and errors list.",
            "status": "pending",
            "testStrategy": "Test RunHistory.add() appends correctly, to_string() formats multi-entry history, empty history returns 'No previous analysis.' Test RefinementResult instantiation and field access.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Orchestrator.__init__ dependency wiring",
            "description": "Wire Config, PromptLibrary, RepomixRunner, analysis_engine factory, PlanWriter, and RunHistory in constructor.",
            "dependencies": [
              1
            ],
            "details": "Initialize self.config, self.prompt_library(config.config_dir), self.repomix(config.timeout), self.analysis_engine=create_analysis_engine(config.perplexity_api_key, use_mock), self.plan_writer(config.output_dir), self.history=RunHistory(). Support use_mock flag.",
            "status": "pending",
            "testStrategy": "Mock all dependencies, verify __init__ passes correct params to each component, test use_mock=True creates mock analysis engine.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement _load_prd() with file existence check",
            "description": "Load PRD content from config.prd_path with clear None return on missing file.",
            "dependencies": [
              2
            ],
            "details": "Use self.config.prd_path.exists() check, read_text(encoding='utf-8') on success, return None on failure. No exceptions, clean failure path for orchestrator.",
            "status": "pending",
            "testStrategy": "Mock Path.exists()=False returns None, mock Path.read_text() returns content, verify encoding handling.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Repomix integration in refine()",
            "description": "Run repomix.pack(), handle failure with warning, truncate content if successful.",
            "dependencies": [
              2,
              3
            ],
            "details": "Call self.repomix.pack(self.config.repo_path), on failure set code_context='[Repomix failed...]', on success truncate with self.repomix.truncate_content(result.content, self.config.max_tokens). Log progress.",
            "status": "pending",
            "testStrategy": "Mock repomix.pack() success with long content (verify truncation), mock failure (verify warning context), verify logging calls.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement stage execution loop with history",
            "description": "Fetch prompts for profile, render each with history/PRD/code_context, run analysis, accumulate StageResult, update history.",
            "dependencies": [
              1,
              2,
              4
            ],
            "details": "Get prompts = self.prompt_library.get_prompts_for_profile(profile_name), loop: render=prompt.render(prd, code_context, history=self.history.to_string(), current_stage=prompt.id), result=self.analysis_engine.analyze(rendered), create StageResult, self.history.add(). Catch exceptions per stage.",
            "status": "pending",
            "testStrategy": "Mock prompt_library.get_prompts_for_profile() returns 2 prompts, verify sequential execution, history updates after each stage, StageResult list accumulates correctly, partial failure continues.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement plan generation and RefinementResult",
            "description": "Generate plan if stages succeeded, assemble final RefinementResult with success/errors semantics.",
            "dependencies": [
              1,
              2,
              5
            ],
            "details": "After stage loop, if stage_results: plan_path=self.plan_writer.write_plan(prd_content, stage_results, profile_name), else plan_path=None. Return RefinementResult(success=len(errors)==0, plan_path, stage_results, errors). Early returns for profile/PRD validation.",
            "status": "pending",
            "testStrategy": "Mock plan_writer.write_plan() returns Path, verify success=True when errors=[], success=False with errors, test no-stages-empty-results case.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Write comprehensive unit tests for Orchestrator",
            "description": "Create tests/test_orchestrator.py with heavy mocking covering all paths: success, missing profile/PRD, Repomix/stage failures.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Use pytest, unittest.mock: Mock PromptLibrary.get_profile()/get_prompts_for_profile(), RepomixRunner.pack(), create_analysis_engine(), PlanWriter.write_plan(). Test 10+ scenarios: valid profile success path, invalid profile early return, missing PRD, Repomix fail continues, stage exceptions accumulate, history chaining, empty profile prompts.",
            "status": "pending",
            "testStrategy": "Aim for 90%+ coverage. Verify orchestration logic isolation from I/O. Test history.to_string() called sequentially, StageResult.prompt_id==stage_name consistency, error accumulation without crashing.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Break down Task 7 (Implement Orchestrator) into 7 subtasks: (1) define RunHistory and RefinementResult data structures; (2) implement Orchestrator.__init__ wiring Config, PromptLibrary, RepomixRunner, AnalysisEngine factory, and PlanWriter; (3) implement _load_prd() with clear failure behavior; (4) implement Repomix integration inside refine(), including warning handling and context truncation; (5) implement stage loop: fetching prompts for a profile, rendering with history, invoking analysis, accumulating StageResult, and updating RunHistory; (6) implement plan generation and final RefinementResult assembly with success/error semantics; (7) write unit tests that heavily mock PromptLibrary, RepomixRunner, and AnalysisEngine to cover success path, missing profile, missing PRD, Repomix failures, partial stage failures, history accumulation, and no-stages cases. Emphasize separation of orchestration logic from I/O for testability.",
        "updatedAt": "2025-12-14T02:00:17.617Z"
      },
      {
        "id": "8",
        "title": "Implement CLI Entrypoint",
        "description": "Create the CLI using Typer with the `metaagent refine` command that validates arguments and invokes the orchestrator.",
        "details": "Implement `src/metaagent/cli.py`:\n\n```python\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Optional\n\nimport typer\nfrom rich.console import Console\nfrom rich.logging import RichHandler\n\nfrom .config import Config\nfrom .orchestrator import Orchestrator\nfrom .prompts import PromptLibrary\n\napp = typer.Typer(\n    name='metaagent',\n    help='Meta-agent for automated codebase refinement from v0 to MVP'\n)\nconsole = Console()\n\ndef setup_logging(level: str):\n    logging.basicConfig(\n        level=level,\n        format='%(message)s',\n        handlers=[RichHandler(rich_tracebacks=True)]\n    )\n\n@app.command()\ndef refine(\n    profile: str = typer.Option(\n        ...,\n        '--profile', '-p',\n        help='Profile to use for refinement (e.g., automation_agent, backend_service)'\n    ),\n    repo: Path = typer.Option(\n        Path('.'),\n        '--repo', '-r',\n        help='Path to the repository to refine'\n    ),\n    prd: Optional[Path] = typer.Option(\n        None,\n        '--prd',\n        help='Path to PRD file (default: docs/prd.md in repo)'\n    ),\n    mock: bool = typer.Option(\n        False,\n        '--mock',\n        help='Use mock analysis engine (no API calls)'\n    ),\n    verbose: bool = typer.Option(\n        False,\n        '--verbose', '-v',\n        help='Enable verbose output'\n    )\n):\n    \"\"\"\n    Refine a codebase from v0 to MVP using automated analysis and planning.\n    \n    Example:\n        metaagent refine --profile automation_agent --repo /path/to/repo\n    \"\"\"\n    # Setup logging\n    log_level = 'DEBUG' if verbose else 'INFO'\n    setup_logging(log_level)\n    \n    # Resolve paths\n    repo_path = repo.resolve()\n    if not repo_path.exists():\n        console.print(f'[red]Error: Repository path does not exist: {repo_path}[/red]')\n        raise typer.Exit(1)\n    \n    # Load config\n    config = Config.from_env(repo_path)\n    if prd:\n        config.prd_path = prd.resolve()\n    \n    # Validate config\n    errors = config.validate()\n    if errors:\n        for err in errors:\n            console.print(f'[red]Configuration error: {err}[/red]')\n        raise typer.Exit(1)\n    \n    # Show available profiles if requested profile doesn't exist\n    prompt_library = PromptLibrary(config.config_dir)\n    if not prompt_library.get_profile(profile):\n        available = prompt_library.list_profiles()\n        console.print(f'[red]Error: Profile \"{profile}\" not found.[/red]')\n        console.print(f'Available profiles: {available}')\n        raise typer.Exit(1)\n    \n    # Run refinement\n    console.print(f'[bold blue]Starting refinement...[/bold blue]')\n    console.print(f'  Profile: {profile}')\n    console.print(f'  Repository: {repo_path}')\n    console.print(f'  Mock mode: {mock}')\n    console.print()\n    \n    orchestrator = Orchestrator(config, use_mock=mock)\n    result = orchestrator.refine(profile)\n    \n    # Report results\n    if result.success:\n        console.print('[bold green]Refinement completed successfully![/bold green]')\n        console.print(f'Plan written to: {result.plan_path}')\n    else:\n        console.print('[bold yellow]Refinement completed with warnings:[/bold yellow]')\n        for err in result.errors:\n            console.print(f'  - {err}')\n        if result.plan_path:\n            console.print(f'Plan written to: {result.plan_path}')\n    \n    console.print(f'\\nStages completed: {len(result.stage_results)}')\n\n@app.command()\ndef list_profiles(\n    config_dir: Optional[Path] = typer.Option(\n        None,\n        '--config-dir', '-c',\n        help='Path to config directory'\n    )\n):\n    \"\"\"List available refinement profiles.\"\"\"\n    config = Config.from_env()\n    if config_dir:\n        config.config_dir = config_dir.resolve()\n    \n    prompt_library = PromptLibrary(config.config_dir)\n    profiles = prompt_library.list_profiles()\n    \n    if not profiles:\n        console.print('[yellow]No profiles found.[/yellow]')\n        return\n    \n    console.print('[bold]Available Profiles:[/bold]')\n    for name in profiles:\n        profile = prompt_library.get_profile(name)\n        console.print(f'  {name}: {profile.description}')\n        console.print(f'    Stages: {profile.stages}')\n\ndef main():\n    app()\n\nif __name__ == '__main__':\n    main()\n```\n\nAdd to `src/metaagent/__init__.py`:\n```python\n__version__ = '0.1.0'\n```",
        "testStrategy": "Unit tests in `tests/test_cli.py`:\n1. Test `refine` command with valid args using CliRunner\n2. Test `refine` command fails gracefully with invalid profile\n3. Test `refine` command fails gracefully with non-existent repo\n4. Test `list-profiles` command outputs available profiles\n5. Test `--mock` flag passes through to orchestrator\n6. Test `--verbose` flag sets logging level correctly\n7. Test help text is displayed with `--help`",
        "priority": "high",
        "dependencies": [
          "2",
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Typer app, main() entrypoint, and package __version__",
            "description": "Create the Typer application object, wire up the main() entrypoint compatible with the pyproject console_script, and expose the package version in __init__.py.",
            "dependencies": [],
            "details": "Implement src/metaagent/cli.py with a module-level typer.Typer instance (name='metaagent') and a main() function that calls app(), suitable for use as the console_script entrypoint. Ensure __init__.py defines __version__ = '0.1.0' and that the CLI module can be imported without side effects beyond defining commands. Confirm the structure aligns with expected project layout so that `metaagent` runs the Typer app when installed.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement logging setup using RichHandler with configurable levels",
            "description": "Add a reusable logging setup function that configures logging with RichHandler and supports INFO/DEBUG levels via a parameter.",
            "dependencies": [
              1
            ],
            "details": "In cli.py, implement setup_logging(level: str) that calls logging.basicConfig with RichHandler(rich_tracebacks=True) and a simple message format. Use the provided code skeleton as reference. Ensure that calling setup_logging with 'DEBUG' or 'INFO' correctly changes the global log level and that it does not re-add duplicate handlers on multiple invocations in typical CLI use. Prepare for verbose flag integration in the refine command.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement `refine` command with argument parsing and orchestrator integration",
            "description": "Create the refine Typer command that parses CLI options, resolves paths, loads and validates config, checks profile existence, calls the orchestrator, and reports results.",
            "dependencies": [
              1,
              2
            ],
            "details": "Define @app.command() refine(...) with options: --profile/-p (required str), --repo/-r (Path, default '.'), --prd (Optional[Path]), --mock (bool), and --verbose/-v (bool). Inside, set log_level to 'DEBUG' when verbose is True, else 'INFO', and call setup_logging. Resolve repo path; if it does not exist, print a red Rich error message and exit with typer.Exit(1). Call Config.from_env(repo_path) to load configuration, apply prd override if provided, and run config.validate(); on validation errors, print each as a Rich red configuration error and exit 1. Instantiate PromptLibrary with config.config_dir, verify the requested profile exists via get_profile(), and if missing, print a red error plus a list of available profiles before exiting 1. On success, log a blue starting message with profile, repo, and mock mode. Create Orchestrator(config, use_mock=mock), call orchestrator.refine(profile), and then print green success output with plan path on success, or yellow warnings listing each error and plan path when present. Always print the count of completed stages at the end.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement `list_profiles` command with optional config_dir override",
            "description": "Add the list_profiles Typer command that loads configuration, applies an optional config directory override, and prints available profiles using PromptLibrary.",
            "dependencies": [
              1,
              3
            ],
            "details": "Define @app.command() list_profiles(config_dir: Optional[Path] = Option(None, '--config-dir', '-c', ...)). Inside, call Config.from_env() with default parameters, and if config_dir is provided, resolve it and assign to config.config_dir. Instantiate PromptLibrary(config.config_dir), call list_profiles(), and if the list is empty print a yellow notice and return without error. Otherwise, print a bold header and for each profile name, fetch the profile object with get_profile(name) and print its description and stages in a readable, indented format using Rich markup. Ensure this command exits with code 0 on normal completion.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Ensure consistent CLI error handling and exit codes with Rich messages",
            "description": "Review and refine CLI error handling so that all failure scenarios return appropriate exit codes and user-friendly Rich-formatted messages.",
            "dependencies": [
              3,
              4
            ],
            "details": "Audit refine and list_profiles commands to make sure all early-return error states use typer.Exit with non-zero codes (e.g., 1) for invalid repo paths, configuration validation failures, missing profiles, and other user errors, while successful paths exit with 0. Standardize Rich output styles for errors (red), warnings (yellow), and success (green/blue). Confirm no unhandled exceptions leak to the user in common failure scenarios, and that messages remain concise and informative across platforms. Avoid using sys.exit directly; rely on typer.Exit for consistency.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Write CLI tests with typer.testing.CliRunner covering success and failure paths",
            "description": "Create comprehensive tests for the CLI using CliRunner to exercise refine and list_profiles commands across success, failure, mock, verbose, and help scenarios without hitting real external APIs or subprocesses.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "In tests/test_cli.py, instantiate CliRunner and write tests that invoke the Typer app (e.g., runner.invoke(cli.app, [...])). Cover: (a) successful refine with valid args, mocking Config.from_env, PromptLibrary, and Orchestrator to avoid real file system, APIs, or subprocesses; (b) refine with invalid profile, ensuring proper error message and non-zero exit code; (c) refine with non-existent repo path, checking path handling is cross-platform safe (e.g., using tmp_path / 'missing'); (d) refine with --mock and --verbose ensuring they alter behavior/log level and output; (e) list_profiles showing available profiles, using mocked PromptLibrary; and (f) help text for top-level and refine/list_profiles commands. Use monkeypatch or similar to inject test doubles, and assert output strings and exit codes without relying on actual environment variables or network calls.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down Task 8 (Implement CLI Entrypoint) into 6 subtasks: (1) set up Typer app structure and main() entrypoint compatible with pyproject console_script; (2) implement logging setup with RichHandler and configurable levels; (3) implement refine command: argument parsing, repo path resolution, Config.from_env usage, optional PRD override, config validation, profile existence check, orchestrator invocation, and result reporting; (4) implement list_profiles command: config_dir override, PromptLibrary usage, and formatted output; (5) ensure CLI error handling exits with proper codes and user-friendly Rich messages; (6) write tests using typer.testing.CliRunner that exercise success and failure paths, mock mode, verbose flag, invalid repo/profile, and help text. Note cross-platform path considerations and how to avoid hitting real APIs/subprocesses in tests.",
        "updatedAt": "2025-12-14T02:00:22.408Z"
      },
      {
        "id": "9",
        "title": "Create Complete Configuration Files",
        "description": "Populate config/prompts.yaml and config/profiles.yaml with all prompt templates and profile definitions from the PRD.",
        "details": "Create `config/prompts.yaml` with exact content from PRD Section 7.1:\n\n```yaml\nprompts:\n  alignment_with_prd:\n    id: alignment_with_prd\n    goal: \"Identify gaps between current implementation and PRD requirements\"\n    stage: alignment\n    template: |\n      You are analyzing a codebase against its PRD.\n\n      ## PRD:\n      {{prd}}\n\n      ## Current Codebase:\n      {{code_context}}\n\n      ## Previous Analysis (if any):\n      {{history}}\n\n      Current Stage: {{current_stage}}\n\n      Please analyze and provide:\n      1. Summary of alignment gaps\n      2. Missing features or incomplete implementations\n      3. Prioritized task list to close gaps\n\n      Format your response as JSON with keys: summary, recommendations, tasks\n\n  architecture_sanity:\n    id: architecture_sanity\n    goal: \"Review architecture for best practices and maintainability\"\n    stage: architecture\n    template: |\n      Review this codebase for architectural quality.\n\n      ## PRD Context:\n      {{prd}}\n\n      ## Codebase:\n      {{code_context}}\n\n      Analyze:\n      1. Code organization and modularity\n      2. Separation of concerns\n      3. Error handling patterns\n      4. Dependency management\n\n      Format your response as JSON with keys: summary, recommendations, tasks\n\n  core_flow_hardening:\n    id: core_flow_hardening\n    goal: \"Identify robustness improvements for core flows\"\n    stage: hardening\n    template: |\n      Analyze core flows for robustness.\n\n      ## PRD:\n      {{prd}}\n\n      ## Codebase:\n      {{code_context}}\n\n      ## Analysis History:\n      {{history}}\n\n      Focus on:\n      1. Error handling and recovery\n      2. Retry logic for external calls\n      3. Input validation\n      4. Edge cases\n\n      Format your response as JSON with keys: summary, recommendations, tasks\n\n  test_suite_mvp:\n    id: test_suite_mvp\n    goal: \"Identify critical tests needed for MVP quality\"\n    stage: testing\n    template: |\n      Review test coverage for this codebase.\n\n      ## PRD:\n      {{prd}}\n\n      ## Codebase:\n      {{code_context}}\n\n      Identify:\n      1. Missing unit tests for core functions\n      2. Missing integration tests for main flows\n      3. Edge cases without test coverage\n\n      Format your response as JSON with keys: summary, recommendations, tasks\n```\n\nCreate `config/profiles.yaml` with content from PRD Section 7.2:\n\n```yaml\nprofiles:\n  automation_agent:\n    name: \"Automation Agent\"\n    description: \"Profile for CLI tools and automation agents\"\n    stages:\n      - alignment_with_prd\n      - architecture_sanity\n      - core_flow_hardening\n      - test_suite_mvp\n\n  backend_service:\n    name: \"Backend Service\"\n    description: \"Profile for API backends and services\"\n    stages:\n      - alignment_with_prd\n      - architecture_sanity\n      - core_flow_hardening\n      - test_suite_mvp\n\n  internal_tool:\n    name: \"Internal Tool\"\n    description: \"Profile for internal developer tools\"\n    stages:\n      - alignment_with_prd\n      - core_flow_hardening\n```",
        "testStrategy": "Validation tests:\n1. Validate prompts.yaml is valid YAML and parses correctly\n2. Validate profiles.yaml is valid YAML and parses correctly\n3. Test PromptLibrary loads all 4 prompts\n4. Test PromptLibrary loads all 3 profiles\n5. Test each prompt template renders without Jinja errors\n6. Verify all stages referenced in profiles exist in prompts",
        "priority": "medium",
        "dependencies": [
          "1",
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Author config/prompts.yaml with PRD templates",
            "description": "Create prompts.yaml file with all 4 prompt templates from PRD Section 7.1 including required Jinja placeholders",
            "dependencies": [],
            "details": "Copy exact content for alignment_with_prd, architecture_sanity, core_flow_hardening, test_suite_mvp prompts. Ensure all templates include {{prd}}, {{code_context}}, {{history}}, {{current_stage}} placeholders where appropriate. Verify YAML indentation and structure.",
            "status": "pending",
            "testStrategy": "Validate YAML parses correctly, check all 4 prompts load via PromptLibrary, test Jinja rendering with sample context for each template",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Author config/profiles.yaml with complete profiles",
            "description": "Create profiles.yaml with all 3 profiles and their stage sequences from PRD Section 7.2",
            "dependencies": [
              1
            ],
            "details": "Implement automation_agent, backend_service, internal_tool profiles with exact stage sequences. Ensure all referenced stage IDs exist in prompts.yaml (alignment_with_prd, architecture_sanity, core_flow_hardening, test_suite_mvp). Keep configs environment-agnostic.",
            "status": "pending",
            "testStrategy": "Validate YAML parses correctly, verify PromptLibrary loads all 3 profiles, check referential integrity between profiles.stages and prompts keys",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement validation tests for config files",
            "description": "Write tests/test_prompts.py to validate both YAML files load correctly and maintain integrity",
            "dependencies": [
              1,
              2
            ],
            "details": "Create pytest suite that: 1) Loads both YAML files via PromptLibrary, 2) Asserts 4 prompts and 3 profiles found, 3) Tests all prompt templates render without Jinja errors using mock context, 4) Verifies profile stage references exist in prompts, 5) Uses relative paths for CI compatibility.",
            "status": "pending",
            "testStrategy": "Run pytest tests/test_prompts.py. Should pass: YAML parsing, prompt/profile counts, template renderability, referential integrity checks. Tests must work in CI without environment-specific paths.",
            "parentId": "undefined"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down Task 9 (Create Complete Configuration Files) into 3 subtasks: (1) author prompts.yaml exactly per PRD, adding required Jinja placeholders (prd, code_context, history, current_stage) and validate formatting; (2) author profiles.yaml with all profiles and stage sequences, ensuring all referenced stages exist in prompts.yaml; (3) write small validation tests or scripts (e.g., in tests/test_prompts.py) to load both YAML files via PromptLibrary and assert prompt/profile counts, renderability, and referential integrity. Note how to keep these configs environment-agnostic for CI.",
        "updatedAt": "2025-12-14T02:00:33.045Z"
      },
      {
        "id": "10",
        "title": "Add Comprehensive Test Suite",
        "description": "Create a complete test suite covering all modules with unit tests, integration tests, and fixtures for testing.",
        "details": "Create comprehensive test files with pytest fixtures and mocks:\n\n`tests/conftest.py`:\n```python\nimport pytest\nfrom pathlib import Path\nimport tempfile\nimport shutil\n\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Create a temporary directory for tests.\"\"\"\n    d = Path(tempfile.mkdtemp())\n    yield d\n    shutil.rmtree(d, ignore_errors=True)\n\n@pytest.fixture\ndef sample_prd():\n    return '''# Test PRD\n## Overview\nThis is a test PRD for unit testing.\n## Requirements\n- REQ1: Feature one\n- REQ2: Feature two\n'''\n\n@pytest.fixture\ndef sample_code_context():\n    return '''# Packed Codebase\n## src/main.py\ndef main():\n    print(\"Hello World\")\n'''\n\n@pytest.fixture\ndef mock_prompts_yaml():\n    return '''prompts:\n  test_prompt:\n    id: test_prompt\n    goal: Test goal\n    stage: test\n    template: |\n      PRD: {{prd}}\n      Code: {{code_context}}\n'''\n\n@pytest.fixture\ndef mock_profiles_yaml():\n    return '''profiles:\n  test_profile:\n    name: Test Profile\n    description: For testing\n    stages:\n      - test_prompt\n'''\n\n@pytest.fixture\ndef config_dir(temp_dir, mock_prompts_yaml, mock_profiles_yaml):\n    \"\"\"Create a config directory with test YAML files.\"\"\"\n    config = temp_dir / 'config'\n    config.mkdir()\n    (config / 'prompts.yaml').write_text(mock_prompts_yaml)\n    (config / 'profiles.yaml').write_text(mock_profiles_yaml)\n    return config\n\n@pytest.fixture\ndef test_repo(temp_dir, sample_prd):\n    \"\"\"Create a test repository structure.\"\"\"\n    repo = temp_dir / 'test_repo'\n    repo.mkdir()\n    docs = repo / 'docs'\n    docs.mkdir()\n    (docs / 'prd.md').write_text(sample_prd)\n    (repo / 'src').mkdir()\n    (repo / 'src' / 'main.py').write_text('print(\"hello\")')\n    return repo\n```\n\nTest files to create:\n- `tests/test_config.py` - Config loading and validation\n- `tests/test_prompts.py` - Prompt/profile loading and rendering\n- `tests/test_repomix.py` - Repomix integration with mocked subprocess\n- `tests/test_analysis.py` - Analysis engine with mock and parsing\n- `tests/test_plan_writer.py` - Plan generation\n- `tests/test_orchestrator.py` - Full workflow with mocks\n- `tests/test_cli.py` - CLI commands with typer.testing.CliRunner\n\nEach test file should:\n- Use pytest fixtures from conftest.py\n- Mock external dependencies (subprocess, httpx)\n- Cover happy path and error cases\n- Be runnable with `pytest tests/`",
        "testStrategy": "Meta-test strategy:\n1. Run `pytest tests/ -v` to execute all tests\n2. Run `pytest tests/ --cov=metaagent --cov-report=html` for coverage\n3. Ensure >80% code coverage\n4. Verify all tests pass in CI environment (no API keys)\n5. Mark integration tests with @pytest.mark.integration\n6. Use `pytest -m \"not integration\"` for fast unit test runs",
        "priority": "medium",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create tests/conftest.py with shared fixtures",
            "description": "Implement conftest.py with fixtures for temp directories, sample PRD/code, mock YAML configs, and test repo structures.",
            "dependencies": [],
            "details": "Copy provided conftest.py code exactly. Add fixtures: temp_dir, sample_prd, sample_code_context, mock_prompts_yaml, mock_profiles_yaml, config_dir, test_repo. Ensure cleanup with shutil.rmtree.",
            "status": "pending",
            "testStrategy": "Run pytest tests/conftest.py -s to verify fixtures create/cleanup correctly without errors",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement tests/test_config.py",
            "description": "Create comprehensive tests for config.py covering env loading, defaults, validation, and path resolution.",
            "dependencies": [
              1
            ],
            "details": "Test Config loading from config_dir fixture, environment variable overrides, default values, path resolution with test_repo, validation errors for missing files/invalid YAML. Use parametrize for edge cases.",
            "status": "pending",
            "testStrategy": "pytest tests/test_config.py --cov=src/metaagent/config -v ensuring 90%+ coverage, happy path and validation errors pass",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement tests/test_prompts.py",
            "description": "Create tests for prompts.py module: loading YAML, rendering templates, error handling for missing/invalid files.",
            "dependencies": [
              1
            ],
            "details": "Test PromptLibrary loading from mock_prompts_yaml/profiles_yaml fixtures, Prompt.render() with jinja variables (prd, code_context, history, stage), get_prompts_for_profile(), list_profiles(), missing file errors.",
            "status": "pending",
            "testStrategy": "pytest tests/test_prompts.py -v verify rendering produces expected output strings, invalid YAML raises ValueError",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement tests/test_repomix.py with subprocess mocks",
            "description": "Test repomix.py with comprehensive subprocess.run mocking for success, failure, timeout, FileNotFoundError, truncation.",
            "dependencies": [
              1
            ],
            "details": "Use pytest mocker for subprocess.run. Test RepomixRunner with test_repo fixture, mock stdout/stderr/returncode, verify truncation logic, error handling paths, RepomixResult parsing.",
            "status": "pending",
            "testStrategy": "pytest tests/test_repomix.py::TestRepomix -v ensure all mock scenarios (0,1,timeout,FileNotFoundError) return expected RepomixResult objects",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement tests/test_analysis.py with httpx mocks",
            "description": "Test analysis.py: MockAnalysisEngine, create_analysis_engine(), _parse_response() with valid JSON and fallbacks.",
            "dependencies": [
              1
            ],
            "details": "Mock httpx.Client for real engine tests. Test mock mode returns deterministic AnalysisResult, parsing valid/invalid JSON responses, fallback parsing, use_mock=True/None api_key behavior.",
            "status": "pending",
            "testStrategy": "pytest tests/test_analysis.py -v verify MockAnalysisEngine.analyze() returns consistent AnalysisResult, parse_response handles malformed JSON gracefully",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement tests/test_plan_writer.py",
            "description": "Test plan_writer.py: file output generation, section formatting, priority sorting, edge case handling.",
            "dependencies": [
              1
            ],
            "details": "Use temp_dir fixture for output files. Test PlanWriter with sample AnalysisResult/tasks, verify markdown sections rendered correctly, priority sorting (high>medium>low), empty tasks edge case.",
            "status": "pending",
            "testStrategy": "pytest tests/test_plan_writer.py -v check generated markdown files match expected content via file.read_text()",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Implement orchestrator.py and CLI tests with coverage",
            "description": "Create tests/test_orchestrator.py and tests/test_cli.py. Add pytest markers, coverage config, CI-ready setup.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Test Orchestrator.refine() full workflow with mocks. Use typer.testing.CliRunner for CLI tests. Add pytest.ini with markers (slow, integration). Ensure no external deps. Mock all APIs/subprocess.",
            "status": "pending",
            "testStrategy": "pytest tests/ --cov=src/metaagent --cov-report=term-missing -m 'not slow' ensuring >80% coverage, all unit tests pass. Separate integration marker tests.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Break down Task 10 (Add Comprehensive Test Suite) into 7 subtasks: (1) create tests/conftest.py with shared fixtures for temp dirs, sample PRD/code, mock config YAMLs, and test repos; (2) implement tests for config.py (env loading, defaults, validation, path resolution); (3) implement tests for prompts.py (loading, rendering, missing files, invalid data); (4) implement tests for repomix.py with mocked subprocess.run covering success/failure/timeout/FileNotFoundError and truncation logic; (5) implement tests for analysis.py with mocked httpx, including mock engine and parse fallbacks; (6) implement tests for plan_writer.py (file output, sections, priority sorting, edge cases); (7) implement orchestrator and CLI tests using mocks/CliRunner, add coverage configuration, and mark slow/integration tests. Explicitly plan for deterministic, isolated tests suitable for CI with no external services or Node dependencies.",
        "updatedAt": "2025-12-14T02:00:39.501Z"
      },
      {
        "id": "11",
        "title": "Create Documentation and Environment Setup",
        "description": "Write README.md with installation instructions, usage examples, and update .env.example with all required environment variables.",
        "details": "Create `README.md`:\n\n```markdown\n# Meta-Agent\n\nA Python CLI tool for automated codebase refinement from v0 to MVP.\n\n## Overview\n\nMeta-Agent analyzes your codebase against a PRD and generates an improvement plan using AI-powered analysis. It integrates:\n- **Repomix** for codebase packing\n- **Perplexity API** for analysis and planning\n- Generates plans for **Claude Code** to implement\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone <repo-url>\ncd meta-agent\n\n# Install with uv (recommended)\nuv pip install -e .\n\n# Or with pip\npip install -e .\n```\n\n## Configuration\n\n1. Copy `.env.example` to `.env`\n2. Add your API keys:\n   - `PERPLEXITY_API_KEY` - Required for AI analysis\n   - `ANTHROPIC_API_KEY` - Optional, for Claude integration\n\n## Usage\n\n### Basic Usage\n\n```bash\n# Refine current directory with automation_agent profile\nmetaagent refine --profile automation_agent --repo .\n\n# Use mock mode for testing (no API calls)\nmetaagent refine --profile automation_agent --repo . --mock\n\n# List available profiles\nmetaagent list-profiles\n```\n\n### Profiles\n\n- `automation_agent` - For CLI tools and automation agents\n- `backend_service` - For API backends and services  \n- `internal_tool` - For internal developer tools\n\n### Output\n\nAfter running, find the improvement plan at `docs/mvp_improvement_plan.md`.\n\n## Development\n\n```bash\n# Install dev dependencies\nuv pip install -e \".[dev]\"\n\n# Run tests\npytest tests/ -v\n\n# Run with coverage\npytest tests/ --cov=metaagent --cov-report=html\n```\n\n## Project Structure\n\n```\nmeta-agent/\n├── src/metaagent/     # Main package\n│   ├── cli.py         # CLI entrypoint\n│   ├── orchestrator.py # Main workflow\n│   ├── repomix.py     # Repomix integration\n│   ├── prompts.py     # Prompt/profile loading\n│   ├── analysis.py    # LLM analysis engine\n│   └── plan_writer.py # Plan generation\n├── config/\n│   ├── prompts.yaml   # Prompt templates\n│   └── profiles.yaml  # Profile definitions\n└── tests/             # Test suite\n```\n\n## License\n\nMIT\n```\n\nUpdate `.env.example` for meta-agent:\n```bash\n# Meta-Agent Configuration\n\n# Required: Perplexity API key for analysis\nPERPLEXITY_API_KEY=\"pplx-...\"\n\n# Optional: Anthropic API key for Claude integration\nANTHROPIC_API_KEY=\"sk-ant-...\"\n\n# Optional: Logging and behavior\nMETAAGENT_LOG_LEVEL=INFO\nMETAAGENT_TIMEOUT=120\nMETAAGENT_MAX_TOKENS=100000\n```",
        "testStrategy": "Verification:\n1. README renders correctly in GitHub/GitLab\n2. Installation instructions work on clean environment\n3. All CLI examples in README work correctly\n4. .env.example contains all environment variables used in code\n5. No sensitive data in example files",
        "priority": "low",
        "dependencies": [
          "1",
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Write Comprehensive README.md",
            "description": "Create the full README.md file including overview, installation instructions for uv and pip, configuration steps, usage examples, profiles description, output details, development workflow, and project structure as specified.",
            "dependencies": [],
            "details": "Use the provided README template as base. Ensure all sections are complete: Overview with integrations (Repomix, Perplexity, Claude), Installation with git clone + uv/pip, Configuration with .env steps, Usage with basic/mock/list-profiles examples, Profiles list, Output location, Development with dev deps/tests/coverage, Project structure tree, and License. Make repo-url placeholder clear.",
            "status": "pending",
            "testStrategy": "Verify Markdown renders correctly in GitHub preview, check all code blocks are valid bash/python, ensure instructions match current CLI behavior from task dependencies.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create and Update .env.example File",
            "description": "Generate .env.example with all environment variables from Config.from_env including required/optional vars, safe placeholders, and explanatory comments.",
            "dependencies": [],
            "details": "Include PERPLEXITY_API_KEY (required), ANTHROPIC_API_KEY (optional), METAAGENT_LOG_LEVEL=INFO, METAAGENT_TIMEOUT=120, METAAGENT_MAX_TOKENS=100000. Match exactly Config dataclass fields: perplexity_api_key, anthropic_api_key, log_level, timeout, max_tokens. Add comments explaining purpose and defaults.",
            "status": "pending",
            "testStrategy": "Validate file is correct YAML-like format, cross-check against src/metaagent/config.py Config.from_env expected vars, ensure no real API keys, test loading with dotenv in clean env.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Validate Documentation in Clean Environment",
            "description": "Manually test README instructions and .env.example by following steps in a fresh environment, verify consistency with code and CLI behavior.",
            "dependencies": [
              1,
              2
            ],
            "details": "In clean dir: 1) Clone/follow install (uv/pip), 2) Copy .env.example to .env with dummy keys, 3) Run CLI examples (metaagent --help, list-profiles, refine --mock), 4) Check output/docs/mvp_improvement_plan.md generates, 5) Run dev commands (pytest). Note any discrepancies for future updates.",
            "status": "pending",
            "testStrategy": "Checklist: Installation succeeds without errors, all README CLI examples execute without crashes, .env vars load correctly per Config.from_env tests, docs match actual project structure/dependencies (tasks 1,2,8,9), no broken links/paths.",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down Task 11 (Create Documentation and Environment Setup) into 3 subtasks: (1) write README.md with clear overview, installation (uv and pip), configuration, usage examples, profiles description, output description, development workflow, and project structure; (2) create .env.example listing all relevant environment variables with safe placeholder values and comments; (3) manually validate docs by following the README in a clean environment and cross-checking that .env.example matches what Config.from_env expects and what the code actually uses. Note how to keep examples up to date with CLI and config behavior.",
        "updatedAt": "2025-12-14T02:00:46.066Z"
      },
      {
        "id": "12",
        "title": "End-to-End Integration Test with Mock Mode",
        "description": "Create an end-to-end test that exercises the complete workflow using mock mode, verifying the system generates a valid improvement plan.",
        "details": "Create `tests/test_e2e.py`:\n\n```python\nimport pytest\nfrom pathlib import Path\nfrom typer.testing import CliRunner\n\nfrom metaagent.cli import app\n\nrunner = CliRunner()\n\nclass TestEndToEnd:\n    \"\"\"End-to-end tests for the complete refinement workflow.\"\"\"\n    \n    @pytest.fixture\n    def sample_repo(self, tmp_path):\n        \"\"\"Create a complete sample repository for testing.\"\"\"\n        repo = tmp_path / 'sample_project'\n        repo.mkdir()\n        \n        # Create docs/prd.md\n        docs = repo / 'docs'\n        docs.mkdir()\n        (docs / 'prd.md').write_text('''\n# Sample Project PRD\n\n## Overview\nA simple calculator CLI application.\n\n## Requirements\n- FR1: Add two numbers\n- FR2: Subtract two numbers\n- FR3: Display help message\n\n## Success Criteria\n- All operations return correct results\n- Error handling for invalid input\n''')\n        \n        # Create source files\n        src = repo / 'src'\n        src.mkdir()\n        (src / 'calculator.py').write_text('''\ndef add(a, b):\n    return a + b\n\ndef subtract(a, b):\n    return a - b\n''')\n        (src / 'main.py').write_text('''\nfrom calculator import add, subtract\nimport sys\n\ndef main():\n    if len(sys.argv) < 4:\n        print(\"Usage: calc <add|sub> <a> <b>\")\n        return\n    op, a, b = sys.argv[1], int(sys.argv[2]), int(sys.argv[3])\n    if op == \"add\":\n        print(add(a, b))\n    elif op == \"sub\":\n        print(subtract(a, b))\n\nif __name__ == \"__main__\":\n    main()\n''')\n        \n        return repo\n    \n    def test_full_refinement_mock_mode(self, sample_repo):\n        \"\"\"Test complete refinement workflow in mock mode.\"\"\"\n        result = runner.invoke(app, [\n            'refine',\n            '--profile', 'automation_agent',\n            '--repo', str(sample_repo),\n            '--mock'\n        ])\n        \n        assert result.exit_code == 0\n        assert 'Refinement completed' in result.stdout\n        \n        # Verify plan was created\n        plan_path = sample_repo / 'docs' / 'mvp_improvement_plan.md'\n        assert plan_path.exists()\n        \n        plan_content = plan_path.read_text()\n        assert '# MVP Improvement Plan' in plan_content\n        assert 'automation_agent' in plan_content\n        assert 'Prioritized Task List' in plan_content\n        assert 'Instructions for Claude Code' in plan_content\n    \n    def test_refinement_with_internal_tool_profile(self, sample_repo):\n        \"\"\"Test refinement with internal_tool profile (fewer stages).\"\"\"\n        result = runner.invoke(app, [\n            'refine',\n            '--profile', 'internal_tool',\n            '--repo', str(sample_repo),\n            '--mock'\n        ])\n        \n        assert result.exit_code == 0\n        \n        plan_path = sample_repo / 'docs' / 'mvp_improvement_plan.md'\n        plan_content = plan_path.read_text()\n        \n        # internal_tool has fewer stages\n        assert 'alignment_with_prd' in plan_content\n        assert 'core_flow_hardening' in plan_content\n    \n    def test_refinement_fails_without_prd(self, tmp_path):\n        \"\"\"Test that refinement fails gracefully without PRD.\"\"\"\n        empty_repo = tmp_path / 'empty'\n        empty_repo.mkdir()\n        \n        result = runner.invoke(app, [\n            'refine',\n            '--profile', 'automation_agent',\n            '--repo', str(empty_repo),\n            '--mock'\n        ])\n        \n        # Should fail or warn about missing PRD\n        assert 'PRD' in result.stdout or result.exit_code != 0\n    \n    def test_list_profiles_command(self):\n        \"\"\"Test list-profiles command shows available profiles.\"\"\"\n        result = runner.invoke(app, ['list-profiles'])\n        \n        assert result.exit_code == 0\n        assert 'automation_agent' in result.stdout\n        assert 'backend_service' in result.stdout\n        assert 'internal_tool' in result.stdout\n```\n\nThis test validates:\n1. CLI invocation works correctly\n2. Mock mode bypasses API calls\n3. Plan file is generated with correct structure\n4. Different profiles produce appropriate output\n5. Error handling for missing PRD\n6. list-profiles command works",
        "testStrategy": "Run e2e tests:\n1. `pytest tests/test_e2e.py -v` to run all e2e tests\n2. Verify tests pass without any API keys configured\n3. Verify tests are isolated (use tmp_path fixtures)\n4. Verify generated plan files contain expected sections\n5. Run with `--tb=long` to debug any failures",
        "priority": "medium",
        "dependencies": [
          "7",
          "8",
          "9",
          "10"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and implement sample_repo fixture with minimal realistic project structure",
            "description": "Create a pytest fixture that builds a temporary sample repository with docs/prd.md and simple src code mirroring a realistic yet minimal project for end-to-end tests.",
            "dependencies": [],
            "details": "Implement the sample_repo fixture in tests/test_e2e.py using tmp_path to create an isolated directory tree (e.g., sample_project/docs/prd.md and src/*.py). Ensure prd.md contains PRD-style sections (Overview, Requirements, Success Criteria) and src includes a small but working CLI app (e.g., calculator main and helper module). Confirm paths match what metaagent refine expects (docs/prd.md and any default repo layout assumptions). Keep file contents minimal but sufficient for downstream PRD parsing and prompt rendering.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Write end-to-end tests invoking metaagent refine in mock mode for multiple profiles",
            "description": "Add pytest tests that use Typer’s CliRunner to invoke the metaagent CLI refine command in mock mode with different profiles, asserting exit codes and key output text.",
            "dependencies": [
              1
            ],
            "details": "In tests/test_e2e.py, use CliRunner (from typer.testing) to run app with commands like ['refine', '--profile', 'automation_agent', '--repo', str(sample_repo), '--mock']. Assert result.exit_code == 0 and that known success strings such as 'Refinement completed' (or equivalent success message) appear in result.stdout. Add a separate test for at least one additional profile (e.g., internal_tool) to ensure profile selection is wired correctly in the CLI and orchestrator. Ensure these tests rely only on mock mode (no network, no real API keys).",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Assert generation and structure of docs/mvp_improvement_plan.md for each profile",
            "description": "Verify that running refine in mock mode creates docs/mvp_improvement_plan.md and that the file contains required sections, headings, and profile-specific markers.",
            "dependencies": [
              2
            ],
            "details": "Extend the refine tests to compute plan_path = sample_repo / 'docs' / 'mvp_improvement_plan.md' and assert plan_path.exists(). Read the file and assert presence of required structural elements such as '# MVP Improvement Plan', a section describing a prioritized task list, and any expected headings like 'Prioritized Task List' and 'Instructions for Claude Code'. Also assert that the active profile name (e.g., 'automation_agent' or 'internal_tool') appears somewhere in the plan body so tests confirm profile-aware content. Keep expectations general enough that normal template copy changes do not cause brittle failures, but specific enough to validate correct plan template wiring.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add tests for failure scenarios and list-profiles CLI behavior",
            "description": "Create additional end-to-end tests covering graceful failure when PRD is missing and verifying that the list-profiles command reports all expected profiles.",
            "dependencies": [
              1,
              2
            ],
            "details": "Add a test that creates an empty temporary repo (e.g., empty_repo = tmp_path / 'empty') without docs/prd.md and runs refine in mock mode. Assert non-zero exit_code or that stdout contains a clear message mentioning missing PRD. Add a separate test invoking ['list-profiles'] and assert exit_code == 0 and that stdout contains all configured profile identifiers (e.g., automation_agent, backend_service, internal_tool). These tests should exercise the same CLI app object and use CliRunner, confirming both error handling paths and discovery/printing of available profiles.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Ensure e2e tests are isolated, fast, and optionally marked as integration tests",
            "description": "Harden the e2e test module by enforcing isolation with tmp_path fixtures, avoiding real network/API calls via mock mode, and optionally marking tests as integration for selective running.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Review tests/test_e2e.py to confirm all filesystem operations use tmp_path-based fixtures (no writes to the real project tree) and that refine is always invoked with --mock so the orchestrator never calls real external services. Optionally add a custom pytest marker like @pytest.mark.integration to the TestEndToEnd class or individual tests so they can be included/excluded via -m integration. Confirm that running pytest tests/test_e2e.py -v completes quickly and does not require any environment variables or API keys. Document any ordering dependencies on other tasks (e.g., requires CLI wiring, profiles/prompt config, and mock analysis path from tasks 7–10 to be implemented) in comments or task notes.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down Task 12 (End-to-End Integration Test with Mock Mode) into 5 subtasks: (1) design sample_repo fixture that mirrors a realistic but minimal project with docs/prd.md and simple src code; (2) write e2e tests using CliRunner to invoke `metaagent refine` in mock mode for different profiles, asserting exit codes and key output strings; (3) assert that docs/mvp_improvement_plan.md is created and contains required sections and profile info; (4) add tests for failure scenarios (e.g., missing PRD) and for list-profiles behavior; (5) ensure tests are isolated via tmp_path, run quickly using mock analysis (no network), and are optionally marked as integration tests. Highlight any ordering dependencies with other tasks/tests.",
        "updatedAt": "2025-12-14T02:00:52.678Z"
      },
      {
        "id": "13",
        "title": "Implement Repo-Agnostic Meta-Agent Architecture",
        "description": "Refactor the meta-agent to separate the meta-agent repository (CLI/configs) from the target repository (codebase being refined), making it fully repo-agnostic with configurable paths and restored batch mode profiling.",
        "details": "## Core Architecture Changes\n\n1. **CLI Refactoring** (`src/metaagent/cli.py`):\n   - Restore `--profile` option for batch mode (removed in prior versions)\n   - Add `--config-dir` option (default: `./config`) to specify prompt library location\n   - Add `--target-repo` option (default: current directory) for target codebase\n   - Add `--prd-path` option (default: `./docs/prd.md`) for PRD location\n   - Update usage: `metaagent refine --target-repo /path/to/project --config-dir ./config --profile batch --prd-path docs/prd.md`\n\n```python\n@app.command()\ndef refine(\n    target_repo: Path = typer.Argument(Path('.'), help=\"Target repository path\"),\n    config_dir: Optional[Path] = typer.Option(Path('./config'), '--config-dir', help=\"Meta-agent config directory\"),\n    prd_path: Optional[Path] = typer.Option(Path('docs/prd.md'), '--prd-path', help=\"PRD file path\"),\n    profile: str = typer.Option('default', '--profile', help=\"Profile name\"),\n    # ... other options\n):\n    config = Config.from_paths(config_dir=config_dir, target_repo=target_repo, prd_path=prd_path)\n```\n\n2. **Config System** (`src/metaagent/config.py`):\n   - Create `Config.from_paths(config_dir: Path, target_repo: Path, prd_path: Path)` factory\n   - Load prompts from `{config_dir}/prompts.yaml`\n   - Load profiles from `{config_dir}/profiles.yaml`\n   - Store `target_repo_path`, `config_dir_path`, `prd_path` as config attributes\n\n3. **Orchestrator Updates** (`src/metaagent/orchestrator.py`):\n   - Accept `Config` instance with repo-agnostic paths\n   - Run repomix/codebase-digest on `config.target_repo_path`\n   - Load PRD from `config.prd_path`\n   - Pass absolute paths to all tools/subprocesses\n\n4. **PromptLibrary** (`src/metaagent/prompts.py`):\n   - Constructor accepts `config_dir: Path`\n   - Load from `{config_dir}/prompts.yaml` instead of hardcoded `./config`\n\n5. **Tool Path Resolution**:\n   - All file operations use `config.target_repo_path / relative_path`\n   - Repomix: `repomix pack {config.target_repo_path} --output {temp_dir}/codebase.pack`\n   - Ensure all subprocess calls use absolute paths\n\n6. **Batch Mode Profile** (`config/profiles.yaml`):\n```yaml\nprofiles:\n  batch:\n    stages: ['alignment', 'planning', 'prioritization']\n    max_iterations: 1\n    auto_approve: true\n  interactive:\n    stages: ['alignment', 'planning', 'prioritization', 'execution']\n    max_iterations: 3\n    auto_approve: false\n```\n\n7. **Error Handling & Validation**:\n   - Validate `target_repo` exists and contains `docs/prd.md` (unless overridden)\n   - Validate `config_dir` contains required YAML files\n   - Clear error messages: \"Target repo not found: {target_repo}\"",
        "testStrategy": "## Comprehensive Test Strategy\n\n1. **Unit Tests** (`tests/test_repo_agnostic.py`):\n   - Test `Config.from_paths()` with various directory combinations\n   - Test `PromptLibrary(config_dir=tmp_config_dir)` loads correctly\n   - Test CLI argument parsing with all new options\n   \n2. **Integration Tests** (`tests/test_cli_repo_agnostic.py`):\n   ```python\n   def test_refine_separate_repos(tmp_path):\n       meta_config = tmp_path / 'meta-agent-config'\n       target_repo = tmp_path / 'sample-project'\n       # Setup both repos...\n       \n       result = runner.invoke(app, [\n           'refine',\n           f'--target-repo={target_repo}',\n           f'--config-dir={meta_config}',\n           '--profile=batch',\n           f'--prd-path={target_repo}/custom_prd.md'\n       ])\n       assert result.exit_code == 0\n   ```\n   \n3. **E2E Tests** (extend `tests/test_e2e.py`):\n   - Test complete workflow with separate meta-agent config dir\n   - Verify repomix runs on target repo only\n   - Verify PRD loaded from custom path\n   - Test `--profile batch` skips interactive stages\n   \n4. **Cross-Repo Validation**:\n   - Create isolated `tmp_path` fixtures for meta-config and target-repo\n   - Verify no file pollution between repositories\n   - Test with `--config-dir` in parent directory\n   \n5. **Edge Cases**:\n   - Missing target repo → clear error\n   - Invalid config dir → validation error\n   - Non-existent PRD path → error\n   - Relative/absolute paths\n   \n6. **Coverage & CI**:\n   - `pytest tests/ --cov=metaagent/config --cov=metaagent/cli`\n   - Ensure 90%+ coverage of new repo-agnostic code\n   - GitHub Actions matrix: different OS, Python versions",
        "status": "done",
        "dependencies": [
          "8",
          "9",
          "10",
          "12"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T03:04:39.134Z"
      },
      {
        "id": "14",
        "title": "Implement Iterative Triage Mode",
        "description": "Create an AI-driven triage system that replaces static profile-based prompt selection with dynamic prompt selection where Perplexity analyzes the codebase against the PRD and decides which 1-3 prompts to run per iteration.",
        "details": "## Overview\n\nThe iterative triage mode is partially implemented in `src/metaagent/orchestrator.py` (see `refine_iterative()` at line 256 and `_run_triage()` at line 437), but needs to be exposed via CLI and enhanced. The `meta_triage.md` prompt already exists at `config/prompt_library/meta_triage.md`.\n\n## Implementation Steps\n\n### 1. Add CLI Command for Iterative Mode\n\nUpdate `src/metaagent/cli.py` to add a new command or option:\n\n```python\n@app.command(\"refine-iterative\")\ndef refine_iterative(\n    repo: Path = typer.Option(\n        Path.cwd(),\n        \"--repo\",\n        \"-r\",\n        help=\"Path to the target repository to refine.\",\n    ),\n    prd: Optional[Path] = typer.Option(\n        None,\n        \"--prd\",\n        help=\"Path to PRD file (default: docs/prd.md in target repo).\",\n    ),\n    config_dir: Optional[Path] = typer.Option(\n        None,\n        \"--config-dir\",\n        \"-c\",\n        help=\"Path to meta-agent config directory containing prompts/profiles.\",\n    ),\n    max_iterations: int = typer.Option(\n        10,\n        \"--max-iterations\",\n        \"-n\",\n        help=\"Maximum number of triage iterations to run.\",\n    ),\n    mock: bool = typer.Option(\n        False,\n        \"--mock\",\n        \"-m\",\n        help=\"Run in mock mode (no API calls).\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\",\n        help=\"Enable verbose output.\",\n    ),\n) -> None:\n    \"\"\"Run iterative refinement with AI-driven triage.\n\n    Instead of running a fixed profile, this mode lets the AI decide which\n    prompts to run based on analyzing the codebase against the PRD.\n    \"\"\"\n    # Similar setup to refine() command\n    # Call orchestrator.refine_iterative(max_iterations)\n```\n\n### 2. Enhance TriageResult Data Class\n\nThe existing `TriageResult` at `orchestrator.py:51` is already well-structured:\n\n```python\n@dataclass\nclass TriageResult:\n    success: bool\n    done: bool = False\n    assessment: str = \"\"\n    priority_issues: list[str] = field(default_factory=list)\n    selected_prompts: list[str] = field(default_factory=list)\n    reasoning: str = \"\"\n    error: Optional[str] = None\n```\n\nConsider adding:\n- `confidence: float = 1.0` - AI's confidence in the selection\n- `iteration_context: str = \"\"` - Context about what was done in previous iterations\n\n### 3. Enhance the `_run_triage()` Method\n\nThe existing implementation at `orchestrator.py:437` parses JSON responses. Enhance it to:\n\n1. **Validate selected prompts exist** - Check each prompt_id against `self.prompt_library`\n2. **Limit to 1-3 prompts** - Enforce the prompt limit in code:\n```python\nif len(selected_prompts) > 3:\n    logger.warning(f\"Triage selected {len(selected_prompts)} prompts, limiting to 3\")\n    selected_prompts = selected_prompts[:3]\n```\n3. **Add fallback for common AI response variations** - Handle cases where AI returns prompt names slightly different from IDs\n\n### 4. Update the `meta_triage.md` Prompt\n\nThe current prompt at `config/prompt_library/meta_triage.md` is well-structured. Consider enhancing:\n\n1. Add the full list of available prompts dynamically from the prompt library\n2. Include previous iteration results in the context\n3. Add clearer termination criteria for the \"done\" flag\n\n### 5. Add Iteration Tracking to Output\n\nUpdate `RefinementResult` at `orchestrator.py:76` (already has `iterations: list[IterationResult]`) to include:\n- Which prompts were run each iteration\n- Why they were selected (from triage reasoning)\n- Whether changes were made\n\n### 6. Enhance Rich Console Output\n\nAdd iteration progress display in CLI:\n```python\nwith console.status(\"[bold blue]Running triage...\") as status:\n    for iteration in range(1, max_iterations + 1):\n        status.update(f\"[bold blue]Iteration {iteration}/{max_iterations}: Analyzing codebase...\")\n        # Display selected prompts\n        console.print(f\"[cyan]Selected prompts:[/cyan] {', '.join(triage_result.selected_prompts)}\")\n        console.print(f\"[dim]Reasoning:[/dim] {triage_result.reasoning}\")\n```\n\n### 7. Add MockAnalysisEngine Support for Triage\n\nUpdate `MockAnalysisEngine` in `src/metaagent/analysis.py` to return proper triage JSON responses when the prompt contains \"meta_triage\":\n\n```python\ndef analyze(self, prompt: str) -> AnalysisResult:\n    if \"meta_triage\" in prompt.lower() or \"triage\" in prompt.lower():\n        return AnalysisResult(\n            summary=json.dumps({\n                \"assessment\": \"Mock triage assessment\",\n                \"priority_issues\": [\"Mock issue 1\"],\n                \"selected_prompts\": [\"quality_error_analysis\"],\n                \"reasoning\": \"Mock reasoning for testing\",\n                \"done\": self.call_count >= 3  # Simulate done after 3 iterations\n            }),\n            success=True,\n        )\n    # ... rest of existing mock logic\n```\n\n## Files to Modify\n\n1. `src/metaagent/cli.py` - Add `refine-iterative` command\n2. `src/metaagent/orchestrator.py` - Enhance `_run_triage()` validation\n3. `src/metaagent/analysis.py` - Add triage-aware mock responses\n4. `config/prompt_library/meta_triage.md` - Optional enhancements to prompt\n5. `tests/test_cli.py` - Add tests for iterative mode CLI\n6. `tests/test_orchestrator.py` (new or existing) - Add tests for triage logic\n\n## Dependencies\n\nThis task depends on:\n- Task 7 (Orchestrator) - Provides the base `refine_iterative()` and `_run_triage()` methods\n- Task 5 (Analysis Engine) - Provides `AnalysisEngine` and `MockAnalysisEngine`\n- Task 3 (Prompt Loading) - Provides `PromptLibrary` for prompt validation\n- Task 8 (CLI) - Provides the CLI infrastructure to add new command",
        "testStrategy": "## Unit Tests\n\n### 1. Test CLI Command (`tests/test_cli.py`)\n```python\ndef test_refine_iterative_help():\n    result = runner.invoke(app, [\"refine-iterative\", \"--help\"])\n    assert result.exit_code == 0\n    assert \"--max-iterations\" in result.stdout\n\ndef test_refine_iterative_mock_mode(mock_config):\n    with patch(\"metaagent.cli.Orchestrator\") as mock_orch:\n        mock_instance = mock_orch.return_value\n        mock_instance.refine_iterative.return_value = RefinementResult(\n            success=True,\n            profile_name=\"iterative\",\n            stages_completed=3,\n            stages_failed=0,\n            iterations=[...],\n        )\n        result = runner.invoke(app, [\n            \"refine-iterative\",\n            \"--repo\", str(mock_config.repo_path),\n            \"--config-dir\", str(mock_config.config_dir),\n            \"--mock\",\n            \"--max-iterations\", \"5\",\n        ])\n        assert result.exit_code == 0\n        mock_instance.refine_iterative.assert_called_once_with(5)\n```\n\n### 2. Test Triage Result Parsing (`tests/test_orchestrator.py`)\n```python\ndef test_run_triage_parses_json_response():\n    # Mock analysis engine to return valid triage JSON\n    orchestrator = Orchestrator(config, analysis_engine=mock_engine)\n    result = orchestrator._run_triage(prd, code_context, history)\n    assert result.success\n    assert len(result.selected_prompts) <= 3\n    assert isinstance(result.done, bool)\n\ndef test_run_triage_validates_prompt_ids():\n    # Selected prompts should exist in prompt library\n    result = orchestrator._run_triage(...)\n    for prompt_id in result.selected_prompts:\n        assert orchestrator.prompt_library.get_prompt(prompt_id) is not None\n\ndef test_run_triage_handles_done_flag():\n    # When done=true, selected_prompts should be empty\n    mock_engine.responses[\"triage\"] = AnalysisResult(\n        summary='{\"done\": true, \"selected_prompts\": [], ...}',\n        success=True,\n    )\n    result = orchestrator._run_triage(...)\n    assert result.done is True\n    assert result.selected_prompts == []\n```\n\n### 3. Test Iterative Loop (`tests/test_orchestrator.py`)\n```python\ndef test_refine_iterative_stops_on_done():\n    # Should stop when triage returns done=true\n    orchestrator = Orchestrator(config, analysis_engine=mock_engine)\n    mock_engine.set_done_after_iterations(2)\n    result = orchestrator.refine_iterative(max_iterations=10)\n    assert result.success\n    assert len(result.iterations) == 2\n\ndef test_refine_iterative_respects_max_iterations():\n    # Should not exceed max_iterations\n    result = orchestrator.refine_iterative(max_iterations=3)\n    assert len(result.iterations) <= 3\n\ndef test_refine_iterative_tracks_prompts_per_iteration():\n    result = orchestrator.refine_iterative(max_iterations=5)\n    for iteration in result.iterations:\n        assert len(iteration.prompts_run) >= 1\n        assert len(iteration.prompts_run) <= 3\n```\n\n### 4. Test Mock Mode for Triage\n```python\ndef test_mock_engine_returns_triage_response():\n    engine = MockAnalysisEngine()\n    result = engine.analyze(\"... meta_triage ...\")\n    assert result.success\n    data = json.loads(result.summary)\n    assert \"selected_prompts\" in data\n    assert \"done\" in data\n```\n\n### 5. Integration Test\n```python\ndef test_full_iterative_workflow(mock_config, tmp_path):\n    \"\"\"End-to-end test of iterative refinement.\"\"\"\n    # Create target repo with PRD\n    target = tmp_path / \"target\"\n    target.mkdir()\n    (target / \"docs\").mkdir()\n    (target / \"docs\" / \"prd.md\").write_text(\"# Test PRD\")\n    (target / \"src\").mkdir()\n    (target / \"src\" / \"main.py\").write_text(\"def main(): pass\")\n\n    result = runner.invoke(app, [\n        \"refine-iterative\",\n        \"--repo\", str(target),\n        \"--config-dir\", str(mock_config.config_dir),\n        \"--mock\",\n        \"--max-iterations\", \"3\",\n    ])\n    assert result.exit_code == 0\n    assert \"iteration\" in result.stdout.lower()\n```\n\n## Manual Verification\n\n1. Run `metaagent refine-iterative --mock --max-iterations 3 --verbose` on meta-agent itself\n2. Verify triage output shows assessment, selected prompts, and reasoning\n3. Verify iteration progress is displayed clearly\n4. Verify loop terminates on \"done\" flag or max iterations\n5. Verify improvement plan includes all iteration results",
        "status": "done",
        "dependencies": [
          "3",
          "5",
          "7",
          "8"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-14T03:23:29.843Z"
      },
      {
        "id": "16",
        "title": "Implement Claude Code Integration",
        "description": "Replace the stub `_implement_with_claude()` method in orchestrator.py with actual Claude Code CLI integration via subprocess to automatically implement recommended changes from the analysis stage. Handle errors gracefully and report implementation status.",
        "details": "## Overview\n\nThe `_implement_with_claude()` method at `src/metaagent/orchestrator.py:510` currently only writes tasks to a file and returns True without actually invoking Claude Code. This task implements full subprocess integration with the Claude Code CLI.\n\n## Implementation Steps\n\n### 1. Create ClaudeCodeRunner Module (`src/metaagent/claude_runner.py`)\n\n```python\n\"\"\"Claude Code CLI integration for automated implementation.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Optional, Any\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ClaudeCodeResult:\n    \"\"\"Result from a Claude Code execution.\"\"\"\n    \n    success: bool\n    output: str = \"\"\n    error: Optional[str] = None\n    files_modified: list[str] = field(default_factory=list)\n    exit_code: int = 0\n\n\nclass ClaudeCodeRunner:\n    \"\"\"Runs Claude Code CLI to implement changes in a repository.\"\"\"\n    \n    def __init__(\n        self,\n        timeout: int = 600,  # 10 minutes default for implementation\n        model: str = \"claude-sonnet-4-20250514\",\n        max_turns: int = 50,\n    ):\n        \"\"\"Initialize the Claude Code runner.\n        \n        Args:\n            timeout: Maximum time in seconds for Claude Code to run.\n            model: Claude model to use for implementation.\n            max_turns: Maximum conversation turns for the agentic loop.\n        \"\"\"\n        self.timeout = timeout\n        self.model = model\n        self.max_turns = max_turns\n    \n    def check_installed(self) -> bool:\n        \"\"\"Check if Claude Code CLI is installed and accessible.\n        \n        Returns:\n            True if claude command is available, False otherwise.\n        \"\"\"\n        try:\n            result = subprocess.run(\n                [\"claude\", \"--version\"],\n                capture_output=True,\n                text=True,\n                timeout=10,\n            )\n            return result.returncode == 0\n        except (FileNotFoundError, subprocess.TimeoutExpired):\n            return False\n    \n    def implement(\n        self,\n        repo_path: Path,\n        prompt: str,\n        plan_file: Optional[Path] = None,\n    ) -> ClaudeCodeResult:\n        \"\"\"Run Claude Code to implement changes.\n        \n        Args:\n            repo_path: Path to the target repository.\n            prompt: Implementation prompt to send to Claude Code.\n            plan_file: Optional path to the improvement plan file.\n        \n        Returns:\n            ClaudeCodeResult with execution outcome.\n        \"\"\"\n        if not self.check_installed():\n            return ClaudeCodeResult(\n                success=False,\n                error=\"Claude Code CLI not installed. Install with: npm install -g @anthropic-ai/claude-code\",\n                exit_code=-1,\n            )\n        \n        # Build the implementation prompt\n        full_prompt = self._build_prompt(prompt, plan_file)\n        \n        try:\n            # Run Claude Code in non-interactive mode with the prompt\n            result = subprocess.run(\n                [\n                    \"claude\",\n                    \"--print\",  # Non-interactive mode, output only\n                    \"--model\", self.model,\n                    \"--max-turns\", str(self.max_turns),\n                    \"--dangerously-skip-permissions\",  # Auto-approve for automation\n                    \"-p\", full_prompt,  # Pass prompt directly\n                ],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=self.timeout,\n            )\n            \n            if result.returncode == 0:\n                # Parse output to find modified files\n                files_modified = self._parse_modified_files(result.stdout, repo_path)\n                \n                return ClaudeCodeResult(\n                    success=True,\n                    output=result.stdout,\n                    files_modified=files_modified,\n                    exit_code=result.returncode,\n                )\n            else:\n                return ClaudeCodeResult(\n                    success=False,\n                    output=result.stdout,\n                    error=result.stderr or f\"Claude Code exited with code {result.returncode}\",\n                    exit_code=result.returncode,\n                )\n                \n        except subprocess.TimeoutExpired:\n            return ClaudeCodeResult(\n                success=False,\n                error=f\"Claude Code timed out after {self.timeout} seconds\",\n                exit_code=-1,\n            )\n        except FileNotFoundError:\n            return ClaudeCodeResult(\n                success=False,\n                error=\"Claude Code CLI not found in PATH\",\n                exit_code=-1,\n            )\n        except Exception as e:\n            return ClaudeCodeResult(\n                success=False,\n                error=f\"Unexpected error running Claude Code: {e}\",\n                exit_code=-1,\n            )\n    \n    def _build_prompt(self, prompt: str, plan_file: Optional[Path]) -> str:\n        \"\"\"Build the full implementation prompt.\n        \n        Args:\n            prompt: Base implementation prompt.\n            plan_file: Optional path to improvement plan.\n        \n        Returns:\n            Full prompt string for Claude Code.\n        \"\"\"\n        parts = []\n        \n        if plan_file and plan_file.exists():\n            parts.append(f\"Read the improvement plan at {plan_file} for context.\")\n        \n        parts.append(prompt)\n        parts.append(\n            \"\\nAfter implementing each task:\\n\"\n            \"1. Run relevant tests to verify changes work\\n\"\n            \"2. Fix any issues before moving to the next task\\n\"\n            \"3. Keep changes focused and incremental\"\n        )\n        \n        return \"\\n\\n\".join(parts)\n    \n    def _parse_modified_files(self, output: str, repo_path: Path) -> list[str]:\n        \"\"\"Parse Claude Code output to identify modified files.\n        \n        Args:\n            output: Claude Code stdout.\n            repo_path: Repository path for checking file existence.\n        \n        Returns:\n            List of modified file paths relative to repo.\n        \"\"\"\n        modified = []\n        \n        # Check git status for actual modifications\n        try:\n            result = subprocess.run(\n                [\"git\", \"diff\", \"--name-only\", \"HEAD\"],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=10,\n            )\n            if result.returncode == 0:\n                modified = [f.strip() for f in result.stdout.strip().split(\"\\n\") if f.strip()]\n        except Exception:\n            pass\n        \n        return modified\n```\n\n### 2. Update Config (`src/metaagent/config.py`)\n\nAdd new configuration options:\n\n```python\n# Add to Config dataclass\nclaude_code_timeout: int = 600  # 10 minutes\nclaude_code_model: str = \"claude-sonnet-4-20250514\"\nclaude_code_max_turns: int = 50\nauto_implement: bool = False  # Whether to auto-invoke Claude Code\n\n# Add to from_env():\nclaude_code_timeout=int(os.getenv(\"METAAGENT_CLAUDE_TIMEOUT\", \"600\")),\nclaude_code_model=os.getenv(\"METAAGENT_CLAUDE_MODEL\", \"claude-sonnet-4-20250514\"),\nclaude_code_max_turns=int(os.getenv(\"METAAGENT_CLAUDE_MAX_TURNS\", \"50\")),\nauto_implement=os.getenv(\"METAAGENT_AUTO_IMPLEMENT\", \"\").lower() in (\"true\", \"1\", \"yes\"),\n```\n\n### 3. Update Orchestrator (`src/metaagent/orchestrator.py`)\n\nReplace `_implement_with_claude()` method:\n\n```python\nfrom .claude_runner import ClaudeCodeRunner, ClaudeCodeResult\n\n# Add to __init__():\nself.claude_runner = claude_runner or ClaudeCodeRunner(\n    timeout=config.claude_code_timeout,\n    model=config.claude_code_model,\n    max_turns=config.claude_code_max_turns,\n)\n\n# Replace _implement_with_claude():\ndef _implement_with_claude(self, stage_results: list[StageResult]) -> bool:\n    \"\"\"Implement changes using Claude Code CLI.\n    \n    Args:\n        stage_results: Results from the analysis stage.\n    \n    Returns:\n        True if changes were made, False otherwise.\n    \"\"\"\n    if not stage_results:\n        return False\n    \n    # Build implementation prompt\n    tasks = []\n    for result in stage_results:\n        for task in result.tasks:\n            tasks.append(task)\n    \n    if not tasks:\n        logger.info(\"No tasks to implement\")\n        return False\n    \n    # Create implementation prompt\n    implementation_prompt = \"Implement the following improvements:\\n\\n\"\n    for i, task in enumerate(tasks, 1):\n        if isinstance(task, dict):\n            title = task.get(\"title\", \"\")\n            desc = task.get(\"description\", str(task))\n            file_ref = task.get(\"file\", \"\")\n            implementation_prompt += f\"{i}. {title}\\n\"\n            if desc:\n                implementation_prompt += f\"   Description: {desc}\\n\"\n            if file_ref:\n                implementation_prompt += f\"   File: {file_ref}\\n\"\n        else:\n            implementation_prompt += f\"{i}. {task}\\n\"\n    \n    # Write tasks to file for reference\n    prompt_file = self.config.repo_path / \".meta-agent-tasks.md\"\n    prompt_file.write_text(implementation_prompt, encoding=\"utf-8\")\n    logger.info(f\"Implementation tasks written to: {prompt_file}\")\n    \n    # Check if auto-implementation is enabled\n    if not self.config.auto_implement:\n        logger.info(\"Auto-implementation disabled. Run Claude Code manually.\")\n        logger.info(\"To enable, set METAAGENT_AUTO_IMPLEMENT=true or --auto-implement flag\")\n        return True  # Tasks written, considered success\n    \n    # Check if Claude Code is available\n    if not self.claude_runner.check_installed():\n        logger.warning(\"Claude Code CLI not installed. Tasks written to file for manual implementation.\")\n        return True\n    \n    # Run Claude Code\n    logger.info(\"Running Claude Code to implement changes...\")\n    result = self.claude_runner.implement(\n        repo_path=self.config.repo_path,\n        prompt=implementation_prompt,\n        plan_file=self.config.repo_path / \"docs\" / \"mvp_improvement_plan.md\",\n    )\n    \n    if result.success:\n        logger.info(f\"Claude Code completed successfully\")\n        if result.files_modified:\n            logger.info(f\"Files modified: {', '.join(result.files_modified)}\")\n        return True\n    else:\n        logger.error(f\"Claude Code failed: {result.error}\")\n        # Still return True because tasks were written for manual implementation\n        return True\n```\n\n### 4. Update CLI (`src/metaagent/cli.py`)\n\nAdd `--auto-implement` flag to refine command:\n\n```python\n@app.command()\ndef refine(\n    # ... existing options ...\n    auto_implement: bool = typer.Option(\n        False,\n        \"--auto-implement\",\n        \"-a\",\n        help=\"Automatically invoke Claude Code to implement changes.\",\n    ),\n) -> None:\n    # ... existing code ...\n    \n    if auto_implement:\n        config.auto_implement = True\n```\n\n### 5. Add Result Tracking to IterationResult\n\nUpdate `IterationResult` dataclass:\n\n```python\n@dataclass\nclass IterationResult:\n    \"\"\"Result from a single iteration of the refinement loop.\"\"\"\n    \n    iteration: int\n    prompts_run: list[str]\n    changes_made: bool\n    committed: bool\n    commit_hash: Optional[str] = None\n    stage_results: list[StageResult] = field(default_factory=list)\n    implementation_result: Optional[ClaudeCodeResult] = None  # NEW\n```\n\n### 6. Error Handling Strategy\n\n- **Claude Code not installed**: Log warning, fall back to file-based workflow\n- **Timeout**: Log error with duration, return partial success if tasks written\n- **Non-zero exit code**: Log stderr, consider as failure but preserve output\n- **Permission errors**: Suggest using `--dangerously-skip-permissions` flag\n- **Network errors**: Retry once, then fall back to manual mode\n\n### 7. Status Reporting\n\nAdd implementation status to the improvement plan:\n\n```python\ndef _generate_implementation_status(self, result: ClaudeCodeResult) -> str:\n    \"\"\"Generate status section for the improvement plan.\"\"\"\n    if result.success:\n        return f\"\"\"## Implementation Status\n\n**Status:** ✅ Implemented automatically\n**Files Modified:** {len(result.files_modified)}\n\nModified files:\n{chr(10).join(f'- {f}' for f in result.files_modified)}\n\"\"\"\n    else:\n        return f\"\"\"## Implementation Status\n\n**Status:** ⚠️ Manual implementation required\n**Reason:** {result.error}\n\nTasks have been written to `.meta-agent-tasks.md` for manual implementation.\n\"\"\"\n```\n\n## Key Design Decisions\n\n1. **Non-interactive mode**: Use `--print` flag for scripted execution\n2. **Permission handling**: Use `--dangerously-skip-permissions` for fully automated runs\n3. **Graceful degradation**: Always fall back to file-based workflow if Claude Code unavailable\n4. **Timeout safety**: 10-minute default timeout with configurable override\n5. **Git integration**: Track modified files via git diff for accurate reporting",
        "testStrategy": "## Unit Tests (`tests/test_claude_runner.py`)\n\n### 1. Test ClaudeCodeRunner Initialization\n```python\ndef test_claude_runner_defaults():\n    runner = ClaudeCodeRunner()\n    assert runner.timeout == 600\n    assert runner.model == \"claude-sonnet-4-20250514\"\n    assert runner.max_turns == 50\n\ndef test_claude_runner_custom_config():\n    runner = ClaudeCodeRunner(timeout=300, model=\"claude-opus-4\", max_turns=20)\n    assert runner.timeout == 300\n    assert runner.model == \"claude-opus-4\"\n    assert runner.max_turns == 20\n```\n\n### 2. Test check_installed()\n```python\ndef test_check_installed_success(mocker):\n    mocker.patch(\"subprocess.run\", return_value=Mock(returncode=0))\n    runner = ClaudeCodeRunner()\n    assert runner.check_installed() is True\n\ndef test_check_installed_not_found(mocker):\n    mocker.patch(\"subprocess.run\", side_effect=FileNotFoundError())\n    runner = ClaudeCodeRunner()\n    assert runner.check_installed() is False\n\ndef test_check_installed_timeout(mocker):\n    mocker.patch(\"subprocess.run\", side_effect=subprocess.TimeoutExpired(\"claude\", 10))\n    runner = ClaudeCodeRunner()\n    assert runner.check_installed() is False\n```\n\n### 3. Test implement() Success\n```python\ndef test_implement_success(mocker, tmp_path):\n    mock_run = mocker.patch(\"subprocess.run\")\n    mock_run.side_effect = [\n        Mock(returncode=0),  # check_installed\n        Mock(returncode=0, stdout=\"Success\", stderr=\"\"),  # implement\n        Mock(returncode=0, stdout=\"file1.py\\nfile2.py\"),  # git diff\n    ]\n    \n    runner = ClaudeCodeRunner()\n    result = runner.implement(tmp_path, \"Fix bugs\")\n    \n    assert result.success is True\n    assert result.error is None\n    assert \"file1.py\" in result.files_modified\n```\n\n### 4. Test implement() Failure Cases\n```python\ndef test_implement_not_installed(mocker, tmp_path):\n    mocker.patch.object(ClaudeCodeRunner, \"check_installed\", return_value=False)\n    runner = ClaudeCodeRunner()\n    result = runner.implement(tmp_path, \"Fix bugs\")\n    \n    assert result.success is False\n    assert \"not installed\" in result.error.lower()\n\ndef test_implement_timeout(mocker, tmp_path):\n    mocker.patch.object(ClaudeCodeRunner, \"check_installed\", return_value=True)\n    mocker.patch(\"subprocess.run\", side_effect=subprocess.TimeoutExpired(\"claude\", 600))\n    \n    runner = ClaudeCodeRunner(timeout=600)\n    result = runner.implement(tmp_path, \"Fix bugs\")\n    \n    assert result.success is False\n    assert \"timeout\" in result.error.lower()\n\ndef test_implement_nonzero_exit(mocker, tmp_path):\n    mocker.patch.object(ClaudeCodeRunner, \"check_installed\", return_value=True)\n    mocker.patch(\"subprocess.run\", return_value=Mock(\n        returncode=1, stdout=\"\", stderr=\"API error\"\n    ))\n    \n    runner = ClaudeCodeRunner()\n    result = runner.implement(tmp_path, \"Fix bugs\")\n    \n    assert result.success is False\n    assert result.exit_code == 1\n```\n\n### 5. Test _build_prompt()\n```python\ndef test_build_prompt_with_plan_file(tmp_path):\n    plan_file = tmp_path / \"plan.md\"\n    plan_file.write_text(\"# Plan\")\n    \n    runner = ClaudeCodeRunner()\n    prompt = runner._build_prompt(\"Fix bugs\", plan_file)\n    \n    assert \"plan.md\" in prompt\n    assert \"Fix bugs\" in prompt\n    assert \"Run relevant tests\" in prompt\n\ndef test_build_prompt_without_plan_file():\n    runner = ClaudeCodeRunner()\n    prompt = runner._build_prompt(\"Fix bugs\", None)\n    \n    assert \"Fix bugs\" in prompt\n    assert \"plan.md\" not in prompt\n```\n\n## Integration Tests (`tests/test_orchestrator_claude.py`)\n\n### 1. Test _implement_with_claude() with Auto-Implement Disabled\n```python\ndef test_implement_with_claude_auto_disabled(mock_orchestrator, tmp_path):\n    mock_orchestrator.config.auto_implement = False\n    stage_results = [StageResult(\n        stage_id=\"test\",\n        stage_name=\"Test\",\n        summary=\"Test\",\n        tasks=[{\"title\": \"Task 1\", \"description\": \"Do something\"}],\n    )]\n    \n    result = mock_orchestrator._implement_with_claude(stage_results)\n    \n    assert result is True\n    assert (tmp_path / \".meta-agent-tasks.md\").exists()\n```\n\n### 2. Test _implement_with_claude() with Auto-Implement Enabled\n```python\ndef test_implement_with_claude_auto_enabled(mock_orchestrator, mocker, tmp_path):\n    mock_orchestrator.config.auto_implement = True\n    mocker.patch.object(\n        mock_orchestrator.claude_runner,\n        \"implement\",\n        return_value=ClaudeCodeResult(success=True, files_modified=[\"src/test.py\"]),\n    )\n    \n    stage_results = [StageResult(\n        stage_id=\"test\",\n        stage_name=\"Test\",\n        summary=\"Test\",\n        tasks=[{\"title\": \"Task 1\"}],\n    )]\n    \n    result = mock_orchestrator._implement_with_claude(stage_results)\n    \n    assert result is True\n```\n\n### 3. Test Empty Tasks\n```python\ndef test_implement_with_claude_no_tasks(mock_orchestrator):\n    result = mock_orchestrator._implement_with_claude([])\n    assert result is False\n\ndef test_implement_with_claude_empty_stage_results(mock_orchestrator):\n    stage_results = [StageResult(\n        stage_id=\"test\",\n        stage_name=\"Test\",\n        summary=\"Test\",\n        tasks=[],\n    )]\n    result = mock_orchestrator._implement_with_claude(stage_results)\n    assert result is False\n```\n\n## CLI Tests (`tests/test_cli.py`)\n\n### 1. Test --auto-implement Flag\n```python\ndef test_refine_auto_implement_flag(runner, mock_config):\n    result = runner.invoke(app, [\n        \"refine\",\n        \"--profile\", \"automation_agent\",\n        \"--mock\",\n        \"--auto-implement\",\n    ])\n    # Should not fail even if Claude Code not installed\n    assert result.exit_code == 0\n\ndef test_refine_without_auto_implement(runner, mock_config):\n    result = runner.invoke(app, [\n        \"refine\",\n        \"--profile\", \"automation_agent\",\n        \"--mock\",\n    ])\n    assert result.exit_code == 0\n    assert \"manual\" in result.stdout.lower() or \"Claude Code\" in result.stdout\n```\n\n## End-to-End Test\n\n### 1. Full Workflow Test (requires Claude Code installed)\n```python\n@pytest.mark.skipif(not shutil.which(\"claude\"), reason=\"Claude Code not installed\")\ndef test_full_implementation_workflow(tmp_path):\n    # Create minimal test repo\n    (tmp_path / \"src\").mkdir()\n    (tmp_path / \"src\" / \"main.py\").write_text(\"# TODO: implement\")\n    (tmp_path / \"docs\").mkdir()\n    (tmp_path / \"docs\" / \"prd.md\").write_text(\"# PRD\\nAdd hello function\")\n    \n    # Initialize git\n    subprocess.run([\"git\", \"init\"], cwd=tmp_path)\n    subprocess.run([\"git\", \"add\", \"-A\"], cwd=tmp_path)\n    subprocess.run([\"git\", \"commit\", \"-m\", \"init\"], cwd=tmp_path)\n    \n    # Run implementation\n    runner = ClaudeCodeRunner(timeout=120)\n    result = runner.implement(\n        tmp_path,\n        \"Add a hello() function that returns 'Hello, World!' to src/main.py\",\n    )\n    \n    assert result.success is True\n    # Verify file was modified\n    content = (tmp_path / \"src\" / \"main.py\").read_text()\n    assert \"hello\" in content.lower()\n```\n\n## Manual Verification Steps\n\n1. Install Claude Code: `npm install -g @anthropic-ai/claude-code`\n2. Run: `metaagent refine --profile automation_agent --auto-implement`\n3. Verify Claude Code is invoked and implements at least one task\n4. Verify error handling when Claude Code is not installed\n5. Verify timeout handling with a complex task",
        "status": "done",
        "dependencies": [
          "5",
          "6",
          "7",
          "8"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-14T03:29:20.657Z"
      },
      {
        "id": "17",
        "title": "Implement Auto-Commit After Implementation",
        "description": "Enhance the orchestrator's git integration to automatically stage and commit changes after Claude Code implements improvements, with descriptive commit messages referencing the analysis prompt, and options to skip commits or provide custom messages.",
        "details": "## Overview\n\nThis task extends the existing `_commit_changes()` method in `src/metaagent/orchestrator.py:553` and integrates it with the Claude Code implementation flow from Task 16. The current implementation is basic and lacks flexibility. This task adds configurable commit behavior, detailed commit messages, and robust error handling.\n\n## Implementation Steps\n\n### 1. Create CommitConfig Dataclass (`src/metaagent/config.py`)\n\nAdd commit-related configuration options:\n\n```python\n@dataclass\nclass CommitConfig:\n    \"\"\"Configuration for auto-commit behavior.\"\"\"\n    enabled: bool = True  # Whether to auto-commit\n    push_enabled: bool = False  # Whether to push after commit (conservative default)\n    custom_message: Optional[str] = None  # Override commit message\n    include_prompt_reference: bool = True  # Include prompt ID in commit message\n    sign_commits: bool = False  # Enable GPG signing\n    \n    @classmethod\n    def from_env(cls) -> 'CommitConfig':\n        \"\"\"Load from environment variables.\"\"\"\n        return cls(\n            enabled=os.getenv(\"METAAGENT_AUTO_COMMIT\", \"true\").lower() in (\"true\", \"1\"),\n            push_enabled=os.getenv(\"METAAGENT_AUTO_PUSH\", \"false\").lower() in (\"true\", \"1\"),\n            sign_commits=os.getenv(\"METAAGENT_SIGN_COMMITS\", \"false\").lower() in (\"true\", \"1\"),\n        )\n```\n\nUpdate the main `Config` class to include `commit_config: CommitConfig`.\n\n### 2. Create CommitResult Dataclass (`src/metaagent/orchestrator.py`)\n\nAdd near other dataclasses (around line 75):\n\n```python\n@dataclass\nclass CommitResult:\n    \"\"\"Result from a git commit operation.\"\"\"\n    success: bool\n    commit_hash: Optional[str] = None\n    pushed: bool = False\n    error: Optional[str] = None\n    skipped: bool = False\n    skip_reason: Optional[str] = None\n    files_staged: int = 0\n```\n\n### 3. Enhance _commit_changes Method\n\nReplace the existing method at `src/metaagent/orchestrator.py:553` with enhanced version:\n\n```python\ndef _commit_changes(\n    self,\n    message: str,\n    prompt_ids: list[str],\n    skip_commit: bool = False,\n    custom_message: Optional[str] = None,\n) -> CommitResult:\n    \"\"\"Commit changes to git with descriptive message.\n\n    Args:\n        message: Base commit message (e.g., iteration summary).\n        prompt_ids: List of prompt IDs that generated the changes.\n        skip_commit: If True, skip the commit entirely.\n        custom_message: Override the generated commit message.\n\n    Returns:\n        CommitResult with details about the operation.\n    \"\"\"\n    if skip_commit or not self.config.commit_config.enabled:\n        return CommitResult(\n            success=True,\n            skipped=True,\n            skip_reason=\"Auto-commit disabled\" if not self.config.commit_config.enabled else \"User requested skip\"\n        )\n\n    try:\n        # Check for uncommitted changes\n        status_result = subprocess.run(\n            [\"git\", \"status\", \"--porcelain\"],\n            cwd=self.config.repo_path,\n            capture_output=True,\n            text=True,\n            timeout=30,\n        )\n\n        if not status_result.stdout.strip():\n            return CommitResult(\n                success=True,\n                skipped=True,\n                skip_reason=\"No changes to commit\"\n            )\n\n        # Count files to be staged\n        changed_files = [line for line in status_result.stdout.strip().split('\\n') if line]\n        \n        # Stage all changes\n        subprocess.run(\n            [\"git\", \"add\", \"-A\"],\n            cwd=self.config.repo_path,\n            check=True,\n            timeout=30,\n        )\n\n        # Build commit message\n        if custom_message:\n            commit_message = custom_message\n        else:\n            commit_message = self._build_commit_message(message, prompt_ids)\n\n        # Prepare commit command\n        commit_cmd = [\"git\", \"commit\", \"-m\", commit_message]\n        if self.config.commit_config.sign_commits:\n            commit_cmd.append(\"-S\")\n\n        subprocess.run(\n            commit_cmd,\n            cwd=self.config.repo_path,\n            check=True,\n            timeout=60,\n        )\n\n        # Get commit hash\n        hash_result = subprocess.run(\n            [\"git\", \"rev-parse\", \"--short\", \"HEAD\"],\n            cwd=self.config.repo_path,\n            capture_output=True,\n            text=True,\n            timeout=10,\n        )\n        commit_hash = hash_result.stdout.strip()\n\n        # Optionally push\n        pushed = False\n        if self.config.commit_config.push_enabled:\n            try:\n                subprocess.run(\n                    [\"git\", \"push\"],\n                    cwd=self.config.repo_path,\n                    check=True,\n                    timeout=120,\n                )\n                pushed = True\n            except subprocess.CalledProcessError as e:\n                logger.warning(f\"Push failed (commit still made): {e}\")\n\n        logger.info(f\"Committed changes: {commit_hash}\")\n        return CommitResult(\n            success=True,\n            commit_hash=commit_hash,\n            pushed=pushed,\n            files_staged=len(changed_files),\n        )\n\n    except subprocess.TimeoutExpired:\n        return CommitResult(success=False, error=\"Git operation timed out\")\n    except subprocess.CalledProcessError as e:\n        return CommitResult(success=False, error=f\"Git operation failed: {e.stderr or e}\")\n    except FileNotFoundError:\n        return CommitResult(success=False, error=\"Git not found in PATH\")\n```\n\n### 4. Add Commit Message Builder Method\n\nAdd helper method to orchestrator:\n\n```python\ndef _build_commit_message(self, message: str, prompt_ids: list[str]) -> str:\n    \"\"\"Build a descriptive commit message referencing analysis prompts.\n\n    Args:\n        message: Base message (e.g., iteration summary).\n        prompt_ids: List of prompt IDs that generated the changes.\n\n    Returns:\n        Formatted commit message.\n    \"\"\"\n    lines = [f\"meta-agent: {message}\"]\n    \n    if prompt_ids and self.config.commit_config.include_prompt_reference:\n        lines.append(\"\")\n        lines.append(\"Analysis prompts applied:\")\n        for prompt_id in prompt_ids:\n            prompt = self.prompt_library.get_prompt(prompt_id)\n            goal = prompt.goal if prompt else prompt_id\n            lines.append(f\"  - {prompt_id}: {goal[:60]}\")\n    \n    lines.append(\"\")\n    lines.append(\"🤖 Generated with meta-agent\")\n    \n    return \"\\n\".join(lines)\n```\n\n### 5. Update refine_iterative() Method\n\nModify the iterative refinement loop (around line 365) to use enhanced commit:\n\n```python\n# Step 5: Commit to GitHub\nlogger.info(\"Step 5: Committing changes to GitHub...\")\ncommit_result = self._commit_changes(\n    message=f\"Iteration {iteration}: improvements from AI analysis\",\n    prompt_ids=triage_result.selected_prompts,\n    skip_commit=self.config.skip_commit,  # From CLI option\n)\n\nif commit_result.success and not commit_result.skipped:\n    logger.info(f\"Committed {commit_result.files_staged} files: {commit_result.commit_hash}\")\nelif commit_result.skipped:\n    logger.info(f\"Commit skipped: {commit_result.skip_reason}\")\nelse:\n    logger.error(f\"Commit failed: {commit_result.error}\")\n\niterations.append(\n    IterationResult(\n        iteration=iteration,\n        prompts_run=triage_result.selected_prompts,\n        changes_made=changes_made,\n        committed=commit_result.success and not commit_result.skipped,\n        commit_hash=commit_result.commit_hash,\n        stage_results=iteration_stage_results,\n    )\n)\n```\n\n### 6. Add CLI Options\n\nUpdate `src/metaagent/cli.py` to add commit-related options to relevant commands:\n\n```python\n@app.command(\"refine-iterative\")\ndef refine_iterative(\n    # ... existing options ...\n    no_commit: bool = typer.Option(\n        False,\n        \"--no-commit\",\n        help=\"Skip auto-committing changes after implementation.\",\n    ),\n    commit_message: Optional[str] = typer.Option(\n        None,\n        \"--commit-message\",\n        \"-cm\",\n        help=\"Custom commit message (overrides auto-generated).\",\n    ),\n    auto_push: bool = typer.Option(\n        False,\n        \"--auto-push\",\n        help=\"Automatically push commits to remote.\",\n    ),\n) -> None:\n```\n\n### 7. Git Error Handling Module (Optional Enhancement)\n\nCreate `src/metaagent/git_utils.py` for reusable git operations:\n\n```python\n\"\"\"Git utility functions with graceful error handling.\"\"\"\n\nfrom __future__ import annotations\n\nimport subprocess\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Optional\n\n@dataclass\nclass GitStatus:\n    \"\"\"Git repository status.\"\"\"\n    is_repo: bool\n    has_changes: bool\n    current_branch: Optional[str] = None\n    error: Optional[str] = None\n\ndef check_git_status(repo_path: Path) -> GitStatus:\n    \"\"\"Check git repository status.\"\"\"\n    try:\n        # Check if directory is a git repo\n        result = subprocess.run(\n            [\"git\", \"rev-parse\", \"--git-dir\"],\n            cwd=repo_path,\n            capture_output=True,\n            text=True,\n            timeout=10,\n        )\n        if result.returncode != 0:\n            return GitStatus(is_repo=False, has_changes=False, error=\"Not a git repository\")\n        \n        # Get current branch\n        branch_result = subprocess.run(\n            [\"git\", \"branch\", \"--show-current\"],\n            cwd=repo_path,\n            capture_output=True,\n            text=True,\n            timeout=10,\n        )\n        branch = branch_result.stdout.strip() or \"HEAD (detached)\"\n        \n        # Check for changes\n        status_result = subprocess.run(\n            [\"git\", \"status\", \"--porcelain\"],\n            cwd=repo_path,\n            capture_output=True,\n            text=True,\n            timeout=30,\n        )\n        has_changes = bool(status_result.stdout.strip())\n        \n        return GitStatus(\n            is_repo=True,\n            has_changes=has_changes,\n            current_branch=branch,\n        )\n    except subprocess.TimeoutExpired:\n        return GitStatus(is_repo=False, has_changes=False, error=\"Git command timed out\")\n    except FileNotFoundError:\n        return GitStatus(is_repo=False, has_changes=False, error=\"Git not found in PATH\")\n```\n\n## Key Files to Modify\n\n1. `src/metaagent/config.py` - Add CommitConfig dataclass\n2. `src/metaagent/orchestrator.py` - Enhance _commit_changes(), add CommitResult, update refine_iterative()\n3. `src/metaagent/cli.py` - Add --no-commit, --commit-message, --auto-push options\n4. `src/metaagent/git_utils.py` - New file for reusable git utilities (optional)\n\n## Environment Variables\n\n| Variable | Default | Description |\n|----------|---------|-------------|\n| METAAGENT_AUTO_COMMIT | true | Enable/disable auto-commit |\n| METAAGENT_AUTO_PUSH | false | Enable/disable auto-push after commit |\n| METAAGENT_SIGN_COMMITS | false | Enable GPG commit signing |",
        "testStrategy": "## Unit Tests (`tests/test_git_commit.py`)\n\n### 1. Test CommitConfig Loading\n```python\ndef test_commit_config_defaults():\n    config = CommitConfig()\n    assert config.enabled is True\n    assert config.push_enabled is False\n    assert config.sign_commits is False\n\ndef test_commit_config_from_env(monkeypatch):\n    monkeypatch.setenv(\"METAAGENT_AUTO_COMMIT\", \"false\")\n    monkeypatch.setenv(\"METAAGENT_AUTO_PUSH\", \"true\")\n    config = CommitConfig.from_env()\n    assert config.enabled is False\n    assert config.push_enabled is True\n```\n\n### 2. Test CommitResult Dataclass\n```python\ndef test_commit_result_success():\n    result = CommitResult(success=True, commit_hash=\"abc1234\", files_staged=5)\n    assert result.success\n    assert result.commit_hash == \"abc1234\"\n    assert not result.skipped\n\ndef test_commit_result_skipped():\n    result = CommitResult(success=True, skipped=True, skip_reason=\"No changes\")\n    assert result.skipped\n    assert result.commit_hash is None\n```\n\n### 3. Test _commit_changes Method\n```python\ndef test_commit_changes_success(mock_subprocess, mock_config, tmp_path):\n    \"\"\"Test successful commit.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    mock_subprocess.return_value.stdout = \"M file.py\\n\"  # has changes\n    \n    result = orchestrator._commit_changes(\n        message=\"Test commit\",\n        prompt_ids=[\"quality_code_review\"]\n    )\n    \n    assert result.success\n    assert result.commit_hash is not None\n    assert not result.skipped\n\ndef test_commit_changes_no_changes(mock_subprocess, mock_config):\n    \"\"\"Test when there are no changes to commit.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    mock_subprocess.return_value.stdout = \"\"  # no changes\n    \n    result = orchestrator._commit_changes(\n        message=\"Test commit\",\n        prompt_ids=[]\n    )\n    \n    assert result.success\n    assert result.skipped\n    assert result.skip_reason == \"No changes to commit\"\n\ndef test_commit_changes_skip_flag(mock_config):\n    \"\"\"Test skip_commit flag.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    \n    result = orchestrator._commit_changes(\n        message=\"Test\",\n        prompt_ids=[],\n        skip_commit=True\n    )\n    \n    assert result.skipped\n    assert \"User requested skip\" in result.skip_reason\n\ndef test_commit_changes_disabled(mock_config):\n    \"\"\"Test when auto-commit is disabled in config.\"\"\"\n    mock_config.commit_config.enabled = False\n    orchestrator = Orchestrator(mock_config)\n    \n    result = orchestrator._commit_changes(\n        message=\"Test\",\n        prompt_ids=[]\n    )\n    \n    assert result.skipped\n    assert \"disabled\" in result.skip_reason.lower()\n\ndef test_commit_changes_custom_message(mock_subprocess, mock_config):\n    \"\"\"Test custom commit message.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    mock_subprocess.return_value.stdout = \"M file.py\\n\"\n    \n    result = orchestrator._commit_changes(\n        message=\"Auto message\",\n        prompt_ids=[],\n        custom_message=\"My custom message\"\n    )\n    \n    # Verify custom message was used in git commit call\n    commit_call = [c for c in mock_subprocess.call_args_list if \"commit\" in str(c)]\n    assert \"My custom message\" in str(commit_call)\n\ndef test_commit_changes_git_error(mock_subprocess, mock_config):\n    \"\"\"Test git command failure.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    mock_subprocess.side_effect = subprocess.CalledProcessError(1, \"git\", stderr=\"error\")\n    \n    result = orchestrator._commit_changes(\n        message=\"Test\",\n        prompt_ids=[]\n    )\n    \n    assert not result.success\n    assert result.error is not None\n\ndef test_commit_changes_timeout(mock_subprocess, mock_config):\n    \"\"\"Test git command timeout.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    mock_subprocess.side_effect = subprocess.TimeoutExpired(\"git\", 30)\n    \n    result = orchestrator._commit_changes(\n        message=\"Test\",\n        prompt_ids=[]\n    )\n    \n    assert not result.success\n    assert \"timed out\" in result.error.lower()\n\ndef test_commit_changes_git_not_found(mock_subprocess, mock_config):\n    \"\"\"Test when git is not installed.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    mock_subprocess.side_effect = FileNotFoundError()\n    \n    result = orchestrator._commit_changes(\n        message=\"Test\",\n        prompt_ids=[]\n    )\n    \n    assert not result.success\n    assert \"not found\" in result.error.lower()\n```\n\n### 4. Test Commit Message Builder\n```python\ndef test_build_commit_message_basic(mock_config):\n    \"\"\"Test basic commit message generation.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    \n    message = orchestrator._build_commit_message(\n        message=\"Iteration 1\",\n        prompt_ids=[]\n    )\n    \n    assert \"meta-agent: Iteration 1\" in message\n    assert \"🤖 Generated with meta-agent\" in message\n\ndef test_build_commit_message_with_prompts(mock_config, mock_prompt_library):\n    \"\"\"Test commit message includes prompt references.\"\"\"\n    orchestrator = Orchestrator(mock_config, prompt_library=mock_prompt_library)\n    \n    message = orchestrator._build_commit_message(\n        message=\"Iteration 1\",\n        prompt_ids=[\"quality_code_review\", \"security_analysis\"]\n    )\n    \n    assert \"Analysis prompts applied:\" in message\n    assert \"quality_code_review\" in message\n    assert \"security_analysis\" in message\n```\n\n### 5. Test CLI Options\n```python\ndef test_refine_iterative_no_commit_flag(mock_orchestrator):\n    \"\"\"Test --no-commit flag is passed to orchestrator.\"\"\"\n    result = runner.invoke(app, [\n        \"refine-iterative\",\n        \"--repo\", str(tmp_path),\n        \"--no-commit\"\n    ])\n    \n    # Verify skip_commit=True was passed\n    assert mock_orchestrator.refine_iterative.called\n    call_kwargs = mock_orchestrator.refine_iterative.call_args[1]\n    assert call_kwargs.get(\"skip_commit\") is True\n\ndef test_refine_iterative_custom_message(mock_orchestrator):\n    \"\"\"Test --commit-message flag.\"\"\"\n    result = runner.invoke(app, [\n        \"refine-iterative\",\n        \"--repo\", str(tmp_path),\n        \"--commit-message\", \"Custom: my changes\"\n    ])\n    \n    call_kwargs = mock_orchestrator.refine_iterative.call_args[1]\n    assert call_kwargs.get(\"custom_commit_message\") == \"Custom: my changes\"\n\ndef test_refine_iterative_auto_push(mock_orchestrator):\n    \"\"\"Test --auto-push flag.\"\"\"\n    result = runner.invoke(app, [\n        \"refine-iterative\",\n        \"--repo\", str(tmp_path),\n        \"--auto-push\"\n    ])\n    \n    # Verify push_enabled was set\n    assert mock_orchestrator.config.commit_config.push_enabled is True\n```\n\n### 6. Integration Tests\n```python\n@pytest.mark.integration\ndef test_commit_changes_real_git(tmp_path):\n    \"\"\"Integration test with real git repository.\"\"\"\n    # Initialize git repo\n    subprocess.run([\"git\", \"init\"], cwd=tmp_path)\n    subprocess.run([\"git\", \"config\", \"user.email\", \"test@test.com\"], cwd=tmp_path)\n    subprocess.run([\"git\", \"config\", \"user.name\", \"Test\"], cwd=tmp_path)\n    \n    # Create initial commit\n    (tmp_path / \"README.md\").write_text(\"# Test\")\n    subprocess.run([\"git\", \"add\", \"-A\"], cwd=tmp_path)\n    subprocess.run([\"git\", \"commit\", \"-m\", \"Initial\"], cwd=tmp_path)\n    \n    # Make changes\n    (tmp_path / \"new_file.py\").write_text(\"print('hello')\")\n    \n    # Test commit\n    config = Config.from_env(tmp_path)\n    orchestrator = Orchestrator(config)\n    \n    result = orchestrator._commit_changes(\n        message=\"Test iteration\",\n        prompt_ids=[\"test_prompt\"]\n    )\n    \n    assert result.success\n    assert result.commit_hash is not None\n    \n    # Verify commit exists\n    log_result = subprocess.run(\n        [\"git\", \"log\", \"--oneline\", \"-1\"],\n        cwd=tmp_path,\n        capture_output=True,\n        text=True,\n    )\n    assert \"meta-agent\" in log_result.stdout\n```\n\n## Manual Testing Checklist\n\n1. Run `metaagent refine-iterative --mock --no-commit` and verify no commits are made\n2. Run `metaagent refine-iterative --mock --commit-message \"Custom msg\"` and verify custom message\n3. Test in a repo without git initialized - verify graceful error\n4. Test with uncommitted changes - verify they are staged and committed\n5. Test with no changes to commit - verify skip message\n6. Verify commit messages include prompt references\n7. Test `--auto-push` on a repo with remote configured",
        "status": "done",
        "dependencies": [
          "7",
          "8",
          "16"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T03:33:46.159Z"
      },
      {
        "id": "18",
        "title": "Implement Full Refinement Loop",
        "description": "Create the complete iterative refinement loop that runs until the AI determines the codebase is complete, orchestrating the full cycle of packing, triage, analysis, implementation, and auto-commit with progress reporting and safety limits.",
        "details": "## Overview\n\nThis task extends the existing `refine_iterative()` method in `src/metaagent/orchestrator.py:256` to create a fully automated end-to-end refinement loop. The current implementation has the structure but relies on stub implementations for Claude Code integration (`_implement_with_claude()` at line 510) and basic git commit (`_commit_changes()` at line 553). This task integrates everything into a complete, production-ready loop.\n\n## Implementation Steps\n\n### 1. Add CLI Command for Full Loop (`src/metaagent/cli.py`)\n\nAdd a new `refine-loop` command that exposes the full iterative refinement:\n\n```python\n@app.command(\"refine-loop\")\ndef refine_loop(\n    repo: Path = typer.Option(\n        Path.cwd(),\n        \"--repo\",\n        \"-r\",\n        help=\"Path to the target repository to refine.\",\n    ),\n    prd: Optional[Path] = typer.Option(\n        None,\n        \"--prd\",\n        help=\"Path to PRD file (default: docs/prd.md in target repo).\",\n    ),\n    config_dir: Optional[Path] = typer.Option(\n        None,\n        \"--config-dir\",\n        \"-c\",\n        help=\"Path to meta-agent config directory.\",\n    ),\n    max_iterations: int = typer.Option(\n        10,\n        \"--max-iterations\",\n        \"-n\",\n        help=\"Maximum number of refinement iterations (safety limit).\",\n    ),\n    auto_commit: bool = typer.Option(\n        True,\n        \"--auto-commit/--no-auto-commit\",\n        help=\"Automatically commit changes after each iteration.\",\n    ),\n    push: bool = typer.Option(\n        False,\n        \"--push/--no-push\",\n        help=\"Push commits to remote after each iteration.\",\n    ),\n    dry_run: bool = typer.Option(\n        False,\n        \"--dry-run\",\n        help=\"Run analysis without implementing changes.\",\n    ),\n    mock: bool = typer.Option(\n        False,\n        \"--mock\",\n        \"-m\",\n        help=\"Run in mock mode (no API calls).\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\",\n        help=\"Enable verbose output.\",\n    ),\n) -> None:\n    \"\"\"Run the full iterative refinement loop until completion.\n    \n    The loop performs these steps each iteration:\n    1. Pack codebase with Repomix\n    2. Run triage to select analysis prompts\n    3. Run selected analysis prompts\n    4. Implement changes with Claude Code\n    5. Auto-commit changes (if enabled)\n    6. Re-pack and repeat from step 2\n    \n    Stops when triage returns done=true or max iterations reached.\n    \"\"\"\n```\n\n### 2. Create Progress Reporter (`src/metaagent/progress.py`)\n\nCreate a new module for rich progress reporting:\n\n```python\n\"\"\"Progress reporting for the refinement loop.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional\n\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.panel import Panel\nfrom rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\nfrom rich.table import Table\n\n\n@dataclass\nclass IterationProgress:\n    \"\"\"Progress tracking for a single iteration.\"\"\"\n    iteration: int\n    max_iterations: int\n    current_step: str\n    step_number: int\n    total_steps: int = 6\n    prompts_selected: list[str] = field(default_factory=list)\n    changes_made: int = 0\n    commit_hash: Optional[str] = None\n    started_at: datetime = field(default_factory=datetime.now)\n\n\nclass ProgressReporter:\n    \"\"\"Rich progress reporter for the refinement loop.\"\"\"\n\n    STEPS = [\n        \"Packing codebase\",\n        \"Running triage\",\n        \"Analyzing codebase\",\n        \"Implementing changes\",\n        \"Committing changes\",\n        \"Checking completion\",\n    ]\n\n    def __init__(self, console: Optional[Console] = None):\n        self.console = console or Console()\n        self.current: Optional[IterationProgress] = None\n        self._live: Optional[Live] = None\n\n    def start_iteration(self, iteration: int, max_iterations: int) -> None:\n        \"\"\"Start tracking a new iteration.\"\"\"\n        self.current = IterationProgress(\n            iteration=iteration,\n            max_iterations=max_iterations,\n            current_step=self.STEPS[0],\n            step_number=1,\n        )\n        self._render()\n\n    def update_step(self, step: int, details: str = \"\") -> None:\n        \"\"\"Update the current step.\"\"\"\n        if self.current:\n            self.current.step_number = step\n            self.current.current_step = f\"{self.STEPS[step - 1]}: {details}\" if details else self.STEPS[step - 1]\n            self._render()\n\n    def set_prompts(self, prompts: list[str]) -> None:\n        \"\"\"Set the prompts selected by triage.\"\"\"\n        if self.current:\n            self.current.prompts_selected = prompts\n            self._render()\n\n    def set_changes(self, count: int) -> None:\n        \"\"\"Set the number of changes made.\"\"\"\n        if self.current:\n            self.current.changes_made = count\n            self._render()\n\n    def set_commit(self, hash: str) -> None:\n        \"\"\"Set the commit hash.\"\"\"\n        if self.current:\n            self.current.commit_hash = hash\n            self._render()\n\n    def complete_iteration(self, success: bool = True) -> None:\n        \"\"\"Mark the iteration as complete.\"\"\"\n        status = \"[green]✓[/green]\" if success else \"[red]✗[/red]\"\n        self.console.print(f\"\\nIteration {self.current.iteration} {status}\")\n\n    def finish(self, done: bool, total_iterations: int) -> None:\n        \"\"\"Finish progress reporting.\"\"\"\n        if done:\n            self.console.print(\"\\n[green bold]✓ Refinement complete![/green bold]\")\n            self.console.print(\"Triage determined the codebase meets PRD requirements.\")\n        else:\n            self.console.print(f\"\\n[yellow]⚠ Maximum iterations ({total_iterations}) reached[/yellow]\")\n\n    def _render(self) -> None:\n        \"\"\"Render current progress.\"\"\"\n        if not self.current:\n            return\n\n        table = Table(show_header=False, box=None)\n        table.add_column(\"Key\", style=\"dim\")\n        table.add_column(\"Value\")\n\n        table.add_row(\"Iteration\", f\"{self.current.iteration}/{self.current.max_iterations}\")\n        table.add_row(\"Step\", f\"{self.current.step_number}/{self.current.total_steps}\")\n        table.add_row(\"Current\", self.current.current_step)\n\n        if self.current.prompts_selected:\n            table.add_row(\"Prompts\", \", \".join(self.current.prompts_selected))\n\n        if self.current.changes_made:\n            table.add_row(\"Changes\", str(self.current.changes_made))\n\n        if self.current.commit_hash:\n            table.add_row(\"Commit\", self.current.commit_hash)\n\n        panel = Panel(table, title=\"[bold blue]Refinement Progress[/bold blue]\")\n        self.console.print(panel, end=\"\\r\")\n```\n\n### 3. Enhance Orchestrator Loop (`src/metaagent/orchestrator.py`)\n\nUpdate `refine_iterative()` to integrate with progress reporting and Task 16/17 implementations:\n\n```python\ndef refine_iterative(\n    self,\n    max_iterations: int = 10,\n    auto_commit: bool = True,\n    push: bool = False,\n    dry_run: bool = False,\n    progress_reporter: Optional[\"ProgressReporter\"] = None,\n) -> RefinementResult:\n    \"\"\"Run the iterative refinement loop with AI-driven triage.\n\n    This is the main entry point for the full refinement loop:\n    1. Pack codebase with Repomix\n    2. Triage (AI decides which prompts to run)\n    3. Run selected prompts\n    4. Implement changes with Claude Code\n    5. Commit to git (if auto_commit enabled)\n    6. Repeat until triage says \"done\" or max iterations reached\n\n    Args:\n        max_iterations: Maximum number of iterations to run (safety limit).\n        auto_commit: Whether to automatically commit after each iteration.\n        push: Whether to push commits to remote.\n        dry_run: If True, skip implementation step.\n        progress_reporter: Optional progress reporter for UI feedback.\n\n    Returns:\n        RefinementResult with the outcome.\n    \"\"\"\n```\n\n### 4. Add Loop Dataclasses (`src/metaagent/orchestrator.py`)\n\nAdd new dataclasses for tracking loop state:\n\n```python\n@dataclass\nclass LoopConfig:\n    \"\"\"Configuration for the refinement loop.\"\"\"\n    max_iterations: int = 10\n    auto_commit: bool = True\n    push: bool = False\n    dry_run: bool = False\n    min_changes_per_iteration: int = 1\n\n\n@dataclass\nclass LoopState:\n    \"\"\"State tracking for the refinement loop.\"\"\"\n    iteration: int = 0\n    total_changes: int = 0\n    total_commits: int = 0\n    total_prompts_run: int = 0\n    consecutive_no_change_iterations: int = 0\n    stopped_reason: Optional[str] = None\n```\n\n### 5. Integrate Claude Code Runner (depends on Task 16)\n\nAfter Task 16 is complete, update `_implement_with_claude()` to use `ClaudeCodeRunner`:\n\n```python\ndef _implement_with_claude(\n    self,\n    stage_results: list[StageResult],\n    progress_reporter: Optional[\"ProgressReporter\"] = None,\n) -> tuple[bool, int]:\n    \"\"\"Implement changes using Claude Code.\n\n    Args:\n        stage_results: Results from the analysis stage.\n        progress_reporter: Optional progress reporter.\n\n    Returns:\n        Tuple of (changes_made, change_count).\n    \"\"\"\n    if not stage_results:\n        return False, 0\n\n    # Import ClaudeCodeRunner from Task 16\n    from .claude_runner import ClaudeCodeRunner\n\n    runner = ClaudeCodeRunner(\n        timeout=self.config.claude_timeout,\n        model=self.config.claude_model,\n        max_turns=self.config.claude_max_turns,\n    )\n\n    # Build implementation prompt from tasks\n    tasks = []\n    for result in stage_results:\n        tasks.extend(result.tasks)\n\n    if not tasks:\n        logger.info(\"No tasks to implement\")\n        return False, 0\n\n    implementation_prompt = self._build_implementation_prompt(tasks)\n\n    if progress_reporter:\n        progress_reporter.update_step(4, f\"{len(tasks)} tasks\")\n\n    result = runner.run(\n        prompt=implementation_prompt,\n        working_dir=self.config.repo_path,\n    )\n\n    return result.success, len(result.files_modified)\n```\n\n### 6. Integrate Git Commit (depends on Task 17)\n\nAfter Task 17 is complete, update `_commit_changes()` to use enhanced git integration:\n\n```python\ndef _commit_changes(\n    self,\n    message: str,\n    push: bool = False,\n    progress_reporter: Optional[\"ProgressReporter\"] = None,\n) -> Optional[str]:\n    \"\"\"Commit changes to git with enhanced options.\n\n    Args:\n        message: Commit message.\n        push: Whether to push to remote.\n        progress_reporter: Optional progress reporter.\n\n    Returns:\n        Commit hash if successful, None otherwise.\n    \"\"\"\n    # Use CommitConfig from Task 17\n    from .git_utils import commit_changes, CommitConfig\n\n    config = CommitConfig(\n        enabled=True,\n        push_enabled=push,\n        message_prefix=\"meta-agent: \",\n        message_suffix=\"\\n\\n🤖 Generated with meta-agent\",\n    )\n\n    result = commit_changes(\n        repo_path=self.config.repo_path,\n        message=message,\n        config=config,\n    )\n\n    if result.success and progress_reporter:\n        progress_reporter.set_commit(result.commit_hash[:8])\n\n    return result.commit_hash if result.success else None\n```\n\n### 7. Add Safety Mechanisms\n\nAdd safety checks to prevent runaway loops:\n\n```python\ndef _check_loop_safety(self, state: LoopState, config: LoopConfig) -> Optional[str]:\n    \"\"\"Check if the loop should be stopped for safety reasons.\n\n    Args:\n        state: Current loop state.\n        config: Loop configuration.\n\n    Returns:\n        Stop reason if loop should stop, None otherwise.\n    \"\"\"\n    # Max iterations reached\n    if state.iteration >= config.max_iterations:\n        return f\"Maximum iterations ({config.max_iterations}) reached\"\n\n    # Too many consecutive no-change iterations\n    if state.consecutive_no_change_iterations >= 3:\n        return \"No changes made in 3 consecutive iterations\"\n\n    # Total runtime check could be added here\n\n    return None\n```\n\n## File Changes Summary\n\n1. **`src/metaagent/cli.py`**: Add `refine-loop` command with `--max-iterations` flag\n2. **`src/metaagent/progress.py`**: New file for rich progress reporting\n3. **`src/metaagent/orchestrator.py`**: Enhance `refine_iterative()` with full loop, add `LoopConfig` and `LoopState` dataclasses\n4. **`src/metaagent/__init__.py`**: Export new classes\n\n## Dependencies on Other Tasks\n\n- **Task 16 (Claude Code Integration)**: Required for `_implement_with_claude()` to actually invoke Claude Code\n- **Task 17 (Auto-Commit)**: Required for `_commit_changes()` to use enhanced git features\n- **Task 14/15 (Iterative Triage)**: The `_run_triage()` method at line 437 is already implemented and will be used as-is",
        "testStrategy": "## Unit Tests (`tests/test_refinement_loop.py`)\n\n### 1. Test CLI Command\n```python\ndef test_refine_loop_help():\n    result = runner.invoke(app, [\"refine-loop\", \"--help\"])\n    assert result.exit_code == 0\n    assert \"--max-iterations\" in result.stdout\n    assert \"--auto-commit\" in result.stdout\n    assert \"--push\" in result.stdout\n    assert \"--dry-run\" in result.stdout\n\ndef test_refine_loop_default_max_iterations(mock_config):\n    \"\"\"Test default max-iterations is 10.\"\"\"\n    with patch(\"metaagent.cli.Orchestrator\") as mock_orch:\n        mock_instance = mock_orch.return_value\n        mock_instance.refine_iterative.return_value = RefinementResult(\n            success=True,\n            profile_name=\"iterative\",\n            stages_completed=1,\n            stages_failed=0,\n        )\n\n        result = runner.invoke(app, [\n            \"refine-loop\",\n            \"--repo\", str(mock_config.repo_path),\n            \"--config-dir\", str(mock_config.config_dir),\n            \"--mock\",\n        ])\n\n        # Verify max_iterations=10 was passed\n        call_args = mock_instance.refine_iterative.call_args\n        assert call_args.kwargs.get(\"max_iterations\", 10) == 10\n```\n\n### 2. Test Progress Reporter\n```python\ndef test_progress_reporter_initialization():\n    reporter = ProgressReporter()\n    assert reporter.current is None\n\ndef test_progress_reporter_start_iteration():\n    reporter = ProgressReporter(Console(quiet=True))\n    reporter.start_iteration(1, 10)\n    assert reporter.current is not None\n    assert reporter.current.iteration == 1\n    assert reporter.current.max_iterations == 10\n\ndef test_progress_reporter_update_step():\n    reporter = ProgressReporter(Console(quiet=True))\n    reporter.start_iteration(1, 10)\n    reporter.update_step(2, \"Running analysis\")\n    assert reporter.current.step_number == 2\n    assert \"Running analysis\" in reporter.current.current_step\n\ndef test_progress_reporter_set_prompts():\n    reporter = ProgressReporter(Console(quiet=True))\n    reporter.start_iteration(1, 10)\n    reporter.set_prompts([\"quality_error_analysis\", \"security_vulnerability_analysis\"])\n    assert len(reporter.current.prompts_selected) == 2\n```\n\n### 3. Test Loop State and Config\n```python\ndef test_loop_config_defaults():\n    config = LoopConfig()\n    assert config.max_iterations == 10\n    assert config.auto_commit is True\n    assert config.push is False\n    assert config.dry_run is False\n\ndef test_loop_state_initialization():\n    state = LoopState()\n    assert state.iteration == 0\n    assert state.total_changes == 0\n    assert state.stopped_reason is None\n```\n\n### 4. Test Safety Mechanisms\n```python\ndef test_check_loop_safety_max_iterations():\n    orchestrator = create_test_orchestrator()\n    state = LoopState(iteration=10)\n    config = LoopConfig(max_iterations=10)\n    reason = orchestrator._check_loop_safety(state, config)\n    assert \"Maximum iterations\" in reason\n\ndef test_check_loop_safety_no_changes():\n    orchestrator = create_test_orchestrator()\n    state = LoopState(consecutive_no_change_iterations=3)\n    config = LoopConfig()\n    reason = orchestrator._check_loop_safety(state, config)\n    assert \"No changes made\" in reason\n\ndef test_check_loop_safety_continue():\n    orchestrator = create_test_orchestrator()\n    state = LoopState(iteration=5, consecutive_no_change_iterations=1)\n    config = LoopConfig(max_iterations=10)\n    reason = orchestrator._check_loop_safety(state, config)\n    assert reason is None\n```\n\n### 5. Test Full Loop Integration\n```python\ndef test_refine_iterative_stops_on_done(mock_orchestrator):\n    \"\"\"Test loop stops when triage returns done=true.\"\"\"\n    # Mock triage to return done=True on second iteration\n    mock_orchestrator._run_triage = MagicMock(side_effect=[\n        TriageResult(success=True, done=False, selected_prompts=[\"quality_error_analysis\"]),\n        TriageResult(success=True, done=True, assessment=\"Complete\"),\n    ])\n\n    result = mock_orchestrator.refine_iterative(max_iterations=10)\n\n    assert result.success\n    assert len(result.iterations) == 1  # Only one iteration before done\n\ndef test_refine_iterative_stops_at_max_iterations(mock_orchestrator):\n    \"\"\"Test loop stops at max iterations.\"\"\"\n    # Mock triage to never return done\n    mock_orchestrator._run_triage = MagicMock(return_value=TriageResult(\n        success=True,\n        done=False,\n        selected_prompts=[\"quality_error_analysis\"],\n    ))\n\n    result = mock_orchestrator.refine_iterative(max_iterations=3)\n\n    assert len(result.iterations) == 3\n\ndef test_refine_iterative_dry_run(mock_orchestrator):\n    \"\"\"Test dry run skips implementation.\"\"\"\n    result = mock_orchestrator.refine_iterative(max_iterations=1, dry_run=True)\n\n    # Implementation should not have been called\n    mock_orchestrator._implement_with_claude.assert_not_called()\n```\n\n### 6. Test Error Handling\n```python\ndef test_refine_iterative_triage_failure(mock_orchestrator):\n    \"\"\"Test graceful handling of triage failure.\"\"\"\n    mock_orchestrator._run_triage = MagicMock(return_value=TriageResult(\n        success=False,\n        error=\"API error\",\n    ))\n\n    result = mock_orchestrator.refine_iterative(max_iterations=10)\n\n    assert not result.success\n    assert \"Triage failed\" in result.error\n\ndef test_refine_iterative_commit_failure(mock_orchestrator):\n    \"\"\"Test loop continues despite commit failure.\"\"\"\n    mock_orchestrator._commit_changes = MagicMock(return_value=None)\n\n    result = mock_orchestrator.refine_iterative(max_iterations=1, auto_commit=True)\n\n    # Should still succeed despite commit failure\n    assert result.iterations[0].committed is False\n```\n\n## Integration Tests\n\n### 1. End-to-End Mock Test\n```python\ndef test_full_loop_end_to_end(mock_config, tmp_path):\n    \"\"\"Test complete loop with all components mocked.\"\"\"\n    # Setup mock responses for triage, analysis, implementation\n    with patch.multiple(\n        \"metaagent.orchestrator.Orchestrator\",\n        _pack_codebase=MagicMock(return_value=\"mock code context\"),\n        _run_triage=MagicMock(side_effect=[\n            TriageResult(success=True, done=False, selected_prompts=[\"quality_error_analysis\"]),\n            TriageResult(success=True, done=True),\n        ]),\n        _implement_with_claude=MagicMock(return_value=(True, 3)),\n        _commit_changes=MagicMock(return_value=\"abc123\"),\n    ):\n        result = runner.invoke(app, [\n            \"refine-loop\",\n            \"--repo\", str(mock_config.repo_path),\n            \"--config-dir\", str(mock_config.config_dir),\n            \"--max-iterations\", \"5\",\n            \"--mock\",\n        ])\n\n        assert result.exit_code == 0\n        assert \"complete\" in result.stdout.lower()\n```\n\n## Manual Verification Steps\n\n1. Run `metaagent refine-loop --help` and verify all options are shown\n2. Run `metaagent refine-loop --mock --max-iterations 2` and verify progress output\n3. Run `metaagent refine-loop --dry-run` and verify no changes are made\n4. Verify loop stops correctly when max iterations reached\n5. Verify `--no-auto-commit` flag prevents git commits\n6. Test with real API keys on a test repository",
        "status": "done",
        "dependencies": [
          "7",
          "8",
          "14",
          "16",
          "17"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-14T03:35:30.227Z"
      },
      {
        "id": "19",
        "title": "Create JSON Response Schema Constant in prompts.py",
        "description": "Add JSON_RESPONSE_SCHEMA constant to prompts.py defining the structured response format with summary, recommendations, and tasks structure for AI analysis outputs.",
        "details": "Update `src/metaagent/prompts.py` to include a JSON schema constant following JSON Schema draft-07 specification and Pydantic best practices for structured LLM outputs[1][2].\n\n**Implementation Steps:**\n\n1. **Define the JSON_RESPONSE_SCHEMA constant** at module level in `prompts.py`:\n\n```python\nfrom typing import List, Dict, Any\nimport json\n\nJSON_RESPONSE_SCHEMA = {\n    '$schema': 'http://json-schema.org/draft-07/schema#',\n    'type': 'object',\n    'properties': {\n        'summary': {\n            'type': 'object',\n            'properties': {\n                'current_state': {'type': 'string', 'description': 'Current codebase state summary'},\n                'gaps_identified': {\n                    'type': 'array',\n                    'items': {'type': 'string'},\n                    'description': 'List of identified gaps vs PRD'\n                },\n                'readiness_score': {'type': 'number', 'minimum': 0, 'maximum': 10}\n            },\n            'required': ['current_state', 'gaps_identified', 'readiness_score']\n        },\n        'recommendations': {\n            'type': 'array',\n            'items': {\n                'type': 'object',\n                'properties': {\n                    'id': {'type': 'string'},\n                    'priority': {'type': 'string', 'enum': ['high', 'medium', 'low']},\n                    'description': {'type': 'string'},\n                    'category': {'type': 'string', 'enum': ['bug', 'feature', 'refactor', 'docs']},\n                    'estimated_effort': {'type': 'string'}\n                },\n                'required': ['id', 'priority', 'description']\n            }\n        },\n        'tasks': {\n            'type': 'array',\n            'items': {\n                'type': 'object',\n                'properties': {\n                    'title': {'type': 'string'},\n                    'description': {'type': 'string'},\n                    'priority': {'type': 'string', 'enum': ['high', 'medium', 'low']},\n                    'dependencies': {\n                        'type': 'array',\n                        'items': {'type': 'number'}\n                    },\n                    'details': {'type': 'string'},\n                    'testStrategy': {'type': 'string'}\n                },\n                'required': ['title', 'description', 'priority']\n            }\n        }\n    },\n    'required': ['summary', 'recommendations', 'tasks'],\n    'additionalProperties': False\n}\n```\n\n2. **Add helper method** to validate JSON responses:\n\n```python\ndef validate_json_response(response_json: str) -> Dict[str, Any]:\n    \"\"\"Validate AI response against JSON_RESPONSE_SCHEMA.\"\"\"\n    try:\n        data = json.loads(response_json)\n        # Use jsonschema.validate if available, or simple structural check\n        return data\n    except json.JSONDecodeError as e:\n        raise ValueError(f'Invalid JSON response: {e}')\n```\n\n3. **Update Prompt.render()** to include schema in system prompt context:\n\n```python\n    def render(self, prd: str, code_context: str, history: str, current_stage: str) -> str:\n        tmpl = Template(self.template)\n        schema_str = json.dumps(JSON_RESPONSE_SCHEMA, indent=2)\n        return tmpl.render(\n            prd=prd,\n            code_context=code_context,\n            history=history,\n            current_stage=current_stage,\n            json_response_schema=schema_str  # Add schema to context\n        )\n```\n\n4. **Update prompt templates** in `config/prompts.yaml` to reference `{{json_response_schema}}` in instructions:\n\n```yaml\ntemplate: |\n  [...]\n  \n  IMPORTANT: Respond EXACTLY in this JSON format:\n  ```json\n  {{json_response_schema}}\n  ```\n  [...]\n```\n\n**Best Practices Applied:**\n- JSON Schema draft-07 compliance[2][7]\n- Pydantic-style nested object definitions[1][2]\n- Strict `additionalProperties: false` to prevent hallucinated fields\n- Clear descriptions for each property\n- Enum constraints for priority/category fields\n- Matches existing task structure from codebase\n\n**File Location:** `src/metaagent/prompts.py` (extends Task 3 implementation)",
        "testStrategy": "Comprehensive validation tests in `tests/test_prompts.py`:\n\n1. **Schema Validation:**\n   ```bash\n   python -c \"from metaagent.prompts import JSON_RESPONSE_SCHEMA; import jsonschema; print('Schema valid')\"\n   ```\n   Verify schema parses as valid JSON Schema[3]\n\n2. **Unit Tests:**\n   - Test `JSON_RESPONSE_SCHEMA` is dict with correct structure\n   - Test `validate_json_response()` accepts valid schema-compliant JSON\n   - Test `validate_json_response()` raises ValueError for invalid JSON\n   - Test `Prompt.render()` includes schema in rendered template\n   - Test schema string is properly escaped in Jinja context\n\n3. **Integration Tests:**\n   - Mock Perplexity response with valid JSON → verify parsing succeeds\n   - Mock Perplexity response with missing `summary` → verify validation fails\n   - Mock Perplexity response with extra properties → verify rejected\n   - Test updated prompt templates render without Jinja errors\n\n4. **Schema Compliance:**\n   - Use `jsonschema` library to validate sample valid/invalid responses\n   - Verify `tasks` array matches task structure from existing codebase\n   - Test `recommendations.priority` only accepts enum values\n\n5. **Coverage:**\n   ```bash\n   pytest tests/test_prompts.py::TestJSONSchema -v --cov=metaagent/prompts\n   ```\n   Ensure 100% coverage of new schema-related code\n\n6. **E2E Verification:**\n   Run `metaagent refine --mock` and verify generated plan contains structured JSON sections\n\n**Expected pytest markers:** `@pytest.mark.schema @pytest.mark.unit @pytest.mark.integration`",
        "status": "done",
        "dependencies": [
          "3"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:32:40.069Z"
      },
      {
        "id": "20",
        "title": "Add source and has_json_schema Fields to Prompt Dataclass",
        "description": "Extend the Prompt dataclass in prompts.py with source (yaml/markdown) and has_json_schema fields to track prompt origin and built-in JSON schema presence, enabling dynamic prompt handling and validation.",
        "details": "## Implementation Steps\n\n### 1. Update Prompt Dataclass (`src/metaagent/prompts.py`)\n\nAdd the two new fields using `dataclass.field()` for proper default handling and metadata support[1][3]:\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n@dataclass\nclass Prompt:\n    id: str\n    goal: str\n    stage: str\n    template: str\n    \n    # NEW FIELDS\n    source: str = field(default='yaml')  # 'yaml' or 'markdown'\n    has_json_schema: bool = field(default=False)\n    \n    def render(self, prd: str, code_context: str, history: str, current_stage: str) -> str:\n        \"\"\"Render template with provided variables.\"\"\"\n        tmpl = Template(self.template)\n        return tmpl.render(\n            prd=prd,\n            code_context=code_context,\n            history=history,\n            current_stage=current_stage\n        )\n\n    @classmethod\n    def from_yaml(cls, data: dict) -> 'Prompt':\n        \"\"\"Factory method for YAML-loaded prompts.\"\"\"\n        return cls(\n            id=data['id'],\n            goal=data['goal'],\n            stage=data['stage'],\n            template=data['template'],\n            source='yaml',\n            has_json_schema=False  # YAML prompts don't have built-in schema\n        )\n\n    @classmethod\n    def from_markdown(cls, path: Path, prompt_id: str) -> 'Prompt':\n        \"\"\"Factory method for Markdown-loaded prompts.\"\"\"\n        with open(path, 'r') as f:\n            template = f.read()\n        return cls(\n            id=prompt_id,\n            goal=f\"Prompt from {path.name}\",\n            stage='dynamic',\n            template=template,\n            source='markdown',\n            has_json_schema='JSON_SCHEMA' in template  # Detect schema marker\n        )\n```\n\n### 2. Update Prompt Loading Logic\n\nIn the existing `PromptLibrary` or loading functions, use the new factory methods:\n\n```python\ndef load_prompts_from_yaml(yaml_path: Path) -> list[Prompt]:\n    with open(yaml_path) as f:\n        data = yaml.safe_load(f)\n    return [Prompt.from_yaml(p) for p in data['prompts'].values()]\n\n# Example for dynamic markdown loading\ndef load_prompt_from_markdown(md_path: Path, prompt_id: str) -> Prompt:\n    return Prompt.from_markdown(md_path, prompt_id)\n```\n\n### 3. Add Field Validation with __post_init__\n\n```python\n    def __post_init__(self):\n        \"\"\"Validate source field.\"\"\"\n        if self.source not in ('yaml', 'markdown'):\n            raise ValueError(f\"Invalid source: {self.source}. Must be 'yaml' or 'markdown'\")\n        \n        # Auto-detect JSON schema for markdown prompts\n        if self.source == 'markdown' and not hasattr(self, '_has_json_schema_detected'):\n            self.has_json_schema = 'JSON_SCHEMA' in self.template or 'json-schema.org' in self.template\n            self._has_json_schema_detected = True[1]\n```\n\n### 4. Update JSON_RESPONSE_SCHEMA Integration (Task 19)\n\nEnsure prompts with `has_json_schema=True` reference the `JSON_RESPONSE_SCHEMA` constant:\n\n```python\n    def get_response_schema(self) -> Optional[dict]:\n        \"\"\"Return JSON schema if prompt has built-in schema.\"\"\"\n        if self.has_json_schema:\n            from .prompts import JSON_RESPONSE_SCHEMA  # Task 19\n            return JSON_RESPONSE_SCHEMA\n        return None\n```\n\n## Best Practices Applied\n- Use `field(default=...)` for mutable defaults[1]\n- Factory methods for different loading sources\n- `__post_init__` for validation[1][4]\n- Type hints throughout\n- Backward compatible with existing YAML prompts",
        "testStrategy": "## Comprehensive Test Strategy (`tests/test_prompts.py`)\n\n### 1. Dataclass Field Tests\n```python\n def test_prompt_new_fields():\n    p = Prompt('test', 'goal', 'stage', 'template')\n    assert p.source == 'yaml'\n    assert p.has_json_schema is False\n    \n    p2 = Prompt('test', 'goal', 'stage', 'template', source='markdown', has_json_schema=True)\n    assert p2.source == 'markdown'\n    assert p2.has_json_schema is True\n```\n\n### 2. Factory Method Tests\n```python\n def test_from_yaml_factory():\n    data = {'id': 'test', 'goal': 'test', 'stage': 'test', 'template': 'test'}\n    p = Prompt.from_yaml(data)\n    assert p.source == 'yaml'\n    assert p.has_json_schema is False\n    \n def test_from_markdown_factory(tmp_path):\n    md_file = tmp_path / 'test.md'\n    md_file.write_text('# Test\\nJSON_SCHEMA')\n    p = Prompt.from_markdown(md_file, 'test')\n    assert p.source == 'markdown'\n    assert p.has_json_schema is True\n```\n\n### 3. Validation Tests\n```python\n def test_invalid_source_raises():\n    with pytest.raises(ValueError):\n        Prompt('id', 'goal', 'stage', 'template', source='invalid')\n        \n def test_post_init_detection():\n    p = Prompt('test', 'goal', 'stage', 'template with JSON_SCHEMA')\n    p.__post_init__()  # Triggers detection\n    assert p.has_json_schema is True\n```\n\n### 4. Integration Tests\n```python\n def test_loading_preserves_new_fields():\n    # Test existing YAML loading still works\n    prompts = load_prompts_from_yaml(Path('config/prompts.yaml'))\n    for p in prompts:\n        assert hasattr(p, 'source')\n        assert hasattr(p, 'has_json_schema')\n        \n def test_schema_integration():\n    p = Prompt.from_markdown(Path('config/prompt_library/meta_triage.md'), 'meta_triage')\n    assert p.has_json_schema is True  # Assuming it contains schema\n    schema = p.get_response_schema()\n    assert schema is not None\n    assert schema['$schema'] == 'http://json-schema.org/draft-07/schema#'\n```\n\n### 5. Roundtrip Serialization Test\n```python\n import json\n from dataclasses import asdict\n \n def test_dataclass_serialization():\n    p = Prompt('test', 'goal', 'stage', 'template', source='markdown', has_json_schema=True)\n    data = asdict(p)\n    assert data['source'] == 'markdown'\n    assert data['has_json_schema'] is True\n    # Reconstruct\n    p2 = Prompt(**data)\n    assert p2 == p\n```\n\nRun with: `pytest tests/test_prompts.py -v -k 'Prompt.*(source|schema)'`",
        "status": "done",
        "dependencies": [
          "3",
          "19"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:32:46.739Z"
      },
      {
        "id": "21",
        "title": "Refactor Prompt.render() to Enforce Context–Instructions–Schema Ordering",
        "description": "Refactor the Prompt.render() method so that markdown-based prompts are constructed in a consistent order—context first, then instructions, then any embedded JSON schema—while maintaining backward compatibility with existing YAML prompts.",
        "details": "## Goal\nEnsure `Prompt.render()` builds prompts in a well-defined, LLM-friendly order: **context → instructions → JSON schema**, especially for markdown-origin prompts that may include structured response schemas.\n\n## Background & Rationale\nModern prompt engineering guidance recommends:\n- Supplying **all relevant context first**, then posing the specific question or task.[4]\n- Using a **consistent structure** and clear delimiters for different prompt sections.[2][4]\n- Clearly separating **task instructions** from **output format / schema constraints** (e.g., JSON schema) to reduce ambiguity and improve adherence.[3][5]\n\nTask 19 introduces `JSON_RESPONSE_SCHEMA`, and Task 20 adds `source` and `has_json_schema` fields to `Prompt`, enabling differentiated handling for markdown prompts that embed schemas. This task leverages those fields to standardize how prompts are assembled.\n\n## Implementation Plan\n\n### 1. Analyze Current Prompt.render() Behavior\n- Open `src/metaagent/prompts.py` and review the existing `Prompt.render()` implementation from Task 3.\n- Identify:\n  - Current template variables (`prd`, `code_context`, `history`, `current_stage`, etc.).\n  - Whether templates themselves currently encode order (likely via YAML templates) versus any post-processing in `render()`.\n  - Any existing handling of JSON schema or structured output hints (may be none yet).\n\n### 2. Define Canonical Prompt Sections\nDesign the logical sections that `render()` should guarantee, independent of the raw template:\n- **Context section** (input data / background):\n  - PRD, code context, previous analysis, current stage, and any other environmental info.\n- **Instruction section**:\n  - The main analytic or action instructions, including role, goals, constraints, and style.\n- **JSON schema section** (for markdown prompts with schemas):\n  - A clearly delimited section that explains the expected JSON response structure (e.g., via `JSON_RESPONSE_SCHEMA` from Task 19 or a markdown-embedded variant).\n\nAdopt clear, consistent markdown heading conventions aligned with best practices for prompt structure:[4]\n- Example (adjust to fit existing templates):\n  - `## Context`\n  - `## Task`\n  - `## Response JSON Schema` (only when `has_json_schema` is true)\n\n### 3. Introduce an Internal Assembly Pipeline\nRefactor `Prompt.render()` to:\n\n1. **Render the base template** with Jinja2, as today, into an intermediate string (to preserve compatibility with existing YAML prompts).\n2. **If `self.source == \"markdown\"`** (from Task 20), further process the rendered content into the canonical order:\n   - Parse or heuristically split the rendered template into provisional sections:\n     - Use markdown headings or custom delimiters where available (e.g., look for `## PRD`, `## Current Codebase`, `## Instructions`, etc.).\n   - Map content into three buckets:\n     - `context_parts`\n     - `instruction_parts`\n     - `schema_parts` (for `has_json_schema` prompts or schema markers)\n   - Reassemble the final prompt as:\n     1. All `context_parts` (in original order)\n     2. All `instruction_parts`\n     3. All `schema_parts`\n   - Use clear separators (double newlines, headings) to avoid conflation.\n\n3. **If `self.source != \"markdown\"`** (default YAML prompts):\n   - Return the Jinja2-rendered string unchanged to preserve current behavior.\n\nImplementation notes:\n- Keep the schema section **at the end of the prompt** so the model reads all context and instructions before learning about the strict output format, aligning with modern guidance.[3][4]\n- Avoid brittle parsing by starting with simple, well-documented conventions; if headings are absent, treat the entire rendered template as instructions + context and append schema separately when `has_json_schema` is true.\n\n### 4. Integrate JSON_RESPONSE_SCHEMA for Markdown Prompts\n- Import `JSON_RESPONSE_SCHEMA` from the same module (Task 19) and provide it to markdown prompts that set `has_json_schema=True`.\n- For these prompts:\n  - Ensure `render()` includes a **machine-readable schema block** at the end, for example:\n    - A fenced code block with `json` and a compact version of `JSON_RESPONSE_SCHEMA`, or\n    - A textual explanation followed by the schema.\n  - Consider pre-serializing `JSON_RESPONSE_SCHEMA` with `json.dumps(..., indent=2, sort_keys=True)` for readability.\n- Keep this behavior gated by `has_json_schema` to avoid affecting prompts that do not need structured JSON output.\n\n### 5. Backward Compatibility & Configuration\n- Guarantee that existing YAML prompts (from Task 9) continue to work identically:\n  - Do **not** reorder or modify templates when `source == \"yaml\"`.\n- Document behavior in the module docstring and/or `Prompt` docstring:\n  - Explain the new `source` semantics and the markdown assembly pipeline.\n  - Describe how `has_json_schema` changes the rendered output.\n\n### 6. Implementation Best Practices\n- Follow current Python best practices:\n  - Keep `Prompt.render()` focused; if logic grows complex, factor new private helpers like `_assemble_markdown_prompt(...)` and `_append_json_schema(...)`.\n  - Ensure deterministic behavior (no random ordering or environment-dependent behavior).\n- Maintain template-agnostic design:\n  - Avoid tying logic to specific prompt IDs; key on `source` and `has_json_schema` instead.\n\n### 7. Documentation\n- Update internal developer docs or code comments to describe the new canonical order and how to author markdown prompts that take advantage of it.\n- Add examples in comments or tests demonstrating the final assembled prompt shape for markdown prompts with and without schemas.\n",
        "testStrategy": "1. **Unit Tests for Prompt Assembly (tests/test_prompts.py)**\n- Add tests that create `Prompt` instances directly (without file I/O) and call `render()` with dummy values for `prd`, `code_context`, `history`, and `current_stage`.\n\n- `test_yaml_prompt_rendering_unchanged`:\n  - Create a `Prompt` with `source='yaml'` and a simple template that includes context and instructions.\n  - Assert that `render()` returns the Jinja2-rendered string exactly as expected (no reordering or extra sections).\n\n- `test_markdown_prompt_orders_context_before_instructions`:\n  - Create a `Prompt` with `source='markdown'`, `has_json_schema=False`, and a template that intermixes context and instruction markers.\n  - After `render()`, verify that:\n    - The context heading (e.g., `## Context`) appears before the instructions heading (e.g., `## Task`).\n    - All context-related substrings appear before instruction-related substrings.\n\n- `test_markdown_prompt_places_schema_last_when_has_json_schema`:\n  - Create a `Prompt` with `source='markdown'`, `has_json_schema=True`.\n  - Call `render()` and assert that:\n    - A schema marker section (e.g., `## Response JSON Schema`) or fenced `json` block is present.\n    - This schema section appears **after** both context and instruction sections.\n\n- `test_markdown_prompt_without_schema_has_no_schema_section`:\n  - Same as above but with `has_json_schema=False`.\n  - Assert that no schema heading or JSON schema block is appended.\n\n- `test_render_includes_json_response_schema_content` (integration with Task 19):\n  - Patch or import `JSON_RESPONSE_SCHEMA` and call `render()` for a `Prompt` with `has_json_schema=True`.\n  - Parse the schema block from the rendered prompt and `json.loads` it.\n  - Assert that the resulting dict equals `JSON_RESPONSE_SCHEMA` (or is a subset with expected keys such as `summary`, `recommendations`, `tasks`).\n\n2. **Ordering and Idempotency Tests**\n- `test_markdown_prompt_reordering_is_deterministic`:\n  - Call `render()` twice on the same `Prompt` and inputs.\n  - Assert both outputs are identical.\n\n- `test_markdown_prompt_keeps_internal_section_order`:\n  - Ensure that within the context, instruction, and schema groups, the relative order of lines/sections is preserved by `render()`.\n\n3. **Regression Tests for Existing Configs (Task 9)**\n- `test_existing_yaml_prompts_render_successfully`:\n  - Load prompts via the existing `PromptLibrary`.\n  - For each YAML-backed prompt, call `render()` with minimal dummy data.\n  - Assert no exceptions are raised and that previously captured snapshots (or key substrings like `## PRD:`) are still present in the correct positions.\n\n4. **Documentation & Behavior Tests**\n- `test_prompt_source_and_has_json_schema_defaults` (integration with Task 20):\n  - Ensure `Prompt` defaults (`source='yaml'`, `has_json_schema=False`) behave as expected with `render()`.\n\n- If doctest-style examples are added to docstrings, run doctests as part of the CI pipeline to confirm sample outputs reflect the new order.\n",
        "status": "done",
        "dependencies": [
          "3",
          "9",
          "19",
          "20"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:32:53.400Z"
      },
      {
        "id": "22",
        "title": "Improve JSON Parsing Robustness in analysis.py",
        "description": "Enhance the JSON parsing logic in analysis.py with multiple extraction strategies, task normalization using the JSON_RESPONSE_SCHEMA, and fallback unstructured text processing to handle malformed LLM responses reliably.",
        "details": "Update `src/metaagent/analysis.py` to implement robust JSON parsing following best practices for LLM output extraction[1][2][6]:\n\n**1. Add Multi-Strategy JSON Extractor Class:**\n```python\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nimport json\nimport re\nfrom jsonschema import validate, ValidationError\nimport logging\n\n@dataclass\nclass ParsedAnalysisResult:\n    success: bool\n    summary: str = ''\n    recommendations: List[str] = None\n    tasks: List[Dict[str, Any]] = None\n    raw_text: str = ''\n    extraction_method: str = ''\n\nclass JSONExtractor:\n    '''Multi-strategy JSON extractor for LLM responses.'''\n    \n    def __init__(self, schema: Dict[str, Any]):\n        self.schema = schema\n        self.logger = logging.getLogger(__name__)\n    \n    def extract(self, response_text: str) -> ParsedAnalysisResult:\n        '''Try multiple extraction strategies in order of preference.'''\n        strategies = [\n            self._extract_json_block,\n            self._extract_regex_json,\n            self._extract_line_json,\n            self._fallback_unstructured\n        ]\n        \n        for strategy in strategies:\n            try:\n                result = strategy(response_text)\n                if result.success:\n                    self.logger.info(f'Successful extraction with {result.extraction_method}')\n                    return result\n            except Exception as e:\n                self.logger.debug(f'{strategy.__name__} failed: {e}')\n                continue\n        \n        return ParsedAnalysisResult(success=False, raw_text=response_text)\n```\n\n**2. Implement Extraction Strategies:**\n- `_extract_json_block`: Find JSON between ```json ... ``` markers\n- `_extract_regex_json`: Regex `r'\\{[\\s\\S]*?\\}'` for standalone objects[1]\n- `_extract_line_json`: Line-by-line parsing for NDJSON format[1]\n- `_fallback_unstructured`: Extract key fields using regex/heuristics\n\n**3. Add Task Normalization:**\n```python\ndef normalize_tasks(tasks: List[Dict], schema: Dict) -> List[Dict]:\n    '''Normalize extracted tasks against JSON_RESPONSE_SCHEMA from prompts.py[19]'''\n    normalized = []\n    for task in tasks:\n        norm_task = {\n            'title': task.get('title', 'Untitled'),\n            'description': task.get('description', ''),\n            'priority': task.get('priority', 'medium'),\n            'details': task.get('details', ''),\n            'testStrategy': task.get('testStrategy', ''),\n            'dependencies': task.get('dependencies', []),\n        }\n        # Validate against schema subset\n        try:\n            validate(instance=norm_task, schema=schema['properties']['tasks']['items'])\n        except ValidationError:\n            # Apply defaults for missing required fields\n            norm_task['description'] = norm_task['description'] or 'No description provided'\n        normalized.append(norm_task)\n    return normalized\n```\n\n**4. Update AnalysisEngine:**\n- Replace `_parse_response()` with `JSONExtractor.extract()`\n- Import `JSON_RESPONSE_SCHEMA` from `prompts.py`\n- Convert `ParsedAnalysisResult` to `AnalysisResult` with validation\n\n**5. Error Handling & Logging:**\n- Log extraction method used and any parsing errors\n- Return partial results when possible (e.g., summary only)\n- Graceful degradation to mock data in mock mode\n\n**6. Configuration:**\nAdd `json_parsing_strategy: 'strict' | 'lenient' | 'fallback'` to Config.",
        "testStrategy": "Comprehensive unit tests in `tests/test_analysis.py` using pytest and Hypothesis:\n\n1. **Strategy Tests:**\n   ```python\n   @pytest.mark.parametrize('strategy,expected', [\n       ('json_block', True),\n       ('regex_json', True),\n       ('line_json', True),\n       ('fallback', True)\n   ])\n   def test_extraction_strategies(self, strategy, expected):\n       # Test each strategy with malformed LLM responses\n   ```\n\n2. **Malformed Response Tests:**\n   - Test ```json malformed content ``` blocks\n   - Test JSON embedded in prose\n   - Test NDJSON line format\n   - Test completely unstructured text\n\n3. **Schema Normalization Tests:**\n   - Test task normalization adds missing fields\n   - Test jsonschema validation passes for normalized tasks\n   - Test malformed tasks get default values\n\n4. **Integration Tests:**\n   - Test full `analyze()` method with various mock LLM responses\n   - Verify `AnalysisResult` always populated (no None values)\n   - Test logging contains extraction method used\n\n5. **Edge Cases:**\n   - Empty response\n   - Extremely large responses (>1MB)\n   - Unicode characters\n   - Nested JSON arrays/objects[2][3]\n\n6. **Coverage & Performance:**\n   ```bash\n   pytest tests/test_analysis.py --cov=metaagent.analysis --cov-report=html\n   # Ensure >95% coverage of new parsing code\n   # Benchmark extraction time <500ms for 10k token responses\n   ```",
        "status": "done",
        "dependencies": [
          "5",
          "19"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:34:05.168Z"
      },
      {
        "id": "23",
        "title": "Add Integration Test for Codebase Digest Prompt Execution Flow",
        "description": "Create an integration test that exercises the full Codebase Digest prompt execution pipeline: loading a markdown-based prompt, rendering it with context, and verifying JSON schema appending and context–instructions–schema ordering.",
        "details": "Implementation steps:\n\n1. **Choose test location and structure**\n- Add a new integration-style test module, e.g. `tests/test_codebase_digest_integration.py`, keeping it separate from low-level unit tests to reflect its broader scope and slightly higher runtime cost.[1][3][5]\n- Use `pytest` fixtures for any reusable test data (e.g., sample markdown prompt file, sample PRD, code context, history) to keep the test clear and maintainable.[5]\n\n2. **Prepare a realistic markdown prompt fixture**\n- Under `tests/fixtures/prompts/` (or similar), add a markdown file that mimics the real **Codebase Digest** prompt used in production, including:\n  - A **context section** placeholder (e.g., headings for PRD, code, history) that will be filled by `Prompt.render()`.\n  - An **instructions section** describing the analysis or task.\n  - A **JSON schema block** that should be appended or embedded according to `JSON_RESPONSE_SCHEMA`, making sure it follows the agreed structure from Task 19.\n- Ensure the prompt metadata (id, goal, stage, source=\"markdown\", has_json_schema=True) is either:\n  - Loaded through the same mechanism as other prompts (if markdown prompts are already wired into `prompts.py`), or\n  - Constructed explicitly in the test, while still using the real markdown template contents.\n\n3. **Load prompt via existing prompt/profile loading logic**\n- Use the public API from `src/metaagent/prompts.py` (from Task 3 and Task 9) to load prompts/config where possible instead of hard-coding paths, so the integration test validates the **same configuration and loading pipeline** used by the CLI.\n- If Codebase Digest has a dedicated profile or prompt id, load it by id; otherwise, configure a minimal profile/prompt entry in a test-local YAML file under `tests/fixtures/config/` and point the prompt library at that file in the test via environment variable or injection.\n\n4. **Render the prompt with realistic context data**\n- In the test, construct representative dummy values for:\n  - `prd`: small markdown string representing a PRD.\n  - `code_context`: content that would plausibly come from the Repomix/Codebase Digest output (a few file summaries or code blocks).\n  - `history`: previous analysis or empty string.\n  - `current_stage`: the appropriate stage string for Codebase Digest.\n- Call `Prompt.render(prd, code_context, history, current_stage)` and capture the resulting string.\n\n5. **Verify context–instructions–schema ordering contract**\n- Using the ordering rules implemented in Task 21, assert that the rendered output respects **context → instructions → JSON schema**:\n  - Locate key markers or headings (e.g., `## PRD`, `## Current Codebase`, `## Instructions`, `## JSON Response Schema` or similar) using string search or regex.\n  - Assert that all context sections (PRD, code context, history, current stage) appear **before** the instructions segment.\n  - Assert that the JSON schema block appears **after** the instructions section and at the end of the prompt (aside from trailing whitespace).\n- Confirm that the context segments appear in the expected **relative order** (e.g., PRD before code context, code context before history) to maintain a predictable prompt structure.\n\n6. **Verify JSON schema content and integration**\n- Import `JSON_RESPONSE_SCHEMA` from `metaagent.prompts` and serialize it in the same way the production code does when building markdown prompts (e.g., `json.dumps(JSON_RESPONSE_SCHEMA, indent=2, sort_keys=False)` if that matches Task 19’s implementation).\n- Assert that the rendered prompt contains:\n  - A recognizable delimiter or heading introducing the schema section.\n  - The serialized schema (possibly after normalizing whitespace) so the integration test guarantees that **the actual constant** is attached, not a stale or hard-coded schema.\n- Optionally, parse the schema substring back into a dict (using `json.loads`) to validate that what is embedded is syntactically valid JSON.\n\n7. **Validate compatibility with markdown-based prompts**\n- Confirm that the integration test explicitly exercises a prompt instance with `source=\"markdown\"` and `has_json_schema=True` (from Task 20), and that these fields influence the render behavior as expected (e.g., conditional schema appending for markdown prompts only).\n- If the render logic branches based on `source` or `has_json_schema`, assert those branches are indeed taken (e.g., by checking for markdown-specific delimiters or schema markers unique to that path).\n\n8. **Keep the test maintainable and fast**\n- Avoid external dependencies (network calls, CLI invocations); focus on in-process interactions among `prompts.py`, configuration, and the template rendering engine to maintain integration-test speed and determinism.[1][3][5]\n- Use clear test names like `test_codebase_digest_markdown_prompt_full_flow()` and add short docstrings to describe intent.\n- Document in comments that this test is a **contract test** for prompt assembly rules, so future changes to the prompt format are made intentionally and with test updates.\n\n9. **CI and documentation**\n- Ensure the new integration test runs as part of the standard `pytest` suite so regressions in prompt ordering or schema embedding are caught early.[1][5]\n- Add a brief note to the project’s testing or contributing documentation describing this test as the canonical reference for Codebase Digest prompt structure, helping contributors avoid unintended prompt format changes.\n",
        "testStrategy": "1. **Run the integration test module**\n- Execute `pytest tests/test_codebase_digest_integration.py -v` and confirm the new test(s) pass locally.\n\n2. **Verify ordering invariants**\n- Temporarily introduce a deliberate violation (e.g., move the JSON schema before the instructions in `Prompt.render()`) and confirm the integration test fails with a clear assertion error pointing to ordering/context issues.\n\n3. **Verify JSON schema assertions**\n- Change a property name in `JSON_RESPONSE_SCHEMA` in `prompts.py` and ensure the test fails because the expected schema content is no longer present in the rendered prompt, proving the test is bound to the real schema constant.\n- Corrupt the schema section in the prompt template (e.g., remove closing braces) and verify the test fails when attempting to `json.loads` the embedded schema substring, confirming JSON validity checks are effective.\n\n4. **Check markdown-specific behavior**\n- Adjust the prompt instance to have `source=\"yaml\"` and `has_json_schema=False` and confirm that the test (or an additional variant) either:\n  - Fails because the schema is no longer appended, or\n  - Passes a separate assertion verifying that non-markdown prompts do not receive a schema, depending on the expected behavior.\n\n5. **Regression protection in CI**\n- Run the full test suite (`pytest -v`) and confirm overall runtime remains acceptable and that this integration test consistently passes.\n- Enable/verify CI configuration executes this test file so any future change to `Prompt.render()`, `JSON_RESPONSE_SCHEMA`, or the Codebase Digest markdown template that breaks the ordering or schema embedding will be caught automatically.[1][5]\n",
        "status": "done",
        "dependencies": [
          "3",
          "9",
          "19",
          "20",
          "21"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:35:47.015Z"
      },
      {
        "id": "24",
        "title": "Fix Prompt.render() to Use Jinja2 Template Interpolation for YAML Prompts",
        "description": "Update the Prompt.render() method to properly apply Jinja2 template interpolation for YAML prompts containing {{ prd }} and {{ code_context }} placeholders, ensuring backward compatibility with existing prompt configurations.",
        "details": "Update `src/metaagent/prompts.py` Prompt.render() method to consistently use Jinja2 Template rendering for all prompt sources (yaml/markdown) following Jinja2 best practices[1][2][4]:\n\n## Implementation Steps\n\n1. **Modify Prompt.render() signature and logic** to handle YAML template interpolation uniformly:\n```python\nfrom jinja2 import Template, StrictUndefined\n\n@dataclass\nclass Prompt:\n    # ... existing fields ...\n\n    def render(self, prd: str, code_context: str, history: str = '', current_stage: str = '') -> str:\n        \"\"\"Render template with Jinja2 interpolation using provided variables.\"\"\"\n        # Use StrictUndefined to raise errors on missing variables (fail-fast)[2][4]\n        tmpl = Template(self.template, undefined=StrictUndefined)\n        \n        context = {\n            'prd': prd,\n            'code_context': code_context,\n            'history': history,\n            'current_stage': current_stage,\n        }\n        \n        rendered = tmpl.render(**context)\n        \n        # Apply ordering logic from Task 21 for markdown prompts with schema\n        if self.source == 'markdown' and self.has_json_schema:\n            rendered = self._apply_ordering(rendered)\n        \n        return rendered\n```\n\n2. **Preserve Task 21 ordering logic** by extracting context→instructions→schema reordering into a private `_apply_ordering()` method that only applies to `markdown` prompts with `has_json_schema=True`.\n\n3. **Update Jinja2 Environment** (optional but recommended): Consider creating a module-level Jinja2 Environment with `undefined=StrictUndefined` and `trim_blocks=True` for consistent rendering across all prompts[2][5].\n\n4. **Handle edge cases**:\n   - Empty strings for optional params (history, current_stage)\n   - Missing template variables raise clear Jinja2 UndefinedError\n   - Preserve exact YAML template formatting (no auto-escaping needed for prompts)[1]\n\n5. **Maintain backward compatibility**: Existing YAML prompts from Task 9 (e.g., alignment_with_prd) must render identically before/after this fix[9].",
        "testStrategy": "Comprehensive unit and integration tests in `tests/test_prompts.py`:\n\n1. **Unit Tests for YAML Template Interpolation**:\n   - `test_yaml_prompt_jinja_rendering`: Load alignment_with_prd prompt from config/prompts.yaml, verify {{ prd }}, {{ code_context }} placeholders render correctly.\n   - `test_yaml_prompt_all_variables`: Test rendering with all 4 variables (prd, code_context, history, current_stage).\n   - `test_yaml_prompt_missing_variable`: Verify StrictUndefined raises TemplateError on undefined vars.\n\n2. **Backward Compatibility Tests**:\n   - `test_yaml_prompt_rendering_unchanged`: Compare before/after render output for existing YAML prompts (must match exactly).\n\n3. **Cross-Source Compatibility**:\n   - `test_render_source_independent`: Verify yaml and markdown prompts both render correctly without interference.\n   - `test_markdown_ordering_preserved`: Confirm Task 21 ordering logic still applies only to markdown+schema prompts.\n\n4. **Integration Test**:\n   - Extend Task 23 integration test to include YAML prompt rendering in full pipeline.\n\n5. **Run full suite**: `pytest tests/test_prompts.py -v --cov=metaagent.prompts` (100% coverage, all pass).",
        "status": "done",
        "dependencies": [
          "3",
          "9",
          "19",
          "20",
          "21"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:46:49.237Z"
      },
      {
        "id": "25",
        "title": "Implement Staged Pipeline Execution with Profiles.yaml Default",
        "description": "Replace the single refine() method with a deterministic stage-based execution flow driven by profiles.yaml as the default behavior, while preserving iterative triage mode as an optional --iterative flag.",
        "details": "Implement staged pipeline execution in `src/metaagent/orchestrator.py` following CI/CD best practices for deterministic, sequential stage execution with quality gates[1][2][4].\n\n## Core Implementation Steps\n\n### 1. Update Orchestrator Class\nAdd stage execution tracking and profile-driven flow:\n```python\n@dataclass\nclass PipelineExecution:\n    profile_name: str\n    stages: list[StageResult] = field(default_factory=list)\n    is_iterative: bool = False\n    current_stage_idx: int = 0\n\nclass Orchestrator:\n    def __init__(self, config: Config):\n        # ... existing init\n        self.execution = None\n\n    def refine(self, profile: str = 'default', iterative: bool = False) -> PipelineExecution:\n        \"\"\"Main entrypoint: deterministic profile-driven OR iterative triage.\"\"\"\n        self.execution = PipelineExecution(profile_name=profile, is_iterative=iterative)\n        \n        if iterative:\n            return self._refine_iterative(profile)\n        \n        return self._refine_staged(profile)\n```\n\n### 2. Implement `_refine_staged()` - Deterministic Flow\n```python\n    def _refine_staged(self, profile: str) -> PipelineExecution:\n        \"\"\"Execute stages deterministically per profiles.yaml order.\"\"\"\n        prompts = self.prompt_library.get_prompts_for_profile(profile)\n        \n        for idx, prompt in enumerate(prompts):\n            self.execution.current_stage_idx = idx\n            stage_result = self._execute_single_stage(prompt, idx)\n            self.execution.stages.append(stage_result)\n            \n            # Quality gate: skip remaining stages if critical failure\n            if not stage_result.analysis_result.success:\n                logger.warning(f\"Stage '{prompt.id}' failed - stopping pipeline\")\n                break\n        \n        self.plan_writer.write_plan(self.prd_content, self.execution.stages)\n        return self.execution\n```\n\n### 3. Implement `_execute_single_stage()`\n```python\n    def _execute_single_stage(self, prompt: Prompt, stage_idx: int) -> StageResult:\n        \"\"\"Execute single stage with full context of prior stages.\"\"\"\n        history = self._build_stage_history(stage_idx)\n        \n        rendered_prompt = prompt.render(\n            prd=self.prd_content,\n            code_context=self.repo_content,\n            history=history,\n            current_stage=prompt.stage\n        )\n        \n        analysis_result = self.analysis_engine.analyze(rendered_prompt)\n        return StageResult(\n            stage_name=prompt.stage,\n            prompt_id=prompt.id,\n            result=analysis_result\n        )\n```\n\n### 4. Update CLI Integration (`src/metaagent/cli.py`)\n```python\n@app.command('refine')\ndef refine(\n    profile: str = typer.Option('default', '--profile'),\n    iterative: bool = typer.Option(False, '--iterative'),\n    # ... other options\n):\n    orchestrator = Orchestrator(config)\n    result = orchestrator.refine(profile=profile, iterative=iterative)\n    \n    console.print(f\"✅ Pipeline {'[Iterative]' if result.is_iterative else '[Staged]'}: {len(result.stages)} stages completed\")\n```\n\n### 5. Enhance Profile Loading (`src/metaagent/prompts.py`)\nEnsure `profiles.yaml` supports stage ordering and dependencies:\n```yaml\nprofiles:\n  default:\n    stages:\n      - prompt: prd_analysis\n        stage: analysis\n      - prompt: arch_review\n        stage: architecture\n        depends_on: analysis  # Optional stage dependency\n      - prompt: implementation_plan\n        stage: planning\n```\n\n### 6. Add Stage History Building\n```python\n    def _build_stage_history(self, current_idx: int) -> str:\n        \"\"\"Build markdown history of prior stage results for context.\"\"\"\n        if not self.execution.stages:\n            return \"No prior stages executed.\"\n            \n        history = \"## Previous Stage Results\\n\\n\"\n        for stage in self.execution.stages[:current_idx]:\n            history += f\"### {stage.stage_name}\\n{stage.result.summary}\\n\\n\"\n        return history\n```\n\n## Best Practices Incorporated\n- **Deterministic execution**: Fixed stage order from profiles.yaml[4]\n- **Stage isolation**: Each stage produces self-contained StageResult[1]\n- **Progressive context**: Each stage receives full prior history[2]\n- **Quality gates**: Early termination on critical failures[3]\n- **Unified result tracking**: PipelineExecution dataclass\n\n## Backward Compatibility\n- Keep `refine_iterative()` unchanged (Task 14)\n- Default to staged mode (`--iterative` flag required for triage)",
        "testStrategy": "Comprehensive unit and integration tests in `tests/test_orchestrator_staged.py`:\n\n### Unit Tests\n1. **Test `_refine_staged()` executes all stages in profile order**:\n   ```python\n   def test_refine_staged_full_pipeline(mock_analysis_engine):\n       prompts = [mock_prompt('analysis'), mock_prompt('planning')]\n       lib = MockPromptLibrary(prompts_for_profile=prompts)\n       orch = Orchestrator(config, prompt_library=lib)\n       result = orch._refine_staged('test')\n       assert len(result.stages) == 2\n       assert result.stages[0].prompt_id == 'analysis'\n   ```\n\n2. **Test quality gate stops on analysis failure**:\n   ```python\n   def test_refine_staged_stops_on_failure(mock_analysis_engine):\n       mock_analysis_engine.analyze.side_effect = [success_result(), failure_result()]\n       result = orch._refine_staged('test')\n       assert len(result.stages) == 1  # Stops after first failure\n   ```\n\n3. **Test history building accumulates correctly**:\n   ```python\n   def test_build_stage_history():\n       orch.execution.stages = [stage1_result(), stage2_result()]\n       history = orch._build_stage_history(2)\n       assert '## Previous Stage Results' in history\n       assert stage1_result().summary in history\n   ```\n\n### Integration Tests\n4. **Test CLI staged vs iterative modes** (`tests/test_cli.py`):\n   ```python\n   def test_refine_staged_mode(cli_runner):\n       result = cli_runner.invoke(app, ['refine', '--profile', 'default'])\n       assert result.exit_code == 0\n       assert '[Staged]' in result.stdout\n   \n   def test_refine_iterative_mode(cli_runner):\n       result = cli_runner.invoke(app, ['refine', '--iterative'])\n       assert '[Iterative]' in result.stdout\n   ```\n\n5. **Test plan_writer receives complete PipelineExecution**:\n   ```python\n   def test_plan_writer_staged_pipeline(plan_writer, mock_stages):\n       execution = PipelineExecution('default', mock_stages)\n       plan_writer.write_plan(prd_content, execution.stages)\n       with open(plan_writer.plan_path) as f:\n           assert '2 stages completed' in f.read()\n   ```\n\n### Edge Cases\n6. Test empty profile returns early error\n7. Test single-stage profile executes correctly\n8. Test profile with stage dependencies skips invalid paths\n9. Test mock mode works with staged execution\n10. Test history rendering with zero prior stages",
        "status": "done",
        "dependencies": [
          "3",
          "7",
          "8",
          "14"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:51:12.424Z"
      },
      {
        "id": "26",
        "title": "Improve Claude Code Handoff: Generate Structured .meta-agent-tasks.md",
        "description": "Implement a generator that produces a well-structured .meta-agent-tasks.md file for Claude Code, including a clear instruction header, prioritized task list with checkboxes, and explicit file paths for each task.",
        "details": "## Goal\nCreate a robust handoff artifact for Claude Code by generating a `.meta-agent-tasks.md` file that encodes the orchestrator’s implementation plan in a concise, agent-friendly format, following modern AGENTS.md-style best practices.[1][2][3][4]\n\n## Target Behavior\n- When the orchestrator decides to invoke Claude Code (via the existing ClaudeCodeRunner), it must:\n  - Assemble a **single markdown file** `.meta-agent-tasks.md` in the project root (or configured workdir).\n  - Include a **Claude-specific instruction header** (persona, goals, constraints, commands) tailored for code-editing workflows, similar to AGENTS.md patterns.[1][3][4]\n  - Emit a **prioritized checklist of tasks**, each with:\n    - Markdown checkbox (`- [ ]` / `- [x]`).\n    - A short, action-oriented title.\n    - One-line summary.\n    - **Explicit file paths** and operation hints (edit, create, delete, refactor).[1][2]\n    - Optional tags (e.g. `[high-priority]`, `[refactor]`, `[tests]`).\n  - Order tasks by priority and dependency where known (high → medium → low, and parent tasks before children).\n\n## Implementation Steps\n\n### 1. Identify Input Data & Extension Points\n- Reuse the **normalized task structures** produced from the analysis stage and JSON parsing improvements in `analysis.py` (Task 22), ensuring the generator consumes a schema-stable representation.[1][2][6]\n- In `src/metaagent/orchestrator.py` (where Claude Code is invoked from Task 16 and where staged pipelines from Task 25 will live):\n  - Add a dedicated method, e.g. `_build_claude_task_handoff(self, tasks: list[PlannedTask]) -> str` that returns markdown content for `.meta-agent-tasks.md`.\n  - Call this method in the stage that prepares implementation for Claude Code (preferably within or just before `_implement_with_claude()`), passing the already-ranked/recommended tasks.\n\n### 2. Define an Internal Task Model for Handoff\n- Introduce a lightweight dataclass (or reuse existing) in a suitable module (e.g., `src/metaagent/models.py` or near orchestrator):\n  ```python\n  @dataclass\n  class ClaudeHandoffTask:\n      id: str\n      title: str\n      description: str\n      priority: str  # \"high\" | \"medium\" | \"low\"\n      file_paths: list[str]\n      done: bool = False\n      tags: list[str] = field(default_factory=list)\n  ```\n- Provide a converter from the analysis/planning representation (from Task 22 and pipeline stages from Task 25) into `ClaudeHandoffTask`, normalizing priority labels and extracting file paths using heuristics:\n  - Prefer explicit `file_path` / `paths` fields from the task structure.\n  - Fallback to regex scanning for `src/`, `tests/`, `config/` path-like substrings in task descriptions.\n\n### 3. Markdown Structure & Content\nFollow AGENTS.md-style patterns so that Claude Code has a predictable, parseable structure.[1][2][3][4]\n\n**3.1 File Layout**\nUse a consistent, minimal hierarchy:\n```md\n# Claude Code Implementation Tasks\n\n> Do not edit this file manually. Generated by MetaAgent orchestrator.\n\n## Instructions for Claude Code\n\n- Role: You are an AI coding agent editing this repository.\n- Goals:\n  - Implement the tasks in the checklist below.\n  - Keep diffs small and focused.\n  - Prefer editing existing files over large rewrites.\n- Constraints:\n  - Follow project style guides, tests, and build commands.\n  - Ask for clarification in comments when requirements are unclear.\n\n## Checklist\n\n1. [ ] [high] Implement staged pipeline execution in `src/metaagent/orchestrator.py`\n   - Summary: Add `_refine_staged()` using profiles.yaml.\n   - Files: `src/metaagent/orchestrator.py`, `config/profiles.yaml`.\n\n2. [ ] [medium] Update prompt rendering for Jinja2.\n   - Summary: Fix `Prompt.render()` to use Jinja2 for YAML prompts.\n   - Files: `src/metaagent/prompts.py`, `tests/test_prompts.py`.\n```\n\n**3.2 Instruction Header Best Practices**\n- Keep the header **short and actionable** (per AGENTS.md guidelines).[1][2][3][4]\n- Include:\n  - Role + capabilities.\n  - High-level workflow (\"read checklist → apply changes → run tests if feasible\").\n  - Critical **do/don’t rules** (e.g., avoid adding heavy deps, prefer small diffs) if available from other project config.\n  - Standard commands (build/test) *if known* from existing config (may later be populated from AGENTS.md or similar files).[1][2][3][4]\n\n### 4. Priority & Ordering Logic\n- Implement a deterministic sort:\n  ```python\n  PRIORITY_ORDER = {\"high\": 0, \"medium\": 1, \"low\": 2}\n\n  def sort_key(t: ClaudeHandoffTask) -> tuple[int, str]:\n      return (PRIORITY_ORDER.get(t.priority, 1), t.id)\n  ```\n- Map any numeric or non-standard priorities to `high`/`medium`/`low`.\n- Optionally group tasks by stage/profile when running under the staged pipeline (Task 25):\n  - E.g., prefix titles with stage name: `[analysis]`, `[implementation]`, `[tests]`.\n\n### 5. File Generation & Location\n- In the orchestrator (implementation stage):\n  - Compute the project root / working directory already used for Claude Code.\n  - Write the generated markdown to `<workdir>/.meta-agent-tasks.md`.\n  - Ensure the file is overwritten on each run to avoid stale content.\n- Consider adding a configuration option (e.g. `handoff_tasks_path`) for advanced users, defaulting to `.meta-agent-tasks.md`.\n\n### 6. Robustness & Edge Cases\n- If **no tasks** are available, either:\n  - Skip file generation and log a warning; or\n  - Generate a file with an empty checklist and a note that implementation is currently a no-op.\n- Sanitize file paths and text:\n  - Ensure lines stay within a reasonable length.\n  - Escape backticks and other markdown-sensitive characters in summaries.\n- Keep the file **small and scoped** as recommended for AGENTS-style docs.[1][2][4][5]\n\n### 7. Integration with Staged Pipeline (Task 25)\n- Ensure `_build_claude_task_handoff()` can receive stage context (e.g. current profile, stage name), so tasks can carry stage metadata.\n- When running staged execution:\n  - Generate or refresh `.meta-agent-tasks.md` in the stage that triggers Claude Code.\n  - Make sure this behavior is deterministic given the same profile and analysis output.\n\n### 8. Documentation\n- Add a short section to the project README or developer docs explaining:\n  - Purpose of `.meta-agent-tasks.md`.\n  - When it is generated.\n  - Rough structure so contributors know what to expect.\n\n",
        "testStrategy": "1. **Unit Tests for Markdown Generation**\n- Add `tests/test_claude_handoff_tasks_md.py`.\n- Test that `_build_claude_task_handoff()` given a list of `ClaudeHandoffTask` instances:\n  - Produces a string starting with `# Claude Code Implementation Tasks` and a `## Instructions for Claude Code` section.\n  - Contains a `## Checklist` section with one numbered item per input task.\n  - Uses `- [ ]` or numbered checklist lines including the priority tag and file paths.\n  - Correctly escapes markdown special characters in titles/descriptions.\n\n2. **Priority & Ordering Tests**\n- Verify tasks are sorted by priority `high` → `medium` → `low` regardless of input order.\n- Verify deterministic ordering within the same priority (e.g., by id).\n\n3. **File Path & Extraction Tests**\n- Using synthetic planned tasks (mirroring Task 22’s normalized schema), verify that:\n  - Explicit `file_paths` are rendered into the `Files:` line.\n  - When only a description mentioning paths is available, the heuristic still extracts plausible file paths.\n\n4. **Integration Test with Orchestrator + Claude Runner**\n- In a new integration test module (e.g. `tests/test_claude_handoff_integration.py`):\n  - Mock the analysis/planning stage to return a small list of tasks.\n  - Mock `ClaudeCodeRunner` from Task 16 to avoid real CLI calls.\n  - Run the relevant orchestrator stage (or `_implement_with_claude()` wrapper) and assert that:\n    - `.meta-agent-tasks.md` is created in the expected working directory.\n    - Its contents match expected structure and include the tasks’ titles and file paths.\n\n5. **Staged Pipeline Behavior Tests (with Task 25)**\n- Under a staged profile, verify that invoking the implementation stage:\n  - Regenerates `.meta-agent-tasks.md` when tasks change.\n  - Keeps ordering stable for the same input.\n\n6. **Edge Case Tests**\n- No tasks: confirm function either returns a minimal file with an explanatory note or that the orchestrator skips generation cleanly with a logged warning.\n- Mixed/unknown priority values: confirm they are normalized and do not cause crashes.\n\n7. **Style & Static Checks**\n- Ensure new modules and tests pass existing tooling: linting, formatting, and test suite, consistent with project standards used in related tasks.",
        "status": "done",
        "dependencies": [
          "16",
          "22",
          "25"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:52:05.524Z"
      },
      {
        "id": "27",
        "title": "Add Profile Validation with Load-Time Checks and --validate Command",
        "description": "Implement validation for profiles.yaml to check for missing prompts on load with warnings, and add a 'list-profiles --validate' CLI command for explicit validation.",
        "details": "Implement comprehensive YAML validation for profiles.yaml in `src/metaagent/prompts.py` following YAML best practices (yamllint-style checks, schema validation) and Kubernetes-inspired config validation patterns[1][2].\n\n## Implementation Steps\n\n### 1. Define Profile Schema\nAdd a `PROFILE_SCHEMA` constant using JSON Schema draft-07 (similar to JSON_RESPONSE_SCHEMA from Task 19):\n```python\nfrom jsonschema import Draft7Validator, validate, ValidationError\nimport yaml\nfrom pathlib import Path\nfrom typing import Dict, Any, List\n\nPROFILE_SCHEMA = {\n    '$schema': 'http://json-schema.org/draft-07/schema#',\n    'type': 'object',\n    'patternProperties': {\n        '^[a-zA-Z0-9_-]+$': {  # profile names\n            'type': 'object',\n            'properties': {\n                'stages': {\n                    'type': 'array',\n                    'items': {'type': 'string', 'pattern': '^[a-zA-Z0-9_-]+$'}\n                }\n            },\n            'required': ['stages'],\n            'additionalProperties': False\n        }\n    },\n    'additionalProperties': False\n}\n```\n\n### 2. Add Validation Function\n```python\ndef validate_profiles_yaml(profiles_path: Path) -> List[str]:\n    \"\"\"Validate profiles.yaml structure and content. Returns list of warnings.\"\"\"\n    warnings = []\n    \n    if not profiles_path.exists():\n        return ['profiles.yaml not found']\n    \n    try:\n        with open(profiles_path) as f:\n            data = yaml.safe_load(f)\n        \n        # Schema validation\n        validate(instance=data, schema=PROFILE_SCHEMA)\n    except ValidationError as e:\n        warnings.append(f'Schema error: {str(e.message)}')\n    except Exception as e:\n        warnings.append(f'YAML parse error: {str(e)}')\n    \n    # Semantic validation: check referenced prompts exist\n    prompts_path = profiles_path.parent / 'prompts.yaml'\n    if prompts_path.exists():\n        try:\n            with open(prompts_path) as f:\n                prompts_data = yaml.safe_load(f)\n            available_prompts = set(prompts_data.get('prompts', {}).keys())\n            \n            for profile_name, profile in data.items():\n                for stage in profile.get('stages', []):\n                    if stage not in available_prompts:\n                        warnings.append(f'Profile \"{profile_name}\" references missing prompt: {stage}')\n        except Exception as e:\n            warnings.append(f'Could not validate prompts.yaml: {str(e)}')\n    else:\n        warnings.append('prompts.yaml not found - cannot validate prompt references')\n    \n    return warnings\n```\n\n### 3. Update Load Functions\nModify existing load functions from Task 3:\n```python\ndef load_profiles(profiles_path: Path) -> Dict[str, Any]:\n    warnings = validate_profiles_yaml(profiles_path)\n    if warnings:\n        logging.warning('Profile validation warnings:')\n        for w in warnings:\n            logging.warning(f'  - {w}')\n    \n    with open(profiles_path) as f:\n        return yaml.safe_load(f)\n```\n\n### 4. Add CLI Command\nUpdate CLI (likely in `src/metaagent/cli.py` or main module):\n```python\nimport click\n\n@click.command()\n@click.option('--validate', is_flag=True, help='Validate profiles.yaml structure and prompt references')\ndef list_profiles(validate: bool):\n    profiles_path = Path('config') / 'profiles.yaml'\n    if validate:\n        warnings = validate_profiles_yaml(profiles_path)\n        if warnings:\n            click.echo('Validation warnings:', err=True)\n            for w in warnings:\n                click.echo(f'  ⚠️  {w}', err=True)\n            return 1\n        click.echo('✅ profiles.yaml validation passed')\n        return 0\n    # Existing list_profiles logic\n```\n\n## Best Practices Incorporated\n- **Schema validation** using JSON Schema draft-07[1][2]\n- **Semantic validation** checking prompt references exist\n- **Graceful warnings** instead of hard failures for development workflow\n- **CLI integration** for explicit validation (CI/CD friendly)[1][2]\n- **Consistent 2-space indentation** and error reporting",
        "testStrategy": "Comprehensive unit and integration tests in `tests/test_prompts_validation.py`:\n\n1. **Unit Tests for `validate_profiles_yaml()`**:\n   ```python\n   @pytest.mark.parametrize('warnings_expected', [\n       ([], 'valid_profiles.yaml'),\n       (['Profile \"test\" references missing prompt: nonexistent_stage'], 'profiles_with_missing_prompt.yaml'),\n       (['Schema error: ...'], 'invalid_schema_profiles.yaml'),\n   ])\n   def test_validate_profiles_yaml(tmp_path):\n       # Create test files and validate\n   ```\n\n2. **Test Load-Time Warnings**:\n   - `test_load_profiles_with_warnings`: Mock `logging.warning`, verify warnings logged but loading succeeds\n   - `test_load_profiles_valid`: No warnings logged\n\n3. **CLI Integration Tests**:\n   ```bash\n   # Test pytest-capture\n   pytest tests/test_prompts_validation.py::test_cli_list_profiles_validate_pass\n   pytest tests/test_prompts_validation.py::test_cli_list_profiles_validate_fail -s\n   ```\n   - Verify `--validate` flag returns warnings and exit code 1 on errors\n   - Verify clean exit code 0 on valid files\n\n4. **Integration Test**:\n   - Create real `config/profiles.yaml` + `config/prompts.yaml` fixture\n   - Run `load_profiles()` + `list-profiles --validate`\n   - Verify end-to-end validation flow\n\n5. **Edge Cases**:\n   - Missing files\n   - Malformed YAML\n   - Empty profiles\n   - Cross-references between profiles\n\nRun: `pytest tests/test_prompts_validation.py -v` and confirm 100% pass rate.",
        "status": "done",
        "dependencies": [
          "3",
          "19",
          "25"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:56:07.999Z"
      },
      {
        "id": "28",
        "title": "Create Integration Tests for Full Refinement Pipeline with Profile-Based Staged Execution and Mock Mode",
        "description": "Develop comprehensive integration tests that validate the complete refinement pipeline using profile-based staged execution in mock mode, ensuring correct workflow coordination without external API dependencies.",
        "details": "Create `tests/integration/test_refinement_pipeline.py` with pytest integration tests following best practices for automated integration testing: modular reusable test cases, isolated Docker-like environments via tmp_path fixtures, and CI/CD pipeline compatibility[1][3].\n\n**Key Implementation Requirements:**\n\n1. **Test Full Pipeline with Different Profiles:**\n   - Test `default`, `quick`, and `comprehensive` profiles from `config/profiles.yaml` (Task 9)\n   - Verify orchestrator executes correct stages per profile using mock analysis engine\n   ```python\nfrom pytest import mark\nimport pytest\nfrom pathlib import Path\nfrom typer.testing import CliRunner\nfrom metaagent.cli import app\nfrom metaagent.orchestrator import Orchestrator\n\n@mark.integration\nclass TestRefinementPipeline:\n    @pytest.fixture\ndef sample_repo(self, tmp_path):\n        # Create complete sample repo with PRD (reuse Task 12 fixture)\n        repo = tmp_path / 'test_repo'\n        repo.mkdir()\n        (repo / 'docs' / 'prd.md').write_text('# Test PRD\\nREQ1: Feature 1')\n        return repo\n\n    def test_default_profile_pipeline(self, sample_repo, monkeypatch):\n        # Mock Perplexity API and analysis engine\n        monkeypatch.setenv('PERPLEXITY_API_KEY', 'mock')\n        monkeypatch.setattr('metaagent.analysis.create_analysis_engine', self.mock_analysis)\n        \n        result = runner.invoke(app, ['refine', str(sample_repo), '--profile', 'default', '--mock'])\n        assert result.exit_code == 0\n        assert (sample_repo / 'output' / 'mvp_improvement_plan.md').exists()\n```\n\n2. **Mock Mode Validation:**\n   - Verify `--mock` flag bypasses real API calls (Task 8 CLI)\n   - Mock `RepomixRunner.pack()` to return consistent sample codebase\n   - Mock analysis engine to return `JSON_RESPONSE_SCHEMA` compliant results (Task 19)\n\n3. **Staged Execution Verification:**\n   - Test stages execute in profile-defined order\n   - Verify `RunHistory` tracks previous analysis (Task 7)\n   - Validate `PlanWriter` aggregates stage results correctly (Task 6)\n\n4. **Edge Cases and Error Handling:**\n   - Test missing PRD file\n   - Test invalid profile name\n   - Test repomix failure with graceful degradation\n   - Test partial stage failures\n\n5. **Performance and Isolation:**\n   - Use `tmp_path` fixtures for complete isolation\n   - Add `@pytest.mark.integration` and timeout decorators\n   - Ensure <30s total execution time for CI compatibility[1]\n\n**Dependencies to Mock:**\n- Task 7 (Orchestrator) - core workflow\n- Task 9 (Configs) - profiles and prompts\n- Task 12 (E2E fixtures) - sample repo setup\n- Task 19 (JSON schema) - mock responses",
        "testStrategy": "**Comprehensive Integration Test Verification:**\n\n1. **Execute Integration Tests:**\n   ```bash\n   pytest tests/integration/ -m integration -v --tb=short\n   ```\n   All tests must pass without API keys.\n\n2. **Coverage Analysis:**\n   ```bash\n   pytest tests/integration/ --cov=src/metaagent/orchestrator --cov=src/metaagent/plan_writer --cov-report=term-missing\n   ```\n   Target >90% coverage of orchestrator and plan_writer modules.\n\n3. **Profile-Specific Validation:**\n   - Verify `default` profile: 3 stages executed\n   - Verify `quick` profile: 1 stage executed\n   - Verify `comprehensive` profile: 5 stages executed\n   - Check generated `mvp_improvement_plan.md` contains profile-specific content\n\n4. **Mock Mode Isolation:**\n   - Confirm zero external API calls (network disabled)\n   - Verify all mocks return `JSON_RESPONSE_SCHEMA` compliant data\n   - Test runs successfully with `PERPLEXITY_API_KEY=invalid`\n\n5. **CI/CD Pipeline Compatibility:**\n   - Tests complete in <30 seconds\n   - No external dependencies (Docker not required)\n   - GitHub Actions compatible (no privileged mode)\n\n6. **Regression Testing:**\n   - Rerun after core module changes\n   - Verify backwards compatibility with existing profiles\n\n7. **Artifact Validation:**\n   - `output/mvp_improvement_plan.md` exists and is readable\n   - Contains PRD summary, stage results, prioritized tasks\n   - Includes Claude Code implementation instructions",
        "status": "done",
        "dependencies": [
          "7",
          "8",
          "9",
          "10",
          "12"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:59:53.642Z"
      },
      {
        "id": "29",
        "title": "Add --dry-run Flag to Preview Analysis Plan Without API Calls",
        "description": "Add a --dry-run CLI flag that executes the full planning/orchestration pipeline without invoking external LLM APIs, printing the prompts that would be sent and an estimated token usage instead of making real calls.",
        "details": "## Goal\nIntroduce a **`--dry-run`** flag to the main CLI that:\n- Runs the same orchestration and prompt selection logic as a real execution.\n- **Never performs real LLM API calls** (relying on existing mock/mode mechanisms from the analysis engine).\n- Outputs, in a human-readable format:\n  - The **rendered prompts** that would be sent for each stage/profile.\n  - A **per-prompt and total token estimate**, based on approximate tokenization best practices.\n\n## Design & Implementation\n\n1. **CLI Surface & Plumbing**\n- Extend the main CLI entrypoint (likely in `src/metaagent/cli.py` or equivalent) to accept a boolean `--dry-run` flag following standard CLI UX conventions (e.g. `--dry-run` only, no short option to avoid accidents).\n- Ensure `--dry-run` is **orthogonal** to profile selection and other flags; it should work with any profile or mode.\n- Thread a `dry_run: bool` parameter through the orchestration layer into the analysis / refinement pipeline, not just the LLM call site, to ensure decisions can adapt based on dry-run mode when needed.\n\n2. **Integration with Analysis Engine Mock Mode**\n- Reuse the existing **mock mode** in `AnalysisEngine` from `analysis.py` so that in `--dry-run` mode the system never attempts real HTTP/LLM calls.[5][1]\n- Add a clear entry point (e.g. `create_analysis_engine(use_mock: bool, dry_run: bool = False, ...)`) so `dry_run=True` implies `use_mock=True`, but is still logically distinct to allow different behavior (mock may simulate responses; dry-run should not even pretend to call).\n- For `--dry-run`, prefer **non-blocking, deterministic behavior**: avoid random or heavyweight mock responses; focus on structural correctness.\n\n3. **Prompt Capture & Display**\n- At the orchestrator level (where prompts from `Prompt.render()` and profiles are orchestrated), introduce a small data structure, e.g.:\n  ```python\n  @dataclass\n  class PlannedCall:\n      stage: str\n      prompt_id: str\n      profile: str\n      rendered_prompt: str\n      estimated_tokens: int\n  ```\n- Whenever the pipeline would normally call `AnalysisEngine.analyze(rendered_prompt)`, in `dry_run` mode instead:\n  - Instantiate a `PlannedCall` entry.\n  - Append it to a list attached to the run context.\n- After orchestration completes, pretty-print the plan:\n  - Group by **profile** and **stage**.\n  - Show `prompt_id`, a short **preview** of the prompt (first N lines or characters), and the **token estimate**.\n  - Optionally provide a `--dry-run-format=json` extension point in the design (even if not implemented now) to later support machine-readable output.\n\n4. **Token Estimation Strategy**\n- Implement a light-weight token estimator instead of full tokenizer for now, following common practice:\n  - Option A: Integrate a real tokenizer for your LLM (e.g. `tiktoken` for OpenAI-style models) when available.[2]\n  - Option B (fallback / default): approximate tokens as `len(text.split()) * 1.3` or similar heuristic, clearly labeled as an estimate.[6]\n- Encapsulate this logic in a utility function in a shared module, e.g. `metaagent.tokens.estimate_tokens(text: str) -> int` so it can be reused by future cost estimation features.\n- Compute:\n  - `estimated_tokens` per `PlannedCall`.\n  - A **total token estimate** across the entire run and print it at the end.\n\n5. **Separation of Concerns & Extensibility**\n- Keep **orchestration logic** identical between dry-run and real run except for the IO boundary; side-effectful calls (LLM, network, file writes that aren’t required for planning) must be gated on `if not dry_run`.[3]\n- Consider a small, centralized helper, e.g. `run_analysis_step(prompt: Prompt, context: RunContext)`, that internally checks `dry_run` to either:\n  - Call into `AnalysisEngine.analyze()` (normal mode).\n  - Or record a `PlannedCall` and return a **minimal synthetic AnalysisResult** (empty tasks/recommendations) that lets the pipeline proceed structurally in dry-run.\n- Ensure this is compatible with the **profile-based staged execution** used by the refinement pipeline so integration tests can reuse this behavior later.\n\n6. **Output UX & Safety Considerations**\n- Prefix dry-run output with a clear banner, e.g.:\n  - `*** DRY RUN: No external API calls were made. This is a preview of planned analysis steps. ***`\n- For each step, show something like:\n  - `Profile: default | Stage: alignment | Prompt: alignment_with_prd`\n  - `Estimated tokens: 123`\n  - `--- Prompt Preview (first 40 lines) ---` followed by truncated prompt.\n- Ensure **no secrets** (API keys, tokens) are ever printed; prompts should be safe by design, but avoid dumping raw environment variables.\n\n7. **Documentation & Developer Notes**\n- Update CLI help (`--help`) to describe `--dry-run` clearly, emphasizing that:\n  - It does not modify the codebase.\n  - It is intended for **previewing the analysis plan and understanding token impact**.\n- Add inline docstrings/comments near the orchestrator and token estimator to explain where to hook in future enhancements (e.g., cost estimation per provider, alternate LLM backends).\n\n## Implementation Sketch (Illustrative)\n\n```python\n# cli.py\n\n@click.command()\n@click.option(\"--profile\", default=\"default\", help=\"Execution profile\")\n@click.option(\"--dry-run\", is_flag=True, help=\"Preview prompts and token estimates without API calls\")\ndef run(profile: str, dry_run: bool) -> None:\n    ctx = OrchestrationContext(profile=profile, dry_run=dry_run)\n    run_refinement_pipeline(ctx)\n\n# orchestrator.py\n\ndef run_refinement_pipeline(ctx: OrchestrationContext) -> None:\n    planned_calls: list[PlannedCall] = []\n    ctx.planned_calls = planned_calls\n\n    for stage_prompt in get_stage_prompts(ctx.profile):\n        rendered = stage_prompt.render(\n            prd=ctx.prd,\n            code_context=ctx.code_context,\n            history=ctx.history,\n            current_stage=stage_prompt.stage,\n        )\n        if ctx.dry_run:\n            est_tokens = estimate_tokens(rendered)\n            ctx.planned_calls.append(\n                PlannedCall(\n                    stage=stage_prompt.stage,\n                    prompt_id=stage_prompt.id,\n                    profile=ctx.profile,\n                    rendered_prompt=rendered,\n                    estimated_tokens=est_tokens,\n                )\n            )\n            # Return a minimal synthetic result to keep pipeline shape\n            result = AnalysisResult(summary=\"\", recommendations=[], tasks=[])\n        else:\n            engine = create_analysis_engine(use_mock=ctx.use_mock, dry_run=False)\n            result = engine.analyze(rendered)\n\n        ctx = handle_stage_result(ctx, stage_prompt, result)\n\n    if ctx.dry_run:\n        print_dry_run_report(ctx.planned_calls)\n\n# tokens.py\n\nimport math\n\ndef estimate_tokens(text: str) -> int:\n    # Simple heuristic; pluggable for real tokenizer later\n    words = text.split()\n    return int(math.ceil(len(words) * 1.3))\n```\n\nThis sketch should be aligned with existing abstractions (names/files may differ) but demonstrates the separation between orchestration, analysis, and dry-run behavior.\n",
        "testStrategy": "1. **Unit Tests for Token Estimation Utility**\n- Add tests in `tests/test_tokens.py` (or equivalent):\n  - `test_estimate_tokens_empty_string_returns_zero_or_small_number`.\n  - `test_estimate_tokens_increases_with_text_length`.\n  - `test_estimate_tokens_is_deterministic_for_same_input`.\n\n2. **Unit Tests for Orchestrator Dry-Run Behavior**\n- In `tests/test_orchestrator_dry_run.py` (or expand existing orchestrator tests):\n  - `test_dry_run_records_planned_calls_instead_of_calling_engine`:\n    - Use a dummy `Prompt` and mock `create_analysis_engine`.\n    - Run pipeline with `dry_run=True`.\n    - Assert engine is never instantiated or called.\n    - Assert `context.planned_calls` contains an entry per expected stage.\n  - `test_dry_run_generates_synthetic_analysis_results`:\n    - Verify downstream pipeline functions receive non-None `AnalysisResult` objects with empty fields, and do not crash.\n\n3. **Unit Tests for CLI Flag Wiring**\n- In `tests/test_cli_dry_run.py`:\n  - Use `CliRunner` (if Click) or equivalent to invoke the CLI with `--dry-run` and a known profile.\n  - Assert:\n    - Exit code is 0.\n    - Output contains the dry-run banner.\n    - Output includes at least one `Profile:` and `Stage:` line.\n    - Output includes `Estimated tokens:` text at least once.\n  - Invoke without `--dry-run` and confirm that the dry-run banner is absent.\n\n4. **Prompt & Profile Integration Tests (Dry-Run)**\n- Add or extend integration tests (e.g. `tests/integration/test_refinement_pipeline_dry_run.py`):\n  - `test_dry_run_uses_real_prompts_from_profiles`:\n    - Use the `default` profile.\n    - Run with `--dry-run`.\n    - Assert that the printed plan references prompt IDs defined in `config/prompts.yaml` and profile stages from `config/profiles.yaml`.\n  - `test_dry_run_token_totals_reported`:\n    - Parse CLI output to find a total token count line.\n    - Assert that the total is >= the maximum per-prompt estimate.\n\n5. **Regression Tests Around Mock Mode**\n- In `tests/test_analysis.py`, add:\n  - `test_create_analysis_engine_dry_run_sets_mock_mode`:\n    - Call `create_analysis_engine(use_mock=False, dry_run=True)`.\n    - Assert that the returned engine is the mock implementation and no network calls are configured.\n\n6. **Non-Functional Checks**\n- Run full test suite (`pytest`) to ensure no existing tests break due to new parameters.\n- Optionally add a smoke test script in CI that runs the CLI with `--dry-run` and `--profile default` to guarantee basic behavior remains functional over time.\n",
        "status": "done",
        "dependencies": [
          "3",
          "5",
          "9",
          "19",
          "22",
          "28"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T07:04:29.645Z"
      },
      {
        "id": "30",
        "title": "Create Comprehensive Test Suite for PlanWriter Module",
        "description": "Create test_plan_writer.py with comprehensive unit tests covering the PlanWriter class and StageResult dataclass, including task aggregation, deduplication, priority grouping, and markdown generation functionality.",
        "details": "Create `tests/test_plan_writer.py` with comprehensive test coverage for the `plan_writer.py` module:\n\n**1. Test File Structure:**\n```python\n\"\"\"Tests for plan_writer module.\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom metaagent.plan_writer import PlanWriter, StageResult\n```\n\n**2. StageResult Dataclass Tests (`TestStageResult`):**\n- `test_stage_result_creation_minimal`: Test creation with only required fields (stage_id, stage_name, summary)\n- `test_stage_result_creation_full`: Test creation with all fields including recommendations and tasks\n- `test_stage_result_defaults`: Verify default empty lists for recommendations and tasks\n- `test_stage_result_with_complex_tasks`: Test with nested task dictionaries containing title, description, priority, file\n\n**3. PlanWriter Initialization Tests (`TestPlanWriterInit`):**\n- `test_init_creates_output_dir`: Verify output_dir is created if it doesn't exist\n- `test_init_with_existing_dir`: Verify existing directory is not modified\n- `test_init_creates_nested_dirs`: Test `parents=True` behavior for nested paths\n\n**4. write_plan() Tests (`TestWritePlan`):**\n- `test_write_plan_creates_file`: Verify file is created at expected path\n- `test_write_plan_returns_path`: Verify correct Path object returned\n- `test_write_plan_custom_filename`: Test custom output_filename parameter\n- `test_write_plan_content_structure`: Verify all sections present (header, PRD summary, stages, tasks, instructions)\n- `test_write_plan_empty_results`: Test with empty stage_results list\n- `test_write_plan_utf8_encoding`: Verify UTF-8 encoding for special characters\n\n**5. Header Generation Tests (`TestGenerateHeader`):**\n- `test_generate_header_contains_profile`: Verify profile name in output\n- `test_generate_header_contains_timestamp`: Verify timestamp format (YYYY-MM-DD HH:MM:SS)\n- `test_generate_header_contains_status`: Verify \"Ready for implementation\" status\n- `test_generate_header_format`: Verify markdown structure with separators\n\n**6. PRD Summary Tests (`TestGeneratePrdSummary` and `TestExtractPrdSummary`):**\n- `test_prd_summary_section_header`: Verify \"## PRD Summary\" header present\n- `test_extract_prd_summary_short_content`: Test content under max_lines returned unchanged\n- `test_extract_prd_summary_long_content`: Test truncation at max_lines (default 20)\n- `test_extract_prd_summary_truncation_notice`: Verify \"[PRD truncated for brevity]\" appended\n- `test_extract_prd_summary_custom_max_lines`: Test custom max_lines parameter\n\n**7. Stage Summaries Tests (`TestGenerateStageSummaries`):**\n- `test_stage_summaries_empty_results`: Verify \"No stages were executed\" message\n- `test_stage_summaries_single_stage`: Test single stage output format\n- `test_stage_summaries_multiple_stages`: Test multiple stages with correct ordering\n- `test_stage_summaries_with_recommendations`: Verify recommendations list formatting\n- `test_stage_summaries_without_recommendations`: Test stage with empty recommendations\n\n**8. Task List and Aggregation Tests (`TestGenerateTaskList` and `TestAggregateTasks`):**\n- `test_task_list_empty_tasks`: Verify \"No tasks were identified\" message\n- `test_task_list_priority_grouping`: Verify tasks grouped by critical/high/medium/low\n- `test_task_list_priority_order`: Verify critical > high > medium > low ordering\n- `test_task_list_checkbox_format`: Verify \"- [ ] **Title**\" format\n- `test_task_list_file_reference`: Verify file path in backticks when present\n- `test_task_list_description`: Verify description on indented line\n- `test_aggregate_tasks_deduplication`: Verify duplicate titles are removed\n- `test_aggregate_tasks_preserves_first_occurrence`: Verify first task with duplicate title kept\n- `test_aggregate_tasks_adds_stage_info`: Verify stage_id added to aggregated tasks\n- `test_aggregate_tasks_empty_title_skipped`: Verify tasks with empty/missing titles ignored\n\n**9. Priority Badge Tests (`TestPriorityBadge`):**\n- `test_priority_badge_critical`: Verify \"[CRITICAL]\" badge\n- `test_priority_badge_high`: Verify \"[HIGH]\" badge\n- `test_priority_badge_medium`: Verify \"[MEDIUM]\" badge\n- `test_priority_badge_low`: Verify \"[LOW]\" badge\n- `test_priority_badge_invalid_defaults_medium`: Verify unknown priority returns \"[MEDIUM]\"\n- `test_priority_badge_case_insensitive`: Test \"HIGH\", \"High\", \"high\" all work\n\n**10. Instructions Tests (`TestGenerateInstructions`):**\n- `test_instructions_contains_claude_code`: Verify Claude Code reference\n- `test_instructions_contains_code_block`: Verify markdown code block with prompt\n- `test_instructions_contains_implementation_notes`: Verify implementation notes section\n\n**11. Edge Cases and Error Handling (`TestEdgeCases`):**\n- `test_missing_task_fields`: Test tasks with missing title/description/priority fields\n- `test_invalid_priority_normalized`: Test invalid priority values default to \"medium\"\n- `test_special_characters_in_content`: Test markdown special characters in titles/descriptions\n- `test_empty_stage_name`: Test StageResult with empty stage_name\n- `test_empty_summary`: Test StageResult with empty summary string\n- `test_none_in_task_fields`: Test tasks with None values for optional fields\n- `test_file_overwrite`: Verify existing file is overwritten on subsequent calls\n\n**12. Fixtures:**\n```python\n@pytest.fixture\ndef plan_writer(tmp_path: Path) -> PlanWriter:\n    \"\"\"Create PlanWriter with temporary output directory.\"\"\"\n    return PlanWriter(output_dir=tmp_path / \"output\")\n\n@pytest.fixture\ndef sample_stage_result() -> StageResult:\n    \"\"\"Create a sample StageResult for testing.\"\"\"\n    return StageResult(\n        stage_id=\"quality_analysis\",\n        stage_name=\"Quality Analysis\",\n        summary=\"Found 3 issues requiring attention.\",\n        recommendations=[\"Fix error handling\", \"Add logging\"],\n        tasks=[\n            {\"title\": \"Fix null check\", \"description\": \"Add null check in process()\", \"priority\": \"high\", \"file\": \"src/main.py\"},\n            {\"title\": \"Add logging\", \"description\": \"Implement logging\", \"priority\": \"medium\"},\n        ],\n    )\n\n@pytest.fixture\ndef sample_prd_content() -> str:\n    \"\"\"Sample PRD content for testing.\"\"\"\n    return \"\"\"# Product Requirements\n## Overview\nThis is a test product.\n## Requirements\n- REQ-001: Feature one\n- REQ-002: Feature two\n\"\"\"\n```\n\n**Implementation Notes:**\n- Use `tmp_path` fixture for all file system operations to ensure test isolation\n- Mock `datetime.now()` in header tests for deterministic timestamps\n- Follow existing test patterns from `test_prompts.py` and `test_analysis.py`\n- Use clear test method naming: `test_<method>_<scenario>`\n- Include type hints for all test methods\n- Target >95% code coverage for plan_writer.py",
        "testStrategy": "**Execution and Verification:**\n\n1. **Run unit tests:**\n   ```bash\n   pytest tests/test_plan_writer.py -v\n   ```\n   All tests must pass.\n\n2. **Coverage analysis:**\n   ```bash\n   pytest tests/test_plan_writer.py --cov=src/metaagent/plan_writer --cov-report=term-missing\n   ```\n   Verify >95% coverage of plan_writer.py.\n\n3. **Test isolation verification:**\n   ```bash\n   pytest tests/test_plan_writer.py -v --tb=short -x\n   ```\n   Each test should be independent and not rely on others.\n\n4. **Verify all PlanWriter methods tested:**\n   - `__init__`\n   - `write_plan`\n   - `_generate_header`\n   - `_generate_prd_summary`\n   - `_extract_prd_summary`\n   - `_generate_stage_summaries`\n   - `_generate_task_list`\n   - `_aggregate_tasks`\n   - `_priority_badge`\n   - `_generate_instructions`\n\n5. **Integration with existing tests:**\n   ```bash\n   pytest tests/ -v --ignore=tests/integration\n   ```\n   Verify new tests don't break existing test suite.\n\n6. **Check test output format:**\n   Review test assertions match actual markdown output format from plan_writer.py lines 70-76 (header), 81-83 (PRD summary), 106-122 (stage summaries), 124-166 (task list).\n\n7. **Validate edge case coverage:**\n   - Empty inputs handled gracefully\n   - Missing/invalid fields don't cause exceptions\n   - File operations work correctly",
        "status": "done",
        "dependencies": [
          "6",
          "10"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T07:14:27.025Z"
      },
      {
        "id": "31",
        "title": "Add Task Normalization and Validation to PlanWriter",
        "description": "Add a `_normalize_task()` method to PlanWriter that validates tasks have required title field, normalizes priority values (defaulting invalid priorities to medium with logging), and adds stage info. Update `_aggregate_tasks()` to use normalization and log warnings for invalid tasks. Add `VALID_PRIORITIES` constant.",
        "details": "## Implementation Details\n\n### 1. Add VALID_PRIORITIES Constant (at module level)\n\nAdd after the imports in `src/metaagent/plan_writer.py`:\n\n```python\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Valid priority levels for task normalization\nVALID_PRIORITIES = frozenset([\"critical\", \"high\", \"medium\", \"low\"])\n```\n\n### 2. Add `_normalize_task()` Method to PlanWriter Class\n\nAdd this method to the `PlanWriter` class (after `_aggregate_tasks()`):\n\n```python\ndef _normalize_task(\n    self,\n    task: dict[str, Any],\n    stage_id: str,\n) -> dict[str, Any] | None:\n    \"\"\"Normalize and validate a single task.\n\n    Args:\n        task: Raw task dictionary from analysis results.\n        stage_id: The stage ID this task originated from.\n\n    Returns:\n        Normalized task dict with required fields, or None if task is invalid.\n    \"\"\"\n    # Validate required title field\n    title = task.get(\"title\")\n    if not title:\n        logger.warning(\n            f\"Skipping task without title from stage '{stage_id}': {task}\"\n        )\n        return None\n\n    # Normalize priority - default to medium if invalid\n    raw_priority = task.get(\"priority\", \"medium\")\n    priority = str(raw_priority).lower().strip()\n    \n    if priority not in VALID_PRIORITIES:\n        logger.warning(\n            f\"Invalid priority '{raw_priority}' for task '{title}', \"\n            f\"defaulting to 'medium'\"\n        )\n        priority = \"medium\"\n\n    # Build normalized task with stage info\n    return {\n        \"title\": title,\n        \"description\": task.get(\"description\", \"\"),\n        \"priority\": priority,\n        \"file\": task.get(\"file\", \"\"),\n        \"stage\": stage_id,\n    }\n```\n\n### 3. Update `_aggregate_tasks()` Method\n\nReplace the existing `_aggregate_tasks()` method:\n\n```python\ndef _aggregate_tasks(self, stage_results: list[StageResult]) -> list[dict]:\n    \"\"\"Aggregate tasks from all stages, removing duplicates.\n\n    Uses _normalize_task() for validation and normalization.\n    Logs warnings for invalid tasks that are skipped.\n\n    Args:\n        stage_results: List of StageResult from analysis stages.\n\n    Returns:\n        List of normalized, deduplicated task dictionaries.\n    \"\"\"\n    all_tasks = []\n    seen_titles: set[str] = set()\n    skipped_count = 0\n\n    for result in stage_results:\n        for task in result.tasks:\n            # Skip non-dict tasks with warning\n            if not isinstance(task, dict):\n                logger.warning(\n                    f\"Skipping non-dict task from stage '{result.stage_id}': {task}\"\n                )\n                skipped_count += 1\n                continue\n\n            # Normalize and validate task\n            normalized = self._normalize_task(task, result.stage_id)\n            if normalized is None:\n                skipped_count += 1\n                continue\n\n            # Deduplicate by title\n            title = normalized[\"title\"]\n            if title in seen_titles:\n                logger.debug(f\"Skipping duplicate task: {title}\")\n                continue\n\n            seen_titles.add(title)\n            all_tasks.append(normalized)\n\n    if skipped_count > 0:\n        logger.warning(\n            f\"Skipped {skipped_count} invalid task(s) during aggregation\"\n        )\n\n    return all_tasks\n```\n\n### 4. File Changes Summary\n\n**File:** `src/metaagent/plan_writer.py`\n\n- Add `import logging` and `logger = logging.getLogger(__name__)` at top\n- Add `VALID_PRIORITIES` constant after imports\n- Add `_normalize_task()` method to PlanWriter class\n- Update `_aggregate_tasks()` to use normalization and add logging\n\n### 5. Alignment with Existing Patterns\n\nThis implementation follows patterns from `analysis.py:283-308` (`_normalize_tasks()`) which:\n- Validates task structure (dict vs string)\n- Provides default values for missing fields\n- Normalizes priority to lowercase\n\nThe task also aligns with `prompts.py:33` which defines valid priorities as `\"critical|high|medium|low\"` in `JSON_RESPONSE_SCHEMA`.",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Tests for VALID_PRIORITIES Constant\n\nIn `tests/test_plan_writer.py`:\n\n```python\ndef test_valid_priorities_constant_is_frozenset():\n    \"\"\"VALID_PRIORITIES should be immutable frozenset.\"\"\"\n    from metaagent.plan_writer import VALID_PRIORITIES\n    assert isinstance(VALID_PRIORITIES, frozenset)\n    assert VALID_PRIORITIES == {\"critical\", \"high\", \"medium\", \"low\"}\n```\n\n### 2. Unit Tests for `_normalize_task()` Method\n\n```python\nclass TestNormalizeTask:\n    \"\"\"Tests for PlanWriter._normalize_task().\"\"\"\n\n    def test_normalize_task_with_valid_task(self, plan_writer):\n        \"\"\"Valid tasks should be normalized with all fields.\"\"\"\n        task = {\"title\": \"Fix bug\", \"priority\": \"high\", \"description\": \"desc\"}\n        result = plan_writer._normalize_task(task, \"stage1\")\n        \n        assert result[\"title\"] == \"Fix bug\"\n        assert result[\"priority\"] == \"high\"\n        assert result[\"stage\"] == \"stage1\"\n\n    def test_normalize_task_missing_title_returns_none(self, plan_writer, caplog):\n        \"\"\"Tasks without title should return None and log warning.\"\"\"\n        task = {\"description\": \"no title\", \"priority\": \"high\"}\n        result = plan_writer._normalize_task(task, \"stage1\")\n        \n        assert result is None\n        assert \"Skipping task without title\" in caplog.text\n\n    def test_normalize_task_empty_title_returns_none(self, plan_writer, caplog):\n        \"\"\"Tasks with empty string title should return None.\"\"\"\n        task = {\"title\": \"\", \"priority\": \"high\"}\n        result = plan_writer._normalize_task(task, \"stage1\")\n        \n        assert result is None\n\n    def test_normalize_task_invalid_priority_defaults_to_medium(self, plan_writer, caplog):\n        \"\"\"Invalid priority should default to medium with warning.\"\"\"\n        task = {\"title\": \"Task\", \"priority\": \"urgent\"}\n        result = plan_writer._normalize_task(task, \"stage1\")\n        \n        assert result[\"priority\"] == \"medium\"\n        assert \"Invalid priority 'urgent'\" in caplog.text\n        assert \"defaulting to 'medium'\" in caplog.text\n\n    def test_normalize_task_uppercase_priority_normalized(self, plan_writer):\n        \"\"\"Priority should be normalized to lowercase.\"\"\"\n        task = {\"title\": \"Task\", \"priority\": \"HIGH\"}\n        result = plan_writer._normalize_task(task, \"stage1\")\n        \n        assert result[\"priority\"] == \"high\"\n\n    def test_normalize_task_missing_priority_defaults_to_medium(self, plan_writer):\n        \"\"\"Missing priority should default to medium without warning.\"\"\"\n        task = {\"title\": \"Task\"}\n        result = plan_writer._normalize_task(task, \"stage1\")\n        \n        assert result[\"priority\"] == \"medium\"\n\n    def test_normalize_task_adds_stage_info(self, plan_writer):\n        \"\"\"Stage ID should be added to normalized task.\"\"\"\n        task = {\"title\": \"Task\", \"priority\": \"low\"}\n        result = plan_writer._normalize_task(task, \"my_stage_id\")\n        \n        assert result[\"stage\"] == \"my_stage_id\"\n\n    @pytest.mark.parametrize(\"priority\", [\"critical\", \"high\", \"medium\", \"low\"])\n    def test_normalize_task_all_valid_priorities(self, plan_writer, priority):\n        \"\"\"All valid priorities should pass through unchanged.\"\"\"\n        task = {\"title\": \"Task\", \"priority\": priority}\n        result = plan_writer._normalize_task(task, \"stage1\")\n        \n        assert result[\"priority\"] == priority\n```\n\n### 3. Unit Tests for Updated `_aggregate_tasks()`\n\n```python\nclass TestAggregateTasks:\n    \"\"\"Tests for PlanWriter._aggregate_tasks().\"\"\"\n\n    def test_aggregate_tasks_skips_non_dict_tasks(self, plan_writer, caplog):\n        \"\"\"Non-dict tasks should be skipped with warning.\"\"\"\n        stage_results = [\n            StageResult(\n                stage_id=\"s1\", stage_name=\"Stage 1\", summary=\"\",\n                tasks=[\"string task\", {\"title\": \"Valid Task\"}]\n            )\n        ]\n        result = plan_writer._aggregate_tasks(stage_results)\n        \n        assert len(result) == 1\n        assert result[0][\"title\"] == \"Valid Task\"\n        assert \"non-dict task\" in caplog.text\n\n    def test_aggregate_tasks_logs_skipped_count(self, plan_writer, caplog):\n        \"\"\"Should log total count of skipped invalid tasks.\"\"\"\n        stage_results = [\n            StageResult(\n                stage_id=\"s1\", stage_name=\"Stage 1\", summary=\"\",\n                tasks=[{}, {\"title\": \"\"}, {\"title\": \"Valid\"}]\n            )\n        ]\n        result = plan_writer._aggregate_tasks(stage_results)\n        \n        assert len(result) == 1\n        assert \"Skipped 2 invalid task(s)\" in caplog.text\n\n    def test_aggregate_tasks_deduplicates_by_title(self, plan_writer):\n        \"\"\"Duplicate titles should be deduplicated.\"\"\"\n        stage_results = [\n            StageResult(\n                stage_id=\"s1\", stage_name=\"Stage 1\", summary=\"\",\n                tasks=[{\"title\": \"Same Task\", \"priority\": \"high\"}]\n            ),\n            StageResult(\n                stage_id=\"s2\", stage_name=\"Stage 2\", summary=\"\",\n                tasks=[{\"title\": \"Same Task\", \"priority\": \"low\"}]\n            ),\n        ]\n        result = plan_writer._aggregate_tasks(stage_results)\n        \n        assert len(result) == 1\n        assert result[0][\"priority\"] == \"high\"  # First occurrence wins\n```\n\n### 4. Integration Test\n\n```python\ndef test_write_plan_with_invalid_tasks(tmp_path):\n    \"\"\"Full write_plan should handle invalid tasks gracefully.\"\"\"\n    writer = PlanWriter(tmp_path)\n    stage_results = [\n        StageResult(\n            stage_id=\"s1\", stage_name=\"Test Stage\", summary=\"Summary\",\n            tasks=[\n                {\"priority\": \"high\"},  # Missing title - should skip\n                {\"title\": \"Valid\", \"priority\": \"INVALID\"},  # Bad priority - should normalize\n            ]\n        )\n    ]\n    \n    output = writer.write_plan(\"PRD\", \"test-profile\", stage_results)\n    content = output.read_text()\n    \n    assert \"Valid\" in content\n    assert \"Medium Priority\" in content  # INVALID normalized to medium\n```\n\n### 5. Verification Commands\n\n```bash\n# Run all plan_writer tests\npytest tests/test_plan_writer.py -v\n\n# Run with coverage\npytest tests/test_plan_writer.py --cov=src/metaagent/plan_writer --cov-report=term-missing\n\n# Run with log capture to verify warnings\npytest tests/test_plan_writer.py -v --log-cli-level=WARNING\n```",
        "status": "done",
        "dependencies": [
          "6",
          "19",
          "22"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T07:16:17.643Z"
      },
      {
        "id": "32",
        "title": "Update README.md with JSON Response Contract Documentation",
        "description": "Add a new documentation section to README.md explaining the expected JSON response format for analysis prompts, including task schema field descriptions and the automatic JSON instruction appending behavior for Codebase Digest prompts.",
        "details": "## Implementation Details\n\nAdd a new section to `README.md` after the \"Adding Custom Profiles\" section (around line 172) titled \"## JSON Response Contract\".\n\n### Section Content Structure\n\n```markdown\n## JSON Response Contract\n\nMeta-agent expects LLM analysis responses to follow a specific JSON schema. This ensures consistent parsing and task generation across all analysis stages.\n\n### Expected Response Format\n\nAll analysis prompts should return JSON in this structure:\n\n```json\n{\n  \"summary\": \"2-4 sentence overview of your analysis findings\",\n  \"recommendations\": [\"High-level recommendation 1\", \"High-level recommendation 2\"],\n  \"tasks\": [\n    {\n      \"title\": \"Short task title\",\n      \"description\": \"Detailed description of what needs to be done\",\n      \"priority\": \"critical|high|medium|low\",\n      \"file\": \"path/to/relevant/file.py\"\n    }\n  ]\n}\n```\n\n### Task Schema Fields\n\n| Field | Type | Required | Description |\n|-------|------|----------|-------------|\n| `title` | string | Yes | Short, actionable task title (e.g., \"Add input validation to CLI\") |\n| `description` | string | Yes | Detailed explanation of what needs to be done and why |\n| `priority` | string | Yes | One of: `critical`, `high`, `medium`, `low` |\n| `file` | string | No | Path to the primary file this task affects |\n\n### Priority Levels\n\n- **critical**: Blocking issues, security vulnerabilities, or broken functionality\n- **high**: Important features missing from PRD, significant bugs\n- **medium**: Improvements, refactoring, non-blocking enhancements\n- **low**: Nice-to-have improvements, minor optimizations\n\n### Automatic JSON Schema Appending\n\nFor Codebase Digest markdown prompts that don't include their own JSON schema instructions, meta-agent automatically appends the `JSON_RESPONSE_SCHEMA` constant (defined in `src/metaagent/prompts.py`). This ensures:\n\n1. All prompts receive consistent response format instructions\n2. Custom prompts can override by including their own schema\n3. The schema is appended after context and instructions (context → instructions → schema ordering)\n\nDetection is based on whether the prompt contains `\"summary\"...\"recommendations\"...\"tasks\"` patterns. Prompts with built-in schemas have `has_json_schema=True` and skip automatic appending.\n```\n\n### File Location\nEdit `README.md` at approximately line 172 (after \"Adding Custom Profiles\" section, before \"Development\" section).\n\n### Related Code References\n- `JSON_RESPONSE_SCHEMA` constant: `src/metaagent/prompts.py:18-41`\n- Schema detection logic: `src/metaagent/prompts.py:301-304`\n- Automatic appending: `src/metaagent/prompts.py:190-194`\n- Task normalization: Will be added in Task 31 (`_normalize_task()` method)",
        "testStrategy": "## Test Strategy\n\n### 1. Documentation Verification\n- README.md renders correctly in GitHub/GitLab markdown preview\n- All code blocks have proper syntax highlighting\n- Table formatting displays correctly\n- Internal cross-references are accurate\n\n### 2. Content Accuracy\n- Verify JSON schema in documentation matches `JSON_RESPONSE_SCHEMA` in `src/metaagent/prompts.py:18-41`\n- Confirm priority values match those in `plan_writer.py` VALID_PRIORITIES constant (from Task 31)\n- Check that field descriptions align with actual usage in `AnalysisResult` dataclass (`analysis.py:18-26`)\n\n### 3. Example Validation\n```bash\n# Verify the documented JSON example is valid JSON\npython -c \"\nimport json\nexample = '''\n{\n  \\\"summary\\\": \\\"Analysis findings\\\",\n  \\\"recommendations\\\": [\\\"rec1\\\"],\n  \\\"tasks\\\": [{\n    \\\"title\\\": \\\"Task\\\",\n    \\\"description\\\": \\\"Description\\\",\n    \\\"priority\\\": \\\"medium\\\",\n    \\\"file\\\": \\\"src/example.py\\\"\n  }]\n}\n'''\njson.loads(example)\nprint('JSON example is valid')\n\"\n```\n\n### 4. Cross-Reference Check\n- Verify \"Adding Custom Prompts\" section mentions the JSON response format requirement\n- Ensure section placement maintains logical flow (after prompts/profiles, before development)\n\n### 5. Link Verification\n- If any internal links are added, verify they point to correct sections\n- Check that file path references (e.g., `src/metaagent/prompts.py`) are accurate",
        "status": "done",
        "dependencies": [
          "11",
          "19"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T07:18:01.069Z"
      },
      {
        "id": "33",
        "title": "Add Retry Logic with Exponential Backoff to Analysis Engine",
        "description": "Add robust retry logic with configurable exponential backoff to the PerplexityAnalysisEngine to handle transient API errors (429 rate limits, 5xx server errors, and timeouts) gracefully.",
        "details": "## Implementation Details\n\n### 1. Add Retry Configuration to Config Class (`src/metaagent/config.py`)\n\nAdd the following fields to the `Config` dataclass after line 43 (after `dry_run`):\n\n```python\n# Retry Settings\nretry_max_attempts: int = 3\nretry_backoff_base: float = 2.0  # Base seconds for exponential backoff\nretry_backoff_max: float = 60.0  # Maximum backoff in seconds\n```\n\nUpdate `Config.from_env()` to load these from environment variables:\n\n```python\nretry_max_attempts=int(os.getenv(\"METAAGENT_RETRY_MAX_ATTEMPTS\", \"3\")),\nretry_backoff_base=float(os.getenv(\"METAAGENT_RETRY_BACKOFF_BASE\", \"2.0\")),\nretry_backoff_max=float(os.getenv(\"METAAGENT_RETRY_BACKOFF_MAX\", \"60.0\")),\n```\n\n### 2. Add Retry Methods to PerplexityAnalysisEngine (`src/metaagent/analysis.py`)\n\nAdd imports at the top of the file:\n\n```python\nimport time\nimport random\n```\n\nModify the `PerplexityAnalysisEngine.__init__` to accept retry configuration:\n\n```python\ndef __init__(\n    self,\n    api_key: str,\n    timeout: int = 120,\n    model: str = \"sonar-pro\",\n    retry_max_attempts: int = 3,\n    retry_backoff_base: float = 2.0,\n    retry_backoff_max: float = 60.0,\n):\n    self.api_key = api_key\n    self.timeout = timeout\n    self.model = model\n    self.retry_max_attempts = retry_max_attempts\n    self.retry_backoff_base = retry_backoff_base\n    self.retry_backoff_max = retry_backoff_max\n    self.client = httpx.Client(timeout=timeout)\n```\n\nAdd helper methods to the class:\n\n```python\n# Transient error status codes eligible for retry\nRETRYABLE_STATUS_CODES = frozenset({429, 500, 502, 503, 504})\n\ndef _should_retry(self, error: Exception, attempt: int) -> bool:\n    \"\"\"Determine if an error is transient and should be retried.\n\n    Args:\n        error: The exception that occurred.\n        attempt: Current attempt number (1-indexed).\n\n    Returns:\n        True if the error is transient and retry is allowed.\n    \"\"\"\n    if attempt >= self.retry_max_attempts:\n        return False\n\n    # Timeout errors are always retryable\n    if isinstance(error, httpx.TimeoutException):\n        return True\n\n    # HTTP errors - check for retryable status codes\n    if isinstance(error, httpx.HTTPStatusError):\n        return error.response.status_code in self.RETRYABLE_STATUS_CODES\n\n    # Connection errors are retryable\n    if isinstance(error, (httpx.ConnectError, httpx.RemoteProtocolError)):\n        return True\n\n    return False\n\ndef _get_backoff_time(self, attempt: int) -> float:\n    \"\"\"Calculate exponential backoff time with jitter.\n\n    Uses exponential backoff with full jitter strategy:\n    backoff = random(0, min(cap, base * 2^attempt))\n\n    Args:\n        attempt: Current attempt number (1-indexed).\n\n    Returns:\n        Backoff time in seconds.\n    \"\"\"\n    exp_backoff = self.retry_backoff_base * (2 ** (attempt - 1))\n    capped_backoff = min(exp_backoff, self.retry_backoff_max)\n    # Add jitter: random value between 0 and capped_backoff\n    return random.uniform(0, capped_backoff)\n```\n\n### 3. Wrap `analyze()` in Retry Loop\n\nReplace the existing `analyze()` method with:\n\n```python\ndef analyze(self, prompt: str) -> AnalysisResult:\n    \"\"\"Run analysis using Perplexity API with retry logic.\n\n    Args:\n        prompt: The rendered prompt to send.\n\n    Returns:\n        AnalysisResult with the analysis output.\n    \"\"\"\n    last_error: Optional[Exception] = None\n\n    for attempt in range(1, self.retry_max_attempts + 1):\n        try:\n            response = self.client.post(\n                self.API_URL,\n                headers={\n                    \"Authorization\": f\"Bearer {self.api_key}\",\n                    \"Content-Type\": \"application/json\",\n                },\n                json={\n                    \"model\": self.model,\n                    \"messages\": [\n                        {\n                            \"role\": \"system\",\n                            \"content\": \"You are a code analysis expert. Analyze codebases and provide structured feedback in JSON format with keys: summary, recommendations, tasks.\",\n                        },\n                        {\n                            \"role\": \"user\",\n                            \"content\": prompt,\n                        },\n                    ],\n                },\n            )\n            response.raise_for_status()\n\n            data = response.json()\n            content = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n\n            return self._parse_response(content)\n\n        except (httpx.HTTPStatusError, httpx.TimeoutException, httpx.ConnectError, httpx.RemoteProtocolError) as e:\n            last_error = e\n\n            if self._should_retry(e, attempt):\n                backoff = self._get_backoff_time(attempt)\n                error_type = type(e).__name__\n                status_info = \"\"\n                if isinstance(e, httpx.HTTPStatusError):\n                    status_info = f\" (status {e.response.status_code})\"\n\n                logger.warning(\n                    f\"Transient error{status_info} on attempt {attempt}/{self.retry_max_attempts}: \"\n                    f\"{error_type}. Retrying in {backoff:.2f}s...\"\n                )\n                time.sleep(backoff)\n                continue\n\n            # Non-retryable or max attempts reached\n            break\n\n        except Exception as e:\n            # Unexpected errors are not retried\n            return AnalysisResult(\n                summary=\"\",\n                success=False,\n                error=f\"Unexpected error calling Perplexity API: {e}\",\n            )\n\n    # Handle the final error after all retries exhausted\n    if isinstance(last_error, httpx.HTTPStatusError):\n        error_body = \"\"\n        try:\n            error_body = last_error.response.text\n        except Exception:\n            pass\n        return AnalysisResult(\n            summary=\"\",\n            success=False,\n            error=f\"HTTP error from Perplexity API after {self.retry_max_attempts} attempts: \"\n                  f\"{last_error.response.status_code} - {error_body}\",\n            raw_response=str(last_error),\n        )\n    elif isinstance(last_error, httpx.TimeoutException):\n        return AnalysisResult(\n            summary=\"\",\n            success=False,\n            error=f\"Request to Perplexity API timed out after {self.retry_max_attempts} attempts \"\n                  f\"(timeout: {self.timeout}s)\",\n        )\n    else:\n        return AnalysisResult(\n            summary=\"\",\n            success=False,\n            error=f\"Error calling Perplexity API after {self.retry_max_attempts} attempts: {last_error}\",\n        )\n```\n\n### 4. Update Factory Function (`create_analysis_engine`)\n\nUpdate the factory function to pass retry configuration:\n\n```python\ndef create_analysis_engine(\n    api_key: Optional[str] = None,\n    mock_mode: bool = False,\n    timeout: int = 120,\n    retry_max_attempts: int = 3,\n    retry_backoff_base: float = 2.0,\n    retry_backoff_max: float = 60.0,\n) -> AnalysisEngine:\n    \"\"\"Factory function to create the appropriate analysis engine.\n\n    Args:\n        api_key: Perplexity API key (required if not mock mode).\n        mock_mode: If True, return a mock engine.\n        timeout: Request timeout in seconds.\n        retry_max_attempts: Maximum number of retry attempts for transient errors.\n        retry_backoff_base: Base seconds for exponential backoff.\n        retry_backoff_max: Maximum backoff time in seconds.\n\n    Returns:\n        AnalysisEngine instance.\n\n    Raises:\n        ValueError: If api_key is missing and not in mock mode.\n    \"\"\"\n    if mock_mode:\n        return MockAnalysisEngine()\n\n    if not api_key:\n        raise ValueError(\"API key is required when not in mock mode\")\n\n    return PerplexityAnalysisEngine(\n        api_key=api_key,\n        timeout=timeout,\n        retry_max_attempts=retry_max_attempts,\n        retry_backoff_base=retry_backoff_base,\n        retry_backoff_max=retry_backoff_max,\n    )\n```\n\n### 5. Update Orchestrator to Pass Config\n\nIn `src/metaagent/orchestrator.py`, update where `create_analysis_engine` is called to pass the retry configuration from Config.",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Tests for Config Retry Settings (`tests/test_config.py`)\n\nAdd tests for new retry configuration:\n\n```python\ndef test_from_env_retry_defaults(self, monkeypatch: pytest.MonkeyPatch, tmp_path: Path) -> None:\n    \"\"\"Test default retry configuration values.\"\"\"\n    monkeypatch.setattr(\"metaagent.config.load_dotenv\", lambda: None)\n    config = Config.from_env(tmp_path)\n\n    assert config.retry_max_attempts == 3\n    assert config.retry_backoff_base == 2.0\n    assert config.retry_backoff_max == 60.0\n\ndef test_from_env_retry_custom(self, monkeypatch: pytest.MonkeyPatch, tmp_path: Path) -> None:\n    \"\"\"Test custom retry configuration from environment.\"\"\"\n    monkeypatch.setenv(\"METAAGENT_RETRY_MAX_ATTEMPTS\", \"5\")\n    monkeypatch.setenv(\"METAAGENT_RETRY_BACKOFF_BASE\", \"1.5\")\n    monkeypatch.setenv(\"METAAGENT_RETRY_BACKOFF_MAX\", \"120.0\")\n    config = Config.from_env(tmp_path)\n\n    assert config.retry_max_attempts == 5\n    assert config.retry_backoff_base == 1.5\n    assert config.retry_backoff_max == 120.0\n```\n\n### 2. Unit Tests for Retry Logic (`tests/test_analysis.py`)\n\nAdd comprehensive tests for retry behavior:\n\n```python\nclass TestPerplexityRetryLogic:\n    \"\"\"Tests for retry logic in PerplexityAnalysisEngine.\"\"\"\n\n    def test_should_retry_timeout(self) -> None:\n        \"\"\"Test that timeouts are retryable.\"\"\"\n        engine = PerplexityAnalysisEngine(api_key=\"test\", retry_max_attempts=3)\n        error = httpx.TimeoutException(\"timeout\")\n        assert engine._should_retry(error, attempt=1) is True\n        assert engine._should_retry(error, attempt=2) is True\n        assert engine._should_retry(error, attempt=3) is False  # max reached\n\n    def test_should_retry_429_rate_limit(self) -> None:\n        \"\"\"Test that 429 rate limit errors are retryable.\"\"\"\n        engine = PerplexityAnalysisEngine(api_key=\"test\", retry_max_attempts=3)\n        response = httpx.Response(429)\n        error = httpx.HTTPStatusError(\"rate limit\", request=None, response=response)\n        assert engine._should_retry(error, attempt=1) is True\n\n    def test_should_retry_5xx_errors(self) -> None:\n        \"\"\"Test that 5xx server errors are retryable.\"\"\"\n        engine = PerplexityAnalysisEngine(api_key=\"test\", retry_max_attempts=3)\n        for status in [500, 502, 503, 504]:\n            response = httpx.Response(status)\n            error = httpx.HTTPStatusError(\"server error\", request=None, response=response)\n            assert engine._should_retry(error, attempt=1) is True\n\n    def test_should_not_retry_400(self) -> None:\n        \"\"\"Test that 400 client errors are not retryable.\"\"\"\n        engine = PerplexityAnalysisEngine(api_key=\"test\", retry_max_attempts=3)\n        response = httpx.Response(400)\n        error = httpx.HTTPStatusError(\"bad request\", request=None, response=response)\n        assert engine._should_retry(error, attempt=1) is False\n\n    def test_should_not_retry_401(self) -> None:\n        \"\"\"Test that 401 unauthorized errors are not retryable.\"\"\"\n        engine = PerplexityAnalysisEngine(api_key=\"test\", retry_max_attempts=3)\n        response = httpx.Response(401)\n        error = httpx.HTTPStatusError(\"unauthorized\", request=None, response=response)\n        assert engine._should_retry(error, attempt=1) is False\n\n    def test_get_backoff_time_exponential(self) -> None:\n        \"\"\"Test that backoff increases exponentially.\"\"\"\n        engine = PerplexityAnalysisEngine(\n            api_key=\"test\",\n            retry_backoff_base=2.0,\n            retry_backoff_max=60.0,\n        )\n        # Backoff should be random between 0 and capped value\n        for attempt in [1, 2, 3]:\n            backoff = engine._get_backoff_time(attempt)\n            max_expected = min(2.0 * (2 ** (attempt - 1)), 60.0)\n            assert 0 <= backoff <= max_expected\n\n    def test_get_backoff_time_capped(self) -> None:\n        \"\"\"Test that backoff is capped at max value.\"\"\"\n        engine = PerplexityAnalysisEngine(\n            api_key=\"test\",\n            retry_backoff_base=2.0,\n            retry_backoff_max=10.0,\n        )\n        # High attempt should be capped\n        backoff = engine._get_backoff_time(10)\n        assert 0 <= backoff <= 10.0\n```\n\n### 3. Integration Tests with Mocked HTTP Responses\n\n```python\n@pytest.fixture\ndef mock_httpx_client(monkeypatch):\n    \"\"\"Create a mock httpx client for testing retries.\"\"\"\n    responses = []\n\n    class MockClient:\n        def __init__(self, timeout):\n            self.call_count = 0\n\n        def post(self, *args, **kwargs):\n            self.call_count += 1\n            if responses:\n                result = responses.pop(0)\n                if isinstance(result, Exception):\n                    raise result\n                return result\n            raise ValueError(\"No mock responses configured\")\n\n    return MockClient, responses\n\ndef test_retry_on_429_succeeds_after_retry(mock_httpx_client, monkeypatch) -> None:\n    \"\"\"Test that 429 error is retried and succeeds.\"\"\"\n    MockClient, responses = mock_httpx_client\n\n    # First call: 429, Second call: success\n    responses.extend([\n        httpx.HTTPStatusError(\"rate limit\", request=None, response=httpx.Response(429)),\n        create_success_response(),\n    ])\n\n    monkeypatch.setattr(httpx, \"Client\", MockClient)\n    engine = PerplexityAnalysisEngine(api_key=\"test\", retry_max_attempts=3)\n\n    result = engine.analyze(\"test prompt\")\n    assert result.success is True\n\ndef test_max_retries_exhausted(mock_httpx_client, monkeypatch) -> None:\n    \"\"\"Test error returned after max retries.\"\"\"\n    MockClient, responses = mock_httpx_client\n\n    # All calls: 503\n    for _ in range(3):\n        responses.append(\n            httpx.HTTPStatusError(\"unavailable\", request=None, response=httpx.Response(503))\n        )\n\n    monkeypatch.setattr(httpx, \"Client\", MockClient)\n    engine = PerplexityAnalysisEngine(api_key=\"test\", retry_max_attempts=3)\n\n    result = engine.analyze(\"test prompt\")\n    assert result.success is False\n    assert \"after 3 attempts\" in result.error\n```\n\n### 4. Run Tests\n\n```bash\n# Run all analysis tests\npytest tests/test_analysis.py -v\n\n# Run with coverage\npytest tests/test_analysis.py tests/test_config.py --cov=src/metaagent/analysis --cov=src/metaagent/config --cov-report=term-missing\n\n# Verify no regressions\npytest -v\n```\n\n### 5. Manual Testing\n\n- Test with invalid API key (should not retry 401)\n- Test with rate limiting by making rapid requests (should retry 429)\n- Verify logs show retry attempts and backoff times\n- Confirm exponential backoff timing with debug logging",
        "status": "done",
        "dependencies": [
          "2",
          "5"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T07:35:18.811Z"
      },
      {
        "id": "34",
        "title": "Implement Strict JSON Parsing with Schema Validation for LLM Responses",
        "description": "Create robust JSON extraction and schema validation utilities for LLM responses with multiple parsing strategies and explicit failure states to improve reliability of analysis results.",
        "details": "## Overview\n\nThe current `_parse_response()` method in `src/metaagent/analysis.py:236` uses basic JSON extraction with a fallback to text parsing. This task implements strict JSON parsing with schema validation and explicit failure modes.\n\n## Implementation Steps\n\n### 1. Create JSON Parsing Module (`src/metaagent/json_parser.py`)\n\n```python\n\"\"\"Strict JSON parsing utilities for LLM responses.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport re\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Any, Optional\n\nlogger = logging.getLogger(__name__)\n\n\nclass ParseStrategy(Enum):\n    \"\"\"JSON extraction strategy used.\"\"\"\n    DIRECT = \"direct\"           # Raw JSON parse\n    MARKDOWN_BLOCK = \"markdown_block\"  # ```json ... ```\n    BALANCED_BRACES = \"balanced_braces\"  # First { to matching }\n    FAILED = \"failed\"\n\n\n@dataclass\nclass ParseResult:\n    \"\"\"Result of JSON parsing attempt.\"\"\"\n    success: bool\n    data: Optional[dict[str, Any]] = None\n    strategy_used: ParseStrategy = ParseStrategy.FAILED\n    error: Optional[str] = None\n    raw_content: str = \"\"\n\n\ndef extract_json_from_response(content: str) -> ParseResult:\n    \"\"\"Extract JSON from LLM response using multiple strategies.\n    \n    Attempts strategies in order of preference:\n    1. Direct JSON parse (cleanest responses)\n    2. Markdown code block extraction (```json ... ```)\n    3. Balanced brace matching (handles surrounding text)\n    \n    Args:\n        content: Raw response from LLM.\n        \n    Returns:\n        ParseResult with extracted data or error details.\n    \"\"\"\n    if not content or not content.strip():\n        return ParseResult(\n            success=False,\n            error=\"Empty response content\",\n            raw_content=content or \"\",\n        )\n    \n    content = content.strip()\n    \n    # Strategy 1: Direct JSON parse\n    result = _try_direct_parse(content)\n    if result.success:\n        return result\n    \n    # Strategy 2: Markdown code block\n    result = _try_markdown_block(content)\n    if result.success:\n        return result\n    \n    # Strategy 3: Balanced brace matching\n    result = _try_balanced_braces(content)\n    if result.success:\n        return result\n    \n    return ParseResult(\n        success=False,\n        error=f\"All JSON extraction strategies failed. Content preview: {content[:200]}\",\n        raw_content=content,\n    )\n\n\ndef _try_direct_parse(content: str) -> ParseResult:\n    \"\"\"Attempt direct JSON parse.\"\"\"\n    try:\n        data = json.loads(content)\n        if isinstance(data, dict):\n            return ParseResult(\n                success=True,\n                data=data,\n                strategy_used=ParseStrategy.DIRECT,\n                raw_content=content,\n            )\n    except json.JSONDecodeError:\n        pass\n    return ParseResult(success=False, raw_content=content)\n\n\ndef _try_markdown_block(content: str) -> ParseResult:\n    \"\"\"Extract JSON from markdown code block.\"\"\"\n    # Match ```json ... ``` or ``` ... ```\n    pattern = r\"```(?:json)?\\s*\\n?([\\s\\S]*?)\\n?```\"\n    match = re.search(pattern, content)\n    if match:\n        json_str = match.group(1).strip()\n        json_str = _clean_json_string(json_str)\n        try:\n            data = json.loads(json_str)\n            if isinstance(data, dict):\n                return ParseResult(\n                    success=True,\n                    data=data,\n                    strategy_used=ParseStrategy.MARKDOWN_BLOCK,\n                    raw_content=content,\n                )\n        except json.JSONDecodeError:\n            pass\n    return ParseResult(success=False, raw_content=content)\n\n\ndef _try_balanced_braces(content: str) -> ParseResult:\n    \"\"\"Extract JSON using balanced brace matching.\"\"\"\n    start = content.find(\"{\")\n    if start == -1:\n        return ParseResult(success=False, raw_content=content)\n    \n    # Find matching closing brace\n    depth = 0\n    in_string = False\n    escape_next = False\n    end = -1\n    \n    for i, char in enumerate(content[start:], start):\n        if escape_next:\n            escape_next = False\n            continue\n        if char == '\\\\':\n            escape_next = True\n            continue\n        if char == '\"' and not escape_next:\n            in_string = not in_string\n            continue\n        if in_string:\n            continue\n        if char == '{':\n            depth += 1\n        elif char == '}':\n            depth -= 1\n            if depth == 0:\n                end = i + 1\n                break\n    \n    if end == -1:\n        return ParseResult(success=False, raw_content=content)\n    \n    json_str = content[start:end]\n    json_str = _clean_json_string(json_str)\n    \n    try:\n        data = json.loads(json_str)\n        if isinstance(data, dict):\n            return ParseResult(\n                success=True,\n                data=data,\n                strategy_used=ParseStrategy.BALANCED_BRACES,\n                raw_content=content,\n            )\n    except json.JSONDecodeError as e:\n        return ParseResult(\n            success=False,\n            error=f\"Balanced brace extraction found JSON but parse failed: {e}\",\n            raw_content=content,\n        )\n    \n    return ParseResult(success=False, raw_content=content)\n\n\ndef _clean_json_string(json_str: str) -> str:\n    \"\"\"Clean common JSON formatting issues.\"\"\"\n    # Remove trailing commas before } or ]\n    json_str = re.sub(r',\\s*}', '}', json_str)\n    json_str = re.sub(r',\\s*]', ']', json_str)\n    return json_str\n```\n\n### 2. Add Schema Validation (`src/metaagent/json_parser.py` continued)\n\n```python\n@dataclass\nclass ValidationResult:\n    \"\"\"Result of schema validation.\"\"\"\n    valid: bool\n    missing_fields: list[str] = field(default_factory=list)\n    type_errors: dict[str, str] = field(default_factory=dict)\n    warnings: list[str] = field(default_factory=list)\n\n\n# Expected schema for analysis responses\nANALYSIS_SCHEMA = {\n    \"summary\": {\"type\": str, \"required\": True},\n    \"recommendations\": {\"type\": list, \"required\": False},\n    \"tasks\": {\"type\": list, \"required\": False},\n}\n\n# Expected schema for triage responses\nTRIAGE_SCHEMA = {\n    \"assessment\": {\"type\": str, \"required\": True},\n    \"selected_prompts\": {\"type\": list, \"required\": True},\n    \"done\": {\"type\": bool, \"required\": True},\n    \"priority_issues\": {\"type\": list, \"required\": False},\n    \"reasoning\": {\"type\": str, \"required\": False},\n}\n\n\ndef validate_analysis_response(data: dict[str, Any]) -> ValidationResult:\n    \"\"\"Validate analysis response against expected schema.\n    \n    Args:\n        data: Parsed JSON data.\n        \n    Returns:\n        ValidationResult with validation status and issues.\n    \"\"\"\n    return _validate_against_schema(data, ANALYSIS_SCHEMA)\n\n\ndef validate_triage_response(data: dict[str, Any]) -> ValidationResult:\n    \"\"\"Validate triage response against expected schema.\n    \n    Args:\n        data: Parsed JSON data.\n        \n    Returns:\n        ValidationResult with validation status and issues.\n    \"\"\"\n    return _validate_against_schema(data, TRIAGE_SCHEMA)\n\n\ndef _validate_against_schema(\n    data: dict[str, Any], \n    schema: dict[str, dict]\n) -> ValidationResult:\n    \"\"\"Validate data against a schema definition.\"\"\"\n    missing = []\n    type_errors = {}\n    warnings = []\n    \n    for field_name, field_def in schema.items():\n        if field_name not in data:\n            if field_def[\"required\"]:\n                missing.append(field_name)\n            continue\n        \n        value = data[field_name]\n        expected_type = field_def[\"type\"]\n        \n        if not isinstance(value, expected_type):\n            type_errors[field_name] = f\"Expected {expected_type.__name__}, got {type(value).__name__}\"\n    \n    # Validate tasks structure if present\n    if \"tasks\" in data and isinstance(data[\"tasks\"], list):\n        for i, task in enumerate(data[\"tasks\"]):\n            if not isinstance(task, dict):\n                warnings.append(f\"Task {i} is not a dict: {type(task).__name__}\")\n            elif \"title\" not in task and \"description\" not in task:\n                warnings.append(f\"Task {i} missing both title and description\")\n    \n    return ValidationResult(\n        valid=len(missing) == 0 and len(type_errors) == 0,\n        missing_fields=missing,\n        type_errors=type_errors,\n        warnings=warnings,\n    )\n```\n\n### 3. Update PerplexityAnalysisEngine._parse_response()\n\nModify `src/metaagent/analysis.py` to use strict parsing:\n\n```python\nfrom .json_parser import (\n    extract_json_from_response,\n    validate_analysis_response,\n    ParseStrategy,\n)\n\ndef _parse_response(self, content: str, strict: bool = True) -> AnalysisResult:\n    \"\"\"Parse the LLM response into structured result.\n    \n    Args:\n        content: Raw response content from the LLM.\n        strict: If True, return failure on invalid JSON. If False, use fallback.\n        \n    Returns:\n        Parsed AnalysisResult.\n    \"\"\"\n    parse_result = extract_json_from_response(content)\n    \n    if parse_result.success and parse_result.data:\n        logger.debug(f\"JSON extracted using strategy: {parse_result.strategy_used.value}\")\n        \n        # Validate against schema\n        validation = validate_analysis_response(parse_result.data)\n        if not validation.valid:\n            logger.warning(f\"Schema validation issues: missing={validation.missing_fields}, types={validation.type_errors}\")\n        \n        if validation.warnings:\n            for warning in validation.warnings:\n                logger.warning(f\"Validation warning: {warning}\")\n        \n        return AnalysisResult(\n            summary=parse_result.data.get(\"summary\", \"\"),\n            recommendations=parse_result.data.get(\"recommendations\", []),\n            tasks=self._normalize_tasks(parse_result.data.get(\"tasks\", [])),\n            raw_response=content,\n            success=True,\n        )\n    \n    # Strict mode: fail explicitly\n    if strict:\n        logger.error(f\"JSON extraction failed: {parse_result.error}\")\n        return AnalysisResult(\n            summary=\"\",\n            success=False,\n            error=f\"Failed to parse LLM response as JSON: {parse_result.error}\",\n            raw_response=content,\n        )\n    \n    # Non-strict: use fallback\n    logger.debug(\"Using fallback text parsing (strict=False)\")\n    return self._create_fallback_result(content)\n```\n\n### 4. Update TriageEngine._parse_triage_response()\n\nModify `src/metaagent/orchestrator.py` to use strict parsing:\n\n```python\nfrom .json_parser import extract_json_from_response, validate_triage_response\n\ndef _parse_triage_response(self, analysis_result: AnalysisResult) -> TriageResult:\n    \"\"\"Parse the triage response into a TriageResult.\"\"\"\n    response_text = analysis_result.summary or analysis_result.raw_response\n    \n    if not response_text or not response_text.strip():\n        return TriageResult(success=False, error=\"Empty triage response\")\n    \n    parse_result = extract_json_from_response(response_text)\n    \n    if not parse_result.success:\n        # Check for natural language \"done\" indicator\n        if \"done\" in response_text.lower() and \"no further\" in response_text.lower():\n            return TriageResult(success=True, done=True, assessment=response_text)\n        return TriageResult(success=False, error=f\"JSON extraction failed: {parse_result.error}\")\n    \n    validation = validate_triage_response(parse_result.data)\n    if not validation.valid:\n        logger.warning(f\"Triage validation: missing={validation.missing_fields}\")\n    \n    data = parse_result.data\n    return TriageResult(\n        success=True,\n        done=data.get(\"done\", False),\n        assessment=data.get(\"assessment\", \"\"),\n        priority_issues=data.get(\"priority_issues\", []),\n        selected_prompts=self._validate_prompts(data.get(\"selected_prompts\", [])),\n        reasoning=data.get(\"reasoning\", \"\"),\n    )\n```\n\n### 5. Add Strict Mode Parameter to Analysis Engine\n\nUpdate `PerplexityAnalysisEngine.__init__()`:\n\n```python\ndef __init__(\n    self, \n    api_key: str, \n    timeout: int = 120, \n    model: str = \"sonar-pro\",\n    strict_parsing: bool = True,\n):\n    \"\"\"Initialize Perplexity analysis engine.\n    \n    Args:\n        api_key: Perplexity API key.\n        timeout: Request timeout in seconds.\n        model: Model to use for analysis.\n        strict_parsing: If True, fail on invalid JSON. If False, use fallback.\n    \"\"\"\n    self.api_key = api_key\n    self.timeout = timeout\n    self.model = model\n    self.strict_parsing = strict_parsing\n    self.client = httpx.Client(timeout=timeout)\n```\n\nUpdate factory function to accept strict_parsing parameter:\n\n```python\ndef create_analysis_engine(\n    api_key: Optional[str] = None,\n    mock_mode: bool = False,\n    timeout: int = 120,\n    strict_parsing: bool = True,\n) -> AnalysisEngine:\n    \"\"\"Factory function to create the appropriate analysis engine.\"\"\"\n    if mock_mode:\n        return MockAnalysisEngine()\n    \n    if not api_key:\n        raise ValueError(\"API key is required when not in mock mode\")\n    \n    return PerplexityAnalysisEngine(\n        api_key=api_key, \n        timeout=timeout,\n        strict_parsing=strict_parsing,\n    )\n```\n\n## Key Design Decisions\n\n1. **Multiple extraction strategies**: Ordered by reliability (direct > markdown > balanced braces)\n2. **Balanced brace matching**: Handles escape sequences and nested structures properly  \n3. **Explicit failure states**: ParseResult and ValidationResult provide detailed error info\n4. **Schema validation**: Lightweight validation without external dependencies\n5. **Strict mode toggle**: Allows gradual migration from fallback behavior\n6. **Separation of concerns**: json_parser.py module is reusable across the codebase",
        "testStrategy": "## Unit Tests (`tests/test_json_parser.py`)\n\n### 1. Test extract_json_from_response() - Direct Parse\n```python\ndef test_extract_direct_json():\n    content = '{\"summary\": \"test\", \"recommendations\": []}'\n    result = extract_json_from_response(content)\n    assert result.success is True\n    assert result.strategy_used == ParseStrategy.DIRECT\n    assert result.data[\"summary\"] == \"test\"\n\ndef test_extract_direct_json_with_whitespace():\n    content = '  \\n{\"summary\": \"test\"}\\n  '\n    result = extract_json_from_response(content)\n    assert result.success is True\n    assert result.strategy_used == ParseStrategy.DIRECT\n```\n\n### 2. Test extract_json_from_response() - Markdown Block\n```python\ndef test_extract_markdown_json_block():\n    content = '''Here's my analysis:\n```json\n{\"summary\": \"markdown test\", \"tasks\": []}\n```\nSome trailing text.'''\n    result = extract_json_from_response(content)\n    assert result.success is True\n    assert result.strategy_used == ParseStrategy.MARKDOWN_BLOCK\n    assert result.data[\"summary\"] == \"markdown test\"\n\ndef test_extract_markdown_block_no_json_tag():\n    content = '''```\n{\"summary\": \"no tag\"}\n```'''\n    result = extract_json_from_response(content)\n    assert result.success is True\n    assert result.strategy_used == ParseStrategy.MARKDOWN_BLOCK\n```\n\n### 3. Test extract_json_from_response() - Balanced Braces\n```python\ndef test_extract_balanced_braces():\n    content = 'Analysis: {\"summary\": \"braces\", \"nested\": {\"a\": 1}} done.'\n    result = extract_json_from_response(content)\n    assert result.success is True\n    assert result.strategy_used == ParseStrategy.BALANCED_BRACES\n    assert result.data[\"nested\"][\"a\"] == 1\n\ndef test_extract_handles_escaped_quotes():\n    content = '{\"summary\": \"test \\\\\"quoted\\\\\" word\"}'\n    result = extract_json_from_response(content)\n    assert result.success is True\n    assert \"quoted\" in result.data[\"summary\"]\n\ndef test_extract_unbalanced_braces_fails():\n    content = '{\"summary\": \"missing close brace\"'\n    result = extract_json_from_response(content)\n    assert result.success is False\n```\n\n### 4. Test Failure States\n```python\ndef test_extract_empty_content():\n    result = extract_json_from_response(\"\")\n    assert result.success is False\n    assert \"Empty\" in result.error\n\ndef test_extract_no_json_content():\n    result = extract_json_from_response(\"Just plain text, no JSON here.\")\n    assert result.success is False\n    assert result.strategy_used == ParseStrategy.FAILED\n\ndef test_extract_invalid_json():\n    content = '{\"summary\": invalid}'\n    result = extract_json_from_response(content)\n    assert result.success is False\n```\n\n### 5. Test validate_analysis_response()\n```python\ndef test_validate_valid_analysis():\n    data = {\"summary\": \"test\", \"recommendations\": [], \"tasks\": []}\n    result = validate_analysis_response(data)\n    assert result.valid is True\n    assert len(result.missing_fields) == 0\n\ndef test_validate_missing_required_field():\n    data = {\"recommendations\": []}  # missing summary\n    result = validate_analysis_response(data)\n    assert result.valid is False\n    assert \"summary\" in result.missing_fields\n\ndef test_validate_wrong_type():\n    data = {\"summary\": 123, \"recommendations\": []}  # summary should be str\n    result = validate_analysis_response(data)\n    assert result.valid is False\n    assert \"summary\" in result.type_errors\n\ndef test_validate_optional_fields_missing_ok():\n    data = {\"summary\": \"test\"}  # optional fields missing\n    result = validate_analysis_response(data)\n    assert result.valid is True\n```\n\n### 6. Test validate_triage_response()\n```python\ndef test_validate_valid_triage():\n    data = {\"assessment\": \"ok\", \"selected_prompts\": [], \"done\": True}\n    result = validate_triage_response(data)\n    assert result.valid is True\n\ndef test_validate_triage_missing_done():\n    data = {\"assessment\": \"ok\", \"selected_prompts\": []}\n    result = validate_triage_response(data)\n    assert result.valid is False\n    assert \"done\" in result.missing_fields\n```\n\n### 7. Test Task Validation Warnings\n```python\ndef test_validate_warns_invalid_task_structure():\n    data = {\"summary\": \"test\", \"tasks\": [\"string task\", {\"title\": \"valid\"}]}\n    result = validate_analysis_response(data)\n    assert result.valid is True  # Not a hard failure\n    assert len(result.warnings) > 0\n    assert \"Task 0\" in result.warnings[0]\n```\n\n## Integration Tests (`tests/test_analysis.py` additions)\n\n### 8. Test _parse_response() Strict Mode\n```python\ndef test_parse_response_strict_success():\n    engine = PerplexityAnalysisEngine(api_key=\"test\", strict_parsing=True)\n    content = '{\"summary\": \"strict test\", \"recommendations\": [], \"tasks\": []}'\n    result = engine._parse_response(content, strict=True)\n    assert result.success is True\n    assert result.summary == \"strict test\"\n\ndef test_parse_response_strict_failure():\n    engine = PerplexityAnalysisEngine(api_key=\"test\", strict_parsing=True)\n    content = \"No JSON here at all.\"\n    result = engine._parse_response(content, strict=True)\n    assert result.success is False\n    assert result.error is not None\n    assert \"Failed to parse\" in result.error\n\ndef test_parse_response_non_strict_fallback():\n    engine = PerplexityAnalysisEngine(api_key=\"test\", strict_parsing=False)\n    content = \"No JSON, but here are recommendations:\\n- Do this\\n- Do that\"\n    result = engine._parse_response(content, strict=False)\n    assert result.success is True  # Fallback worked\n```\n\n## Test Orchestrator Integration (`tests/test_orchestrator.py` additions)\n\n### 9. Test TriageEngine with New Parser\n```python\ndef test_triage_parses_valid_json():\n    # Setup mock analysis result with valid JSON\n    triage_json = json.dumps({\n        \"assessment\": \"Test assessment\",\n        \"selected_prompts\": [\"quality_error_analysis\"],\n        \"done\": False,\n        \"reasoning\": \"Test reasoning\"\n    })\n    analysis_result = AnalysisResult(summary=triage_json, raw_response=triage_json, success=True)\n    \n    triage_engine = TriageEngine(...)\n    result = triage_engine._parse_triage_response(analysis_result)\n    \n    assert result.success is True\n    assert result.assessment == \"Test assessment\"\n    assert \"quality_error_analysis\" in result.selected_prompts\n\ndef test_triage_handles_invalid_json():\n    analysis_result = AnalysisResult(summary=\"Not JSON\", raw_response=\"Not JSON\", success=True)\n    triage_engine = TriageEngine(...)\n    result = triage_engine._parse_triage_response(analysis_result)\n    assert result.success is False\n```\n\n## Performance Tests\n\n### 10. Test Large Response Handling\n```python\ndef test_extract_large_json():\n    # 1MB of JSON\n    large_tasks = [{\"title\": f\"Task {i}\", \"description\": \"x\" * 1000} for i in range(100)]\n    content = json.dumps({\"summary\": \"large\", \"tasks\": large_tasks})\n    result = extract_json_from_response(content)\n    assert result.success is True\n    assert len(result.data[\"tasks\"]) == 100\n```",
        "status": "done",
        "dependencies": [
          "5",
          "7",
          "14"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-14T09:38:16.704Z"
      },
      {
        "id": "35",
        "title": "Implement Strict JSON Parsing with Schema Validation",
        "description": "Add robust JSON extraction and validation functions to the analysis module, including extract_json_from_response() with multiple parsing strategies and validate_analysis_response() for required field verification, then update _parse_response() to support strict mode.",
        "details": "## Overview\n\nThe current `_parse_response()` method in `src/metaagent/analysis.py:236` uses basic JSON extraction that can be fragile. This task creates a more robust, multi-strategy JSON extraction system with schema validation.\n\n## Implementation Steps\n\n### 1. Add New Extraction Function: `extract_json_from_response()`\n\nAdd this function to `src/metaagent/analysis.py` before the `PerplexityAnalysisEngine` class:\n\n```python\nimport re\nfrom typing import Optional, Tuple\n\nclass JsonExtractionError(Exception):\n    \"\"\"Raised when JSON extraction fails.\"\"\"\n    pass\n\ndef extract_json_from_response(content: str, strict: bool = False) -> Tuple[dict, str]:\n    \"\"\"Extract JSON from LLM response using multiple strategies.\n    \n    Args:\n        content: Raw response content from LLM.\n        strict: If True, raise exception on failure instead of returning empty dict.\n    \n    Returns:\n        Tuple of (extracted_dict, strategy_used)\n    \n    Raises:\n        JsonExtractionError: If strict=True and no valid JSON found.\n    \"\"\"\n    strategies = [\n        (\"direct_parse\", _try_direct_parse),\n        (\"markdown_code_block\", _try_markdown_code_block),\n        (\"balanced_braces\", _try_balanced_braces),\n    ]\n    \n    for strategy_name, strategy_fn in strategies:\n        result = strategy_fn(content)\n        if result is not None:\n            logger.debug(f\"JSON extracted using strategy: {strategy_name}\")\n            return result, strategy_name\n    \n    if strict:\n        raise JsonExtractionError(\n            f\"Failed to extract valid JSON from response. \"\n            f\"Content preview: {content[:200]}...\"\n        )\n    \n    return {}, \"none\"\n\ndef _try_direct_parse(content: str) -> Optional[dict]:\n    \"\"\"Strategy 1: Try to parse the entire content as JSON.\"\"\"\n    content = content.strip()\n    try:\n        data = json.loads(content)\n        if isinstance(data, dict):\n            return data\n    except json.JSONDecodeError:\n        pass\n    return None\n\ndef _try_markdown_code_block(content: str) -> Optional[dict]:\n    \"\"\"Strategy 2: Extract JSON from markdown code blocks.\"\"\"\n    # Match ```json ... ``` or ``` ... ```\n    patterns = [\n        r\"```json\\s*([\\s\\S]*?)```\",\n        r\"```\\s*([\\s\\S]*?)```\",\n    ]\n    \n    for pattern in patterns:\n        match = re.search(pattern, content, re.IGNORECASE)\n        if match:\n            json_str = match.group(1).strip()\n            json_str = _clean_json_string(json_str)\n            try:\n                data = json.loads(json_str)\n                if isinstance(data, dict):\n                    return data\n            except json.JSONDecodeError:\n                continue\n    return None\n\ndef _try_balanced_braces(content: str) -> Optional[dict]:\n    \"\"\"Strategy 3: Find JSON using balanced brace matching.\"\"\"\n    # Find the first { and extract until balanced }\n    start_idx = content.find('{')\n    if start_idx == -1:\n        return None\n    \n    depth = 0\n    in_string = False\n    escape_next = False\n    end_idx = start_idx\n    \n    for i, char in enumerate(content[start_idx:], start=start_idx):\n        if escape_next:\n            escape_next = False\n            continue\n        if char == '\\\\':\n            escape_next = True\n            continue\n        if char == '\"' and not escape_next:\n            in_string = not in_string\n            continue\n        if in_string:\n            continue\n        if char == '{':\n            depth += 1\n        elif char == '}':\n            depth -= 1\n            if depth == 0:\n                end_idx = i + 1\n                break\n    \n    if depth != 0:\n        return None\n    \n    json_str = content[start_idx:end_idx]\n    json_str = _clean_json_string(json_str)\n    \n    try:\n        data = json.loads(json_str)\n        if isinstance(data, dict):\n            return data\n    except json.JSONDecodeError:\n        pass\n    return None\n\ndef _clean_json_string(json_str: str) -> str:\n    \"\"\"Clean common JSON issues from LLM output.\"\"\"\n    # Remove trailing commas before } or ]\n    json_str = re.sub(r',\\s*}', '}', json_str)\n    json_str = re.sub(r',\\s*]', ']', json_str)\n    # Remove comments (// style)\n    json_str = re.sub(r'//[^\\n]*\\n', '\\n', json_str)\n    return json_str\n```\n\n### 2. Add Validation Function: `validate_analysis_response()`\n\n```python\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass ValidationError:\n    \"\"\"Represents a validation error.\"\"\"\n    field: str\n    message: str\n    severity: str = \"error\"  # \"error\" or \"warning\"\n\ndef validate_analysis_response(\n    data: dict,\n    required_fields: Optional[List[str]] = None,\n    strict: bool = False\n) -> Tuple[bool, List[ValidationError]]:\n    \"\"\"Validate an analysis response against expected schema.\n    \n    Args:\n        data: Parsed JSON data to validate.\n        required_fields: List of required top-level fields. \n                        Defaults to [\"summary\", \"recommendations\", \"tasks\"].\n        strict: If True, treat warnings as errors.\n    \n    Returns:\n        Tuple of (is_valid, list_of_errors)\n    \"\"\"\n    if required_fields is None:\n        required_fields = [\"summary\", \"recommendations\", \"tasks\"]\n    \n    errors: List[ValidationError] = []\n    \n    # Check required fields exist\n    for field in required_fields:\n        if field not in data:\n            errors.append(ValidationError(\n                field=field,\n                message=f\"Required field '{field}' is missing\",\n                severity=\"error\"\n            ))\n        elif data[field] is None:\n            errors.append(ValidationError(\n                field=field,\n                message=f\"Field '{field}' is null\",\n                severity=\"warning\" if not strict else \"error\"\n            ))\n    \n    # Validate field types\n    if \"summary\" in data and not isinstance(data.get(\"summary\"), str):\n        errors.append(ValidationError(\n            field=\"summary\",\n            message=\"Field 'summary' must be a string\",\n            severity=\"error\"\n        ))\n    \n    if \"recommendations\" in data:\n        recs = data.get(\"recommendations\")\n        if not isinstance(recs, list):\n            errors.append(ValidationError(\n                field=\"recommendations\",\n                message=\"Field 'recommendations' must be a list\",\n                severity=\"error\"\n            ))\n        elif not all(isinstance(r, str) for r in recs):\n            errors.append(ValidationError(\n                field=\"recommendations\",\n                message=\"All recommendations must be strings\",\n                severity=\"warning\"\n            ))\n    \n    if \"tasks\" in data:\n        tasks = data.get(\"tasks\")\n        if not isinstance(tasks, list):\n            errors.append(ValidationError(\n                field=\"tasks\",\n                message=\"Field 'tasks' must be a list\",\n                severity=\"error\"\n            ))\n        else:\n            for i, task in enumerate(tasks):\n                if not isinstance(task, dict):\n                    errors.append(ValidationError(\n                        field=f\"tasks[{i}]\",\n                        message=f\"Task at index {i} must be an object\",\n                        severity=\"error\"\n                    ))\n                elif \"title\" not in task:\n                    errors.append(ValidationError(\n                        field=f\"tasks[{i}].title\",\n                        message=f\"Task at index {i} is missing 'title' field\",\n                        severity=\"warning\"\n                    ))\n    \n    # Determine overall validity\n    has_errors = any(e.severity == \"error\" for e in errors)\n    is_valid = not has_errors\n    \n    return is_valid, errors\n```\n\n### 3. Update `_parse_response()` Method\n\nModify `PerplexityAnalysisEngine._parse_response()` to use the new functions:\n\n```python\ndef _parse_response(self, content: str, strict: bool = False) -> AnalysisResult:\n    \"\"\"Parse the LLM response into structured result.\n    \n    Uses multiple extraction strategies for robustness.\n    \n    Args:\n        content: Raw response content from the LLM.\n        strict: If True, require valid JSON and schema compliance.\n    \n    Returns:\n        Parsed AnalysisResult.\n    \n    Raises:\n        JsonExtractionError: If strict=True and JSON extraction fails.\n    \"\"\"\n    try:\n        data, strategy = extract_json_from_response(content, strict=strict)\n        \n        if data:\n            # Validate the response\n            is_valid, validation_errors = validate_analysis_response(data, strict=strict)\n            \n            if strict and not is_valid:\n                error_msgs = [f\"{e.field}: {e.message}\" for e in validation_errors if e.severity == \"error\"]\n                raise JsonExtractionError(f\"Response validation failed: {'; '.join(error_msgs)}\")\n            \n            # Log validation warnings\n            for error in validation_errors:\n                if error.severity == \"warning\":\n                    logger.warning(f\"Validation warning - {error.field}: {error.message}\")\n            \n            return AnalysisResult(\n                summary=data.get(\"summary\", \"\"),\n                recommendations=data.get(\"recommendations\", []),\n                tasks=self._normalize_tasks(data.get(\"tasks\", [])),\n                raw_response=content,\n                success=True,\n            )\n        \n        if strict:\n            raise JsonExtractionError(\"No valid JSON found in response\")\n            \n    except JsonExtractionError:\n        if strict:\n            raise\n        logger.warning(\"JSON extraction failed, using fallback\")\n    except Exception as e:\n        logger.warning(f\"Unexpected error in _parse_response: {e}\")\n        if strict:\n            raise JsonExtractionError(f\"Parse error: {e}\") from e\n    \n    # Fallback: Create structured result from raw text\n    logger.debug(\"Using fallback text parsing\")\n    return self._create_fallback_result(content)\n```\n\n### 4. Update Module Exports\n\nAdd new exports to `src/metaagent/__init__.py`:\n\n```python\nfrom .analysis import (\n    AnalysisResult,\n    AnalysisEngine,\n    MockAnalysisEngine,\n    PerplexityAnalysisEngine,\n    create_analysis_engine,\n    extract_json_from_response,\n    validate_analysis_response,\n    ValidationError,\n    JsonExtractionError,\n)\n```\n\n### 5. Integration with Orchestrator\n\nUpdate `src/metaagent/orchestrator.py:_parse_triage_response()` to optionally use the new extraction function:\n\n```python\ndef _parse_triage_response(self, analysis_result: AnalysisResult, strict: bool = False) -> TriageResult:\n    \"\"\"Parse the triage response into a TriageResult.\"\"\"\n    try:\n        response_text = analysis_result.summary or analysis_result.raw_response\n        \n        data, strategy = extract_json_from_response(response_text, strict=strict)\n        \n        if data:\n            # Validate triage-specific fields\n            triage_fields = [\"selected_prompts\", \"done\"]\n            is_valid, errors = validate_analysis_response(\n                data, \n                required_fields=triage_fields,\n                strict=False  # Triage has different requirements\n            )\n            \n            selected_prompts = self._validate_prompts(data.get(\"selected_prompts\", []))\n            \n            return TriageResult(\n                success=True,\n                done=data.get(\"done\", False),\n                assessment=data.get(\"assessment\", \"\"),\n                priority_issues=data.get(\"priority_issues\", []),\n                selected_prompts=selected_prompts,\n                reasoning=data.get(\"reasoning\", \"\"),\n            )\n        # ... rest of fallback logic\n```\n\n## File Changes Summary\n\n1. `src/metaagent/analysis.py` - Add extraction functions, validation, update _parse_response()\n2. `src/metaagent/__init__.py` - Export new functions\n3. `src/metaagent/orchestrator.py` - Optionally integrate with _parse_triage_response()",
        "testStrategy": "## Unit Tests (`tests/test_analysis.py`)\n\n### 1. Test `extract_json_from_response()` - Direct Parse Strategy\n```python\ndef test_extract_json_direct_parse():\n    \"\"\"Test direct JSON parsing.\"\"\"\n    content = '{\"summary\": \"test\", \"recommendations\": [], \"tasks\": []}'\n    data, strategy = extract_json_from_response(content)\n    \n    assert data[\"summary\"] == \"test\"\n    assert strategy == \"direct_parse\"\n\ndef test_extract_json_direct_parse_with_whitespace():\n    \"\"\"Test direct parse handles whitespace.\"\"\"\n    content = '  \\n  {\"summary\": \"test\"}\\n  '\n    data, strategy = extract_json_from_response(content)\n    \n    assert data[\"summary\"] == \"test\"\n    assert strategy == \"direct_parse\"\n```\n\n### 2. Test Markdown Code Block Strategy\n```python\ndef test_extract_json_markdown_block():\n    \"\"\"Test extraction from markdown code block.\"\"\"\n    content = '''Here's my analysis:\n    \n```json\n{\"summary\": \"markdown test\", \"recommendations\": [\"rec1\"]}\n```\n\nSome additional notes.'''\n    \n    data, strategy = extract_json_from_response(content)\n    \n    assert data[\"summary\"] == \"markdown test\"\n    assert strategy == \"markdown_code_block\"\n\ndef test_extract_json_markdown_block_no_language():\n    \"\"\"Test extraction from code block without json specifier.\"\"\"\n    content = '```\\n{\"summary\": \"no lang\"}\\n```'\n    data, strategy = extract_json_from_response(content)\n    \n    assert data[\"summary\"] == \"no lang\"\n```\n\n### 3. Test Balanced Braces Strategy\n```python\ndef test_extract_json_balanced_braces():\n    \"\"\"Test balanced brace extraction.\"\"\"\n    content = 'Analysis result: {\"summary\": \"brace test\", \"nested\": {\"key\": \"value\"}} end.'\n    data, strategy = extract_json_from_response(content)\n    \n    assert data[\"summary\"] == \"brace test\"\n    assert data[\"nested\"][\"key\"] == \"value\"\n    assert strategy == \"balanced_braces\"\n\ndef test_extract_json_handles_nested_braces():\n    \"\"\"Test deeply nested JSON extraction.\"\"\"\n    content = 'Result: {\"a\": {\"b\": {\"c\": \"deep\"}}} done'\n    data, strategy = extract_json_from_response(content)\n    \n    assert data[\"a\"][\"b\"][\"c\"] == \"deep\"\n\ndef test_extract_json_handles_strings_with_braces():\n    \"\"\"Test JSON with braces inside strings.\"\"\"\n    content = '{\"message\": \"Use {name} placeholder\", \"count\": 1}'\n    data, strategy = extract_json_from_response(content)\n    \n    assert data[\"message\"] == \"Use {name} placeholder\"\n```\n\n### 4. Test Strict Mode\n```python\ndef test_extract_json_strict_raises_on_failure():\n    \"\"\"Test strict mode raises exception.\"\"\"\n    from metaagent.analysis import JsonExtractionError\n    \n    content = \"This is plain text with no JSON\"\n    \n    with pytest.raises(JsonExtractionError) as exc_info:\n        extract_json_from_response(content, strict=True)\n    \n    assert \"Failed to extract valid JSON\" in str(exc_info.value)\n\ndef test_extract_json_non_strict_returns_empty():\n    \"\"\"Test non-strict mode returns empty dict.\"\"\"\n    content = \"No JSON here\"\n    data, strategy = extract_json_from_response(content, strict=False)\n    \n    assert data == {}\n    assert strategy == \"none\"\n```\n\n### 5. Test `validate_analysis_response()`\n```python\ndef test_validate_response_valid():\n    \"\"\"Test validation of valid response.\"\"\"\n    data = {\n        \"summary\": \"Test summary\",\n        \"recommendations\": [\"rec1\", \"rec2\"],\n        \"tasks\": [{\"title\": \"Task 1\", \"description\": \"Do something\"}]\n    }\n    \n    is_valid, errors = validate_analysis_response(data)\n    \n    assert is_valid is True\n    assert len(errors) == 0\n\ndef test_validate_response_missing_required_fields():\n    \"\"\"Test validation catches missing fields.\"\"\"\n    data = {\"summary\": \"Only summary\"}\n    \n    is_valid, errors = validate_analysis_response(data)\n    \n    assert is_valid is False\n    assert any(e.field == \"recommendations\" for e in errors)\n    assert any(e.field == \"tasks\" for e in errors)\n\ndef test_validate_response_wrong_types():\n    \"\"\"Test validation catches wrong types.\"\"\"\n    data = {\n        \"summary\": 123,  # Should be string\n        \"recommendations\": \"not a list\",\n        \"tasks\": \"also not a list\"\n    }\n    \n    is_valid, errors = validate_analysis_response(data)\n    \n    assert is_valid is False\n    assert len(errors) >= 3\n\ndef test_validate_response_custom_required_fields():\n    \"\"\"Test validation with custom required fields.\"\"\"\n    data = {\"selected_prompts\": [], \"done\": True}\n    \n    is_valid, errors = validate_analysis_response(\n        data, \n        required_fields=[\"selected_prompts\", \"done\"]\n    )\n    \n    assert is_valid is True\n\ndef test_validate_response_strict_mode():\n    \"\"\"Test strict mode treats warnings as errors.\"\"\"\n    data = {\n        \"summary\": \"Test\",\n        \"recommendations\": [],\n        \"tasks\": [{\"description\": \"Missing title\"}]  # title missing\n    }\n    \n    is_valid_normal, _ = validate_analysis_response(data, strict=False)\n    is_valid_strict, errors_strict = validate_analysis_response(data, strict=True)\n    \n    # In non-strict mode, missing task title is a warning\n    # In strict mode, it becomes an error\n    assert is_valid_normal is True  # warnings don't fail\n```\n\n### 6. Test Updated `_parse_response()`\n```python\ndef test_parse_response_strict_mode_success():\n    \"\"\"Test _parse_response with strict mode and valid JSON.\"\"\"\n    engine = PerplexityAnalysisEngine(api_key=\"test\")\n    \n    content = json.dumps({\n        \"summary\": \"Valid response\",\n        \"recommendations\": [\"Do this\"],\n        \"tasks\": [{\"title\": \"Task\", \"description\": \"Desc\"}]\n    })\n    \n    result = engine._parse_response(content, strict=True)\n    \n    assert result.success is True\n    assert result.summary == \"Valid response\"\n\ndef test_parse_response_strict_mode_failure():\n    \"\"\"Test _parse_response raises in strict mode on invalid input.\"\"\"\n    from metaagent.analysis import JsonExtractionError\n    \n    engine = PerplexityAnalysisEngine(api_key=\"test\")\n    \n    with pytest.raises(JsonExtractionError):\n        engine._parse_response(\"Not valid JSON\", strict=True)\n\ndef test_parse_response_non_strict_uses_fallback():\n    \"\"\"Test non-strict mode falls back gracefully.\"\"\"\n    engine = PerplexityAnalysisEngine(api_key=\"test\")\n    \n    content = \"This is plain text analysis without JSON.\"\n    result = engine._parse_response(content, strict=False)\n    \n    assert result.success is True\n    assert result.summary == content\n```\n\n### 7. Integration Test with Orchestrator\n```python\ndef test_triage_response_uses_new_extraction(mock_config, tmp_path):\n    \"\"\"Test orchestrator triage uses new JSON extraction.\"\"\"\n    # Setup orchestrator with mock engine\n    # Verify _parse_triage_response handles various formats\n```\n\n## Run Tests\n```bash\npytest tests/test_analysis.py -v --tb=short\npytest tests/test_analysis.py::TestExtractJson -v\npytest tests/test_analysis.py::TestValidateResponse -v\n```",
        "status": "done",
        "dependencies": [
          "5"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-14T09:58:52.801Z"
      },
      {
        "id": "36",
        "title": "Smoke Test Meta-Agent Dogfooding (Self-Refinement)",
        "description": "Create an end-to-end smoke test that runs `metaagent refine --profile automation_agent --repo .` on the meta-agent repository itself to verify the complete refinement pipeline works correctly, including PRD discovery, stage execution order, and plan file generation.",
        "details": "## Overview\n\nThis task creates a comprehensive smoke test that validates meta-agent's ability to \"dogfood\" itself - running the refinement pipeline on its own codebase. The test verifies the complete happy path from CLI invocation through plan generation.\n\n## Implementation Steps\n\n### 1. Create Smoke Test Module (`tests/test_smoke_dogfood.py`)\n\nCreate a new test file dedicated to end-to-end dogfooding tests:\n\n```python\n\"\"\"Smoke tests for meta-agent dogfooding (self-refinement).\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport pytest\n\n\nclass TestDogfoodingSmoke:\n    \"\"\"End-to-end smoke tests for running meta-agent on itself.\"\"\"\n\n    @pytest.fixture\n    def project_root(self) -> Path:\n        \"\"\"Get the project root directory.\"\"\"\n        return Path(__file__).parent.parent\n\n    @pytest.fixture\n    def backup_plan(self, project_root: Path) -> None:\n        \"\"\"Backup existing plan file before test, restore after.\"\"\"\n        plan_path = project_root / \"docs\" / \"mvp_improvement_plan.md\"\n        backup_path = plan_path.with_suffix(\".md.bak\")\n        \n        # Backup if exists\n        if plan_path.exists():\n            plan_path.rename(backup_path)\n        \n        yield\n        \n        # Restore backup after test\n        if backup_path.exists():\n            if plan_path.exists():\n                plan_path.unlink()\n            backup_path.rename(plan_path)\n\n    def test_dogfood_finds_prd(self, project_root: Path) -> None:\n        \"\"\"Verify meta-agent discovers docs/prd.md in its own repo.\"\"\"\n        prd_path = project_root / \"docs\" / \"prd.md\"\n        assert prd_path.exists(), f\"PRD not found at {prd_path}\"\n        \n        content = prd_path.read_text(encoding=\"utf-8\")\n        assert len(content) > 100, \"PRD appears empty or too short\"\n        assert \"Meta-Agent\" in content or \"meta-agent\" in content.lower()\n\n    def test_dogfood_mock_mode_runs_stages_in_order(\n        self, project_root: Path, backup_plan: None\n    ) -> None:\n        \"\"\"Run metaagent refine in mock mode and verify stages run in order.\"\"\"\n        result = subprocess.run(\n            [\n                sys.executable, \"-m\", \"metaagent.cli\",\n                \"refine\",\n                \"--profile\", \"automation_agent\",\n                \"--repo\", str(project_root),\n                \"--mock\",\n                \"--verbose\",\n            ],\n            cwd=project_root,\n            capture_output=True,\n            text=True,\n            timeout=120,\n        )\n        \n        # Check command succeeded\n        assert result.returncode == 0, f\"Command failed: {result.stderr}\"\n        \n        # Verify output indicates stages were processed\n        output = result.stdout + result.stderr\n        \n        # automation_agent profile stages from profiles.yaml:\n        expected_stages = [\n            \"architecture_layer_identification\",\n            \"architecture_design_pattern_identification\", \n            \"quality_error_analysis\",\n            \"quality_code_complexity_analysis\",\n            \"testing_unit_test_generation\",\n            \"improvement_best_practice_analysis\",\n        ]\n        \n        # Verify stages appear in output (may be in logs or status messages)\n        for stage in expected_stages:\n            # Stage names may appear in various forms\n            assert any(\n                part in output.lower() \n                for part in [stage, stage.replace(\"_\", \" \"), stage.replace(\"_\", \"-\")]\n            ), f\"Stage {stage} not found in output\"\n\n    def test_dogfood_produces_nonempty_plan(\n        self, project_root: Path, backup_plan: None\n    ) -> None:\n        \"\"\"Verify metaagent refine produces non-empty mvp_improvement_plan.md.\"\"\"\n        # Run the refinement\n        result = subprocess.run(\n            [\n                sys.executable, \"-m\", \"metaagent.cli\",\n                \"refine\",\n                \"--profile\", \"automation_agent\",\n                \"--repo\", str(project_root),\n                \"--mock\",\n            ],\n            cwd=project_root,\n            capture_output=True,\n            text=True,\n            timeout=120,\n        )\n        \n        assert result.returncode == 0, f\"Command failed: {result.stderr}\"\n        \n        # Check plan file was created\n        plan_path = project_root / \"docs\" / \"mvp_improvement_plan.md\"\n        assert plan_path.exists(), f\"Plan file not created at {plan_path}\"\n        \n        # Verify plan is non-empty\n        content = plan_path.read_text(encoding=\"utf-8\")\n        assert len(content) > 500, f\"Plan file too short ({len(content)} chars)\"\n\n    def test_dogfood_plan_has_sensible_structure(\n        self, project_root: Path, backup_plan: None\n    ) -> None:\n        \"\"\"Verify the generated plan has expected structure and content.\"\"\"\n        # Run the refinement\n        subprocess.run(\n            [\n                sys.executable, \"-m\", \"metaagent.cli\",\n                \"refine\",\n                \"--profile\", \"automation_agent\",\n                \"--repo\", str(project_root),\n                \"--mock\",\n            ],\n            cwd=project_root,\n            capture_output=True,\n            text=True,\n            timeout=120,\n        )\n        \n        plan_path = project_root / \"docs\" / \"mvp_improvement_plan.md\"\n        content = plan_path.read_text(encoding=\"utf-8\")\n        \n        # Verify expected sections\n        assert \"# MVP Improvement Plan\" in content, \"Missing header\"\n        assert \"## PRD Summary\" in content, \"Missing PRD Summary section\"\n        assert \"## Analysis Stages\" in content, \"Missing Analysis Stages section\"\n        assert \"## Implementation Tasks\" in content, \"Missing Tasks section\"\n        assert \"## Instructions for Claude Code\" in content, \"Missing instructions\"\n        \n        # Verify profile name appears\n        assert \"Automation Agent\" in content or \"automation_agent\" in content\n        \n        # Verify has checkbox tasks (mock produces at least placeholder tasks)\n        assert \"- [ ]\" in content, \"No checkbox tasks found in plan\"\n\n    def test_dogfood_quick_review_profile(\n        self, project_root: Path, backup_plan: None\n    ) -> None:\n        \"\"\"Test with quick_review profile for faster smoke test.\"\"\"\n        result = subprocess.run(\n            [\n                sys.executable, \"-m\", \"metaagent.cli\",\n                \"refine\",\n                \"--profile\", \"quick_review\",\n                \"--repo\", str(project_root),\n                \"--mock\",\n            ],\n            cwd=project_root,\n            capture_output=True,\n            text=True,\n            timeout=60,\n        )\n        \n        assert result.returncode == 0, f\"Command failed: {result.stderr}\"\n        \n        plan_path = project_root / \"docs\" / \"mvp_improvement_plan.md\"\n        assert plan_path.exists()\n        \n        content = plan_path.read_text(encoding=\"utf-8\")\n        # quick_review uses quality_error_analysis and quality_code_complexity_analysis\n        assert \"Analysis Stages\" in content\n\n\nclass TestDogfoodingDryRun:\n    \"\"\"Dry-run tests for validating prompts without API calls.\"\"\"\n\n    @pytest.fixture\n    def project_root(self) -> Path:\n        \"\"\"Get the project root directory.\"\"\"\n        return Path(__file__).parent.parent\n\n    def test_dry_run_shows_planned_stages(self, project_root: Path) -> None:\n        \"\"\"Verify dry-run mode shows planned API calls and token estimates.\"\"\"\n        result = subprocess.run(\n            [\n                sys.executable, \"-m\", \"metaagent.cli\",\n                \"refine\",\n                \"--profile\", \"automation_agent\",\n                \"--repo\", str(project_root),\n                \"--dry-run\",\n            ],\n            cwd=project_root,\n            capture_output=True,\n            text=True,\n            timeout=60,\n        )\n        \n        assert result.returncode == 0, f\"Command failed: {result.stderr}\"\n        \n        output = result.stdout\n        \n        # Verify dry-run indicators\n        assert \"DRY\" in output.upper() or \"dry\" in output.lower()\n        \n        # Verify token estimates shown\n        assert \"token\" in output.lower(), \"Token estimates not shown\"\n        \n        # Verify stage count shown\n        assert \"stage\" in output.lower()\n```\n\n### 2. Add pytest marker for slow tests\n\nIn `pyproject.toml` or `pytest.ini`, ensure smoke tests can be selectively run:\n\n```toml\n[tool.pytest.ini_options]\nmarkers = [\n    \"smoke: marks tests as smoke tests (deselect with '-m \\\"not smoke\\\"')\",\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n]\n```\n\nAdd markers to smoke test class:\n```python\n@pytest.mark.smoke\n@pytest.mark.slow\nclass TestDogfoodingSmoke:\n    ...\n```\n\n### 3. Key Verification Points\n\nThe smoke test validates:\n\n1. **PRD Discovery**: `docs/prd.md` exists and contains meaningful content\n2. **Stage Execution Order**: All stages from `automation_agent` profile run (from `config/profiles.yaml`:\n   - architecture_layer_identification\n   - architecture_design_pattern_identification\n   - quality_error_analysis\n   - quality_code_complexity_analysis\n   - testing_unit_test_generation\n   - improvement_best_practice_analysis\n\n3. **Plan Generation**: `docs/mvp_improvement_plan.md` is:\n   - Created successfully\n   - Non-empty (>500 chars)\n   - Contains expected sections (header, PRD summary, stages, tasks, instructions)\n   - Contains checkbox tasks for Claude Code\n\n4. **No Regressions**: CLI exits with code 0, no exceptions thrown\n\n### 4. Fixture for Plan File Backup\n\nThe `backup_plan` fixture ensures:\n- Existing plan files are not destroyed by tests\n- Tests start with a clean state\n- Original plan is restored after test completion\n\n### 5. Environment Considerations\n\nTests use `--mock` mode to avoid requiring API keys. For full integration testing with real APIs, a separate test can be added with appropriate skip markers:\n\n```python\n@pytest.mark.skipif(\n    not os.getenv(\"PERPLEXITY_API_KEY\"),\n    reason=\"PERPLEXITY_API_KEY not set\"\n)\ndef test_dogfood_real_api(self, project_root: Path, backup_plan: None) -> None:\n    \"\"\"Integration test with real API (requires PERPLEXITY_API_KEY).\"\"\"\n    ...\n```\n\n## Files to Create/Modify\n\n- **Create**: `tests/test_smoke_dogfood.py` - New smoke test module\n- **Modify**: `pyproject.toml` - Add pytest markers (if not present)",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Test Verification\n\nRun the new smoke tests in isolation:\n\n```bash\n# Run all smoke tests\npytest tests/test_smoke_dogfood.py -v\n\n# Run with verbose output to see stage execution\npytest tests/test_smoke_dogfood.py -v --capture=no\n\n# Run only the quick smoke test\npytest tests/test_smoke_dogfood.py::TestDogfoodingSmoke::test_dogfood_quick_review_profile -v\n```\n\n### 2. Verify PRD Discovery Test\n\n```bash\npytest tests/test_smoke_dogfood.py::TestDogfoodingSmoke::test_dogfood_finds_prd -v\n```\n\nExpected: Pass, confirming `docs/prd.md` exists with valid content.\n\n### 3. Verify Stage Execution Order Test\n\n```bash\npytest tests/test_smoke_dogfood.py::TestDogfoodingSmoke::test_dogfood_mock_mode_runs_stages_in_order -v\n```\n\nExpected: Pass, all 6 stages from `automation_agent` profile appear in output.\n\n### 4. Verify Plan Generation Tests\n\n```bash\npytest tests/test_smoke_dogfood.py::TestDogfoodingSmoke::test_dogfood_produces_nonempty_plan -v\npytest tests/test_smoke_dogfood.py::TestDogfoodingSmoke::test_dogfood_plan_has_sensible_structure -v\n```\n\nExpected: \n- Plan file created at `docs/mvp_improvement_plan.md`\n- File is >500 characters\n- Contains all required sections\n\n### 5. Verify Dry-Run Mode\n\n```bash\npytest tests/test_smoke_dogfood.py::TestDogfoodingDryRun::test_dry_run_shows_planned_stages -v\n```\n\nExpected: Token estimates and planned stages displayed without actual API calls.\n\n### 6. Manual CLI Verification\n\nRun the actual CLI command manually to verify end-to-end:\n\n```bash\n# From project root\nmetaagent refine --profile automation_agent --repo . --mock --verbose\n\n# Verify output\ncat docs/mvp_improvement_plan.md\n```\n\nCheck:\n- Command exits with code 0\n- Output shows all stages completing\n- Plan file contains sensible tasks\n\n### 7. Full Test Suite Regression\n\n```bash\n# Run full test suite to ensure no regressions\npytest --tb=short\n\n# Run with coverage\npytest --cov=metaagent --cov-report=term-missing\n```\n\n### 8. CI Integration Verification\n\nEnsure smoke tests are included in CI pipeline. They should:\n- Run on every PR\n- Use `--mock` mode (no API keys required)\n- Complete within reasonable timeout (2 minutes)\n\n### Acceptance Criteria\n\n- [ ] `test_dogfood_finds_prd` passes\n- [ ] `test_dogfood_mock_mode_runs_stages_in_order` passes with all 6 stages\n- [ ] `test_dogfood_produces_nonempty_plan` creates plan >500 chars\n- [ ] `test_dogfood_plan_has_sensible_structure` finds all expected sections\n- [ ] `test_dogfood_quick_review_profile` passes with quick profile\n- [ ] `test_dry_run_shows_planned_stages` shows token estimates\n- [ ] Existing tests continue to pass (no regressions)\n- [ ] Manual `metaagent refine --profile automation_agent --repo . --mock` succeeds",
        "status": "pending",
        "dependencies": [
          "13",
          "18"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": "37",
        "title": "Implement Stage-Aware Prompt Selection with Full Prompt Library Triage",
        "description": "Replace fixed prompt selection with AI-driven stage-aware triage that evaluates ALL relevant prompts from the full Codebase Digest library for each conceptual stage (architecture, quality, security, etc.), selecting the most appropriate prompts dynamically based on codebase analysis.",
        "details": "## Overview\n\nCurrently, the meta-agent uses a fixed mapping in `prompts.py:DEFAULT_STAGE_PROMPTS` that statically maps stages to a small subset of prompts. The triage in `meta_triage.md` lists only ~20 prompts while the full prompt library contains 70+ prompts. This task implements intelligent per-stage prompt selection from the complete library.\n\n## Implementation Steps\n\n### 1. Discover and Document Existing Wiring (`src/metaagent/prompts.py`)\n\nThe current architecture:\n- `DEFAULT_STAGE_PROMPTS` (lines 44-70): Static dict mapping stages to 2-3 fixed prompts\n- `PromptLibrary.get_prompts_for_stage()` (lines 474-487): Returns prompts from DEFAULT_STAGE_PROMPTS\n- `TriageEngine._validate_prompts()` (orchestrator.py:471-494): Validates selected prompts exist\n- `meta_triage.md`: Lists ~20 prompts, missing 50+ from the library\n\n### 2. Create Stage-to-Candidate-Prompts Configuration\n\nCreate `config/stage_candidates.yaml`:\n```yaml\n# Maps conceptual stages to ALL candidate prompts that could be relevant\nstage_candidates:\n  architecture:\n    - architecture_layer_identification\n    - architecture_design_pattern_identification\n    - architecture_coupling_cohesion_analysis\n    - architecture_api_conformance_check\n    - architecture_api_client_code_generation\n    - architecture_database_schema_review\n    - architecture_database_schema_documentation\n    - architecture_diagram_generation\n    - architecture_refactoring_for_design_patterns\n    max_prompts: 3  # AI selects up to 3 from candidates\n    \n  quality:\n    - quality_error_analysis\n    - quality_code_complexity_analysis\n    - quality_code_duplication_analysis\n    - quality_code_style_consistency_analysis\n    - quality_code_documentation_coverage_analysis\n    - quality_risk_assessment\n    - quality_documentation_generation\n    max_prompts: 3\n    \n  security:\n    - security_vulnerability_analysis\n    max_prompts: 2\n    \n  performance:\n    - performance_bottleneck_identification\n    - performance_code_optimization_suggestions\n    - performance_scalability_analysis\n    - performance_resource_usage_profiling\n    - performance_concurrency_synchronization_analysis\n    - performance_test_scenario_generation\n    - performance_configuration_tuning\n    max_prompts: 3\n    \n  testing:\n    - testing_unit_test_generation\n    max_prompts: 2\n    \n  evolution:\n    - evolution_technical_debt_estimation\n    - evolution_code_churn_hotspot_analysis\n    - evolution_refactoring_recommendation_generation\n    - evolution_impact_analysis_of_code_changes\n    - evolution_codebase_evolution_visualization\n    - evolution_code_evolution_report_generation\n    max_prompts: 3\n    \n  improvement:\n    - improvement_refactoring\n    - improvement_best_practice_analysis\n    - improvement_language_translation\n    max_prompts: 2\n    \n  learning:\n    - learning_backend_code_analysis\n    - learning_frontend_code_analysis\n    - learning_code_pattern_recognition\n    - learning_code_review_checklist\n    - learning_code_style_readability_analysis\n    # ... all 15 learning prompts\n    max_prompts: 2\n    \n  business:\n    - business_impact_analysis\n    - business_model_canvas_analysis\n    - swot_analysis\n    - value_chain_analysis\n    # ... all business strategy prompts\n    max_prompts: 2\n```\n\n### 3. Update PromptLibrary to Load Stage Candidates\n\nAdd to `src/metaagent/prompts.py`:\n\n```python\n@dataclass\nclass StageConfig:\n    \"\"\"Configuration for a conceptual stage.\"\"\"\n    candidates: list[str]\n    max_prompts: int = 3\n\nclass PromptLibrary:\n    def __init__(\n        self,\n        prompts_path: Optional[Path] = None,\n        profiles_path: Optional[Path] = None,\n        prompt_library_path: Optional[Path] = None,\n        stage_candidates_path: Optional[Path] = None,  # NEW\n    ):\n        self.stage_candidates_path = stage_candidates_path\n        self._stage_configs: dict[str, StageConfig] = {}\n    \n    def _load_stage_candidates(self) -> None:\n        \"\"\"Load stage-to-candidate-prompts mapping from YAML.\"\"\"\n        if not self.stage_candidates_path or not self.stage_candidates_path.exists():\n            # Fall back to DEFAULT_STAGE_PROMPTS\n            for stage, prompts in DEFAULT_STAGE_PROMPTS.items():\n                self._stage_configs[stage] = StageConfig(candidates=prompts)\n            return\n        \n        with open(self.stage_candidates_path) as f:\n            data = yaml.safe_load(f) or {}\n        \n        for stage, config in data.get(\"stage_candidates\", {}).items():\n            if isinstance(config, list):\n                self._stage_configs[stage] = StageConfig(candidates=config)\n            else:\n                self._stage_configs[stage] = StageConfig(\n                    candidates=config.get(\"candidates\", config),\n                    max_prompts=config.get(\"max_prompts\", 3),\n                )\n    \n    def get_stage_config(self, stage: str) -> Optional[StageConfig]:\n        \"\"\"Get configuration for a stage.\"\"\"\n        self.load()\n        return self._stage_configs.get(stage)\n    \n    def get_all_candidate_prompts_for_stage(self, stage: str) -> list[Prompt]:\n        \"\"\"Get all candidate prompts for a stage (for triage to select from).\"\"\"\n        self.load()\n        config = self._stage_configs.get(stage)\n        if not config:\n            return []\n        return [self.get_prompt(pid) for pid in config.candidates if self.get_prompt(pid)]\n```\n\n### 4. Create Stage-Specific Triage Prompt\n\nCreate `config/prompt_library/meta_stage_triage.md`:\n\n```markdown\n# Stage-Specific Prompt Selection\n\n**Objective:** Select the most relevant analysis prompts for the {{stage}} stage from the available candidates.\n\n**Context:** You are a senior software architect. Given the codebase and PRD, select which prompts would provide the most value for the {{stage}} analysis phase.\n\n**Candidate Prompts for {{stage}}:**\n{{#each candidates}}\n- `{{this.id}}`: {{this.goal}}\n{{/each}}\n\n**Instructions:**\n1. Review the codebase structure and current state\n2. Consider what aspects of {{stage}} need the most attention\n3. Select up to {{max_prompts}} prompts that would provide the most value\n4. Prioritize prompts that address gaps between the current code and PRD requirements\n\n**Response Format (JSON only):**\n```json\n{\n  \"selected_prompts\": [\"prompt_id_1\", \"prompt_id_2\"],\n  \"reasoning\": \"Brief explanation of why these prompts were selected for {{stage}}\"\n}\n```\n```\n\n### 5. Update TriageEngine for Per-Stage Selection\n\nModify `src/metaagent/orchestrator.py`:\n\n```python\n@dataclass\nclass StageTriageResult:\n    \"\"\"Result from stage-specific triage.\"\"\"\n    success: bool\n    stage: str\n    selected_prompts: list[str] = field(default_factory=list)\n    reasoning: str = \"\"\n    error: Optional[str] = None\n\n\nclass TriageEngine:\n    def __init__(\n        self,\n        analysis_engine: AnalysisEngine,\n        prompt_library: PromptLibrary,\n        max_prompts_per_iteration: int = 3,\n    ):\n        self.analysis_engine = analysis_engine\n        self.prompt_library = prompt_library\n        self.max_prompts_per_iteration = max_prompts_per_iteration\n    \n    def triage_stage(\n        self,\n        stage: str,\n        prd_content: str,\n        code_context: str,\n        history: RunHistory,\n    ) -> StageTriageResult:\n        \"\"\"Run triage for a specific stage to select best prompts.\n        \n        Args:\n            stage: The conceptual stage (e.g., 'architecture', 'quality').\n            prd_content: The PRD content.\n            code_context: The packed codebase.\n            history: Previous analysis history.\n        \n        Returns:\n            StageTriageResult with selected prompts for this stage.\n        \"\"\"\n        stage_config = self.prompt_library.get_stage_config(stage)\n        if not stage_config:\n            return StageTriageResult(\n                success=False,\n                stage=stage,\n                error=f\"No configuration found for stage: {stage}\",\n            )\n        \n        # Get candidate prompts for this stage\n        candidates = self.prompt_library.get_all_candidate_prompts_for_stage(stage)\n        if not candidates:\n            return StageTriageResult(\n                success=False,\n                stage=stage,\n                error=f\"No candidate prompts found for stage: {stage}\",\n            )\n        \n        # Build stage-specific triage prompt\n        triage_prompt = self._build_stage_triage_prompt(\n            stage=stage,\n            candidates=candidates,\n            max_prompts=stage_config.max_prompts,\n            prd_content=prd_content,\n            code_context=code_context,\n            history=history,\n        )\n        \n        # Run triage\n        result = self.analysis_engine.analyze(triage_prompt)\n        if not result.success:\n            return StageTriageResult(\n                success=False,\n                stage=stage,\n                error=result.error,\n            )\n        \n        return self._parse_stage_triage_response(stage, result, candidates)\n    \n    def _build_stage_triage_prompt(\n        self,\n        stage: str,\n        candidates: list[Prompt],\n        max_prompts: int,\n        prd_content: str,\n        code_context: str,\n        history: RunHistory,\n    ) -> str:\n        \"\"\"Build the stage-specific triage prompt.\"\"\"\n        candidate_list = \"\\n\".join([\n            f\"- `{p.id}`: {p.goal}\" for p in candidates\n        ])\n        \n        return f\"\"\"# Stage-Specific Prompt Selection: {stage.title()}\n\n## Product Requirements Document\n{prd_content}\n\n## Codebase\n{code_context}\n\n## Previous Analysis\n{history.format_for_prompt()}\n\n---\n\n**Objective:** Select the most relevant analysis prompts for the {stage} stage.\n\n**Candidate Prompts:**\n{candidate_list}\n\n**Instructions:**\n1. Review the codebase and PRD\n2. Select up to {max_prompts} prompts that would provide the most value for {stage} analysis\n3. Prioritize prompts addressing gaps between current code and PRD requirements\n\n**Response (JSON only):**\n```json\n{{\n  \"selected_prompts\": [\"prompt_id_1\", \"prompt_id_2\"],\n  \"reasoning\": \"Why these prompts were selected\"\n}}\n```\"\"\"\n```\n\n### 6. Update Orchestrator to Use Stage-Aware Triage\n\nAdd new method to `Orchestrator`:\n\n```python\ndef refine_with_stage_triage(\n    self,\n    stages: list[str],\n) -> RefinementResult:\n    \"\"\"Run refinement with per-stage AI triage.\n    \n    Instead of running fixed prompts, this method:\n    1. For each conceptual stage, runs triage to select best prompts\n    2. Runs the selected prompts\n    3. Aggregates results\n    \n    Args:\n        stages: List of conceptual stages to run (e.g., ['architecture', 'quality']).\n    \n    Returns:\n        RefinementResult with outcomes.\n    \"\"\"\n    logger.info(f\"Starting stage-aware refinement: {stages}\")\n    \n    prd_content = self._load_prd()\n    if prd_content is None:\n        return RefinementResult(\n            success=False,\n            profile_name=\"stage-triage\",\n            stages_completed=0,\n            stages_failed=0,\n            error=f\"PRD file not found: {self.config.prd_path}\",\n        )\n    \n    code_context = self._pack_codebase()\n    history = RunHistory()\n    all_results: list[StageResult] = []\n    completed = 0\n    failed = 0\n    \n    for stage in stages:\n        logger.info(f\"=== Stage: {stage} ===\")\n        \n        # Step 1: Triage to select prompts for this stage\n        triage_result = self.triage_engine.triage_stage(\n            stage=stage,\n            prd_content=prd_content,\n            code_context=code_context,\n            history=history,\n        )\n        \n        if not triage_result.success:\n            logger.error(f\"Triage failed for {stage}: {triage_result.error}\")\n            failed += 1\n            continue\n        \n        logger.info(f\"Selected prompts for {stage}: {triage_result.selected_prompts}\")\n        \n        # Step 2: Run selected prompts\n        prompts = [\n            self.prompt_library.get_prompt(pid)\n            for pid in triage_result.selected_prompts\n            if self.prompt_library.get_prompt(pid)\n        ]\n        \n        stage_results, stage_completed, stage_failed = self.stage_runner.run_stages(\n            prompts=prompts,\n            prd_content=prd_content,\n            code_context=code_context,\n            history=history,\n        )\n        \n        all_results.extend(stage_results)\n        completed += stage_completed\n        failed += stage_failed\n    \n    # Write plan\n    plan_path = None\n    if all_results:\n        plan_path = self.plan_writer.write_plan(\n            prd_content=prd_content,\n            profile_name=\"Stage-Aware Triage\",\n            stage_results=all_results,\n        )\n    \n    return RefinementResult(\n        success=failed == 0 and completed > 0,\n        profile_name=\"stage-triage\",\n        stages_completed=completed,\n        stages_failed=failed,\n        plan_path=plan_path,\n        stage_results=all_results,\n        partial_success=failed > 0 and completed > 0,\n    )\n```\n\n### 7. Add CLI Command for Stage-Aware Triage\n\nAdd to `src/metaagent/cli.py`:\n\n```python\n@app.command(\"refine-smart\")\ndef refine_smart(\n    stages: list[str] = typer.Option(\n        [\"architecture\", \"quality\"],\n        \"--stage\",\n        \"-s\",\n        help=\"Stages to run (can specify multiple).\",\n    ),\n    repo: Path = typer.Option(Path.cwd(), \"--repo\", \"-r\"),\n    prd: Optional[Path] = typer.Option(None, \"--prd\"),\n    config_dir: Optional[Path] = typer.Option(None, \"--config-dir\", \"-c\"),\n    mock: bool = typer.Option(False, \"--mock\", \"-m\"),\n    verbose: bool = typer.Option(False, \"--verbose\"),\n) -> None:\n    \"\"\"Run refinement with AI-driven per-stage prompt selection.\n    \n    For each stage, the AI evaluates ALL candidate prompts and selects\n    the most relevant ones based on the current codebase state.\n    \n    Available stages: architecture, quality, security, performance,\n    testing, evolution, improvement, learning, business\n    \"\"\"\n    # Implementation similar to refine() but calling refine_with_stage_triage()\n```\n\n### 8. Preserve JSON Schema Compatibility\n\nEnsure the stage triage response parsing maintains the same JSON schema structure used by `TriageEngine._parse_triage_response()`. The `StageTriageResult` dataclass follows the established pattern.\n\n### 9. Update Config to Include Stage Candidates Path\n\nAdd to `src/metaagent/config.py`:\n\n```python\n@property\ndef stage_candidates_path(self) -> Path:\n    \"\"\"Path to stage_candidates.yaml.\"\"\"\n    return self.config_dir / \"stage_candidates.yaml\"\n```\n\n### Files to Create/Modify:\n- **Create**: `config/stage_candidates.yaml` - Stage-to-candidates mapping\n- **Create**: `config/prompt_library/meta_stage_triage.md` - Stage triage prompt (optional)\n- **Modify**: `src/metaagent/prompts.py` - Add `StageConfig`, `get_stage_config()`, `get_all_candidate_prompts_for_stage()`\n- **Modify**: `src/metaagent/orchestrator.py` - Add `StageTriageResult`, `triage_stage()`, `refine_with_stage_triage()`\n- **Modify**: `src/metaagent/config.py` - Add `stage_candidates_path` property\n- **Modify**: `src/metaagent/cli.py` - Add `refine-smart` command",
        "testStrategy": "## Unit Tests (`tests/test_stage_triage.py`)\n\n### 1. Test Stage Configuration Loading\n```python\ndef test_load_stage_candidates_yaml(tmp_path: Path) -> None:\n    \"\"\"Test loading stage candidates from YAML file.\"\"\"\n    # Create stage_candidates.yaml\n    config_file = tmp_path / \"stage_candidates.yaml\"\n    config_file.write_text(\"\"\"\nstage_candidates:\n  architecture:\n    - architecture_layer_identification\n    - architecture_design_pattern_identification\n    max_prompts: 3\n  quality:\n    - quality_error_analysis\n    max_prompts: 2\n\"\"\")\n    \n    library = PromptLibrary(stage_candidates_path=config_file)\n    library.load()\n    \n    arch_config = library.get_stage_config(\"architecture\")\n    assert arch_config is not None\n    assert len(arch_config.candidates) == 2\n    assert arch_config.max_prompts == 3\n\ndef test_fallback_to_default_stage_prompts(tmp_path: Path) -> None:\n    \"\"\"Test fallback when stage_candidates.yaml doesn't exist.\"\"\"\n    library = PromptLibrary(stage_candidates_path=tmp_path / \"nonexistent.yaml\")\n    library.load()\n    \n    arch_config = library.get_stage_config(\"architecture\")\n    assert arch_config is not None  # Should use DEFAULT_STAGE_PROMPTS\n```\n\n### 2. Test Stage-Specific Triage\n```python\ndef test_triage_stage_selects_from_candidates(mock_analysis_engine) -> None:\n    \"\"\"Test that stage triage selects prompts from candidate list.\"\"\"\n    # Setup mock to return specific prompts\n    mock_analysis_engine.responses[\"Stage-Specific Prompt Selection\"] = AnalysisResult(\n        summary='{\"selected_prompts\": [\"quality_error_analysis\"], \"reasoning\": \"test\"}',\n        raw_response='{\"selected_prompts\": [\"quality_error_analysis\"], \"reasoning\": \"test\"}',\n        success=True,\n    )\n    \n    triage_engine = TriageEngine(mock_analysis_engine, prompt_library)\n    result = triage_engine.triage_stage(\n        stage=\"quality\",\n        prd_content=\"Test PRD\",\n        code_context=\"Test code\",\n        history=RunHistory(),\n    )\n    \n    assert result.success\n    assert \"quality_error_analysis\" in result.selected_prompts\n\ndef test_triage_stage_validates_against_candidates() -> None:\n    \"\"\"Test that triage rejects prompts not in candidate list.\"\"\"\n    # If AI returns a prompt not in candidates, it should be filtered out\n    ...\n\ndef test_triage_stage_respects_max_prompts() -> None:\n    \"\"\"Test that triage limits prompts to max_prompts setting.\"\"\"\n    ...\n```\n\n### 3. Test Orchestrator Stage-Aware Refinement\n```python\ndef test_refine_with_stage_triage_runs_all_stages(mock_config) -> None:\n    \"\"\"Test that refine_with_stage_triage processes all requested stages.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    result = orchestrator.refine_with_stage_triage(\n        stages=[\"architecture\", \"quality\"]\n    )\n    \n    assert result.stages_completed >= 2\n    assert \"stage-triage\" in result.profile_name\n\ndef test_refine_with_stage_triage_handles_missing_stage() -> None:\n    \"\"\"Test graceful handling of unknown stages.\"\"\"\n    ...\n```\n\n### 4. Test CLI Integration\n```python\ndef test_refine_smart_command_help(cli_runner) -> None:\n    \"\"\"Test refine-smart command shows help.\"\"\"\n    result = cli_runner.invoke(app, [\"refine-smart\", \"--help\"])\n    assert result.exit_code == 0\n    assert \"--stage\" in result.stdout\n\ndef test_refine_smart_with_mock(cli_runner, mock_config) -> None:\n    \"\"\"Test refine-smart in mock mode.\"\"\"\n    result = cli_runner.invoke(app, [\n        \"refine-smart\",\n        \"--mock\",\n        \"--stage\", \"architecture\",\n        \"--stage\", \"quality\",\n    ])\n    assert result.exit_code == 0\n```\n\n### 5. Test JSON Schema Preservation\n```python\ndef test_stage_triage_response_matches_schema() -> None:\n    \"\"\"Test that stage triage response follows expected JSON schema.\"\"\"\n    response = '{\"selected_prompts\": [\"p1\", \"p2\"], \"reasoning\": \"test\"}'\n    result = triage_engine._parse_stage_triage_response(\"quality\", response, candidates)\n    \n    assert isinstance(result.selected_prompts, list)\n    assert isinstance(result.reasoning, str)\n\ndef test_analysis_result_schema_unchanged() -> None:\n    \"\"\"Verify AnalysisResult schema is preserved for compatibility.\"\"\"\n    # Ensure new code doesn't break existing analysis flow\n    ...\n```\n\n## Integration Tests\n\n### 1. Full Pipeline Test\n```python\ndef test_stage_triage_end_to_end(tmp_path: Path) -> None:\n    \"\"\"Integration test: stage triage -> prompt selection -> analysis.\"\"\"\n    # Setup real prompt library with markdown prompts\n    # Run full stage-aware triage\n    # Verify prompts are selected and run\n```\n\n### 2. Test with Real Prompt Library\n```python\ndef test_all_library_prompts_are_candidates() -> None:\n    \"\"\"Verify all 70+ prompts appear in at least one stage's candidates.\"\"\"\n    library = PromptLibrary(\n        prompt_library_path=Path(\"config/prompt_library\"),\n        stage_candidates_path=Path(\"config/stage_candidates.yaml\"),\n    )\n    library.load()\n    \n    all_prompts = {p.id for p in library.list_prompts()}\n    all_candidates = set()\n    for stage_config in library._stage_configs.values():\n        all_candidates.update(stage_config.candidates)\n    \n    # Most prompts should be candidates (some meta/special prompts excluded)\n    coverage = len(all_candidates & all_prompts) / len(all_prompts)\n    assert coverage > 0.9, f\"Only {coverage*100:.0f}% of prompts are candidates\"\n```\n\n## Manual Testing Checklist\n\n1. [ ] Run `metaagent refine-smart --mock --stage architecture` - verify prompts selected\n2. [ ] Run `metaagent refine-smart --mock --stage quality --stage security` - verify multiple stages\n3. [ ] Verify stage_candidates.yaml loads correctly on startup\n4. [ ] Verify fallback to DEFAULT_STAGE_PROMPTS when config missing\n5. [ ] Check logs show which prompts were selected for each stage\n6. [ ] Verify output plan contains results from dynamically selected prompts",
        "status": "done",
        "dependencies": [
          "3",
          "5",
          "7",
          "14"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-14T10:23:22.201Z"
      },
      {
        "id": "38",
        "title": "Complete Meta-Agent Refinement Loop with ImplementationExecutor Integration",
        "description": "Integrate the ImplementationExecutor into all refine methods (refine, refine_iterative, refine_with_stage_triage) with full auto_implement flag support and comprehensive CLI testing on clipvid repo.",
        "details": "## Implementation Overview\n\nThis task completes the meta-agent refinement loop by wiring the existing `ImplementationExecutor` (from Task 18) into all three refine methods in `src/metaagent/orchestrator.py`. Currently only `refine_iterative()` calls it; extend to `refine()` and `refine_with_stage_triage()` with consistent `--auto-implement` flag handling.\n\n## Step-by-Step Implementation\n\n### 1. Add auto_implement Parameter to All Refine Methods\n\nUpdate method signatures in `Orchestrator` class:\n\n```python\nclass Orchestrator:\n    def refine(\n        self,\n        profile: str,\n        auto_implement: bool = False,  # NEW\n        max_iterations: int = 3,\n        # ... other params\n    ) -> RunHistory:\n        # ...\n\n    def refine_iterative(  # Already exists\n        self,\n        profile: str,\n        auto_implement: bool = False,\n        max_iterations: int = 10,\n        # ...\n    ) -> RunHistory:\n        # ...\n\n    def refine_with_stage_triage(  # NEW\n        self,\n        profile: str,\n        auto_implement: bool = False,  # NEW\n        # ...\n    ) -> RunHistory:\n        # ...\n```\n\n### 2. Implement ImplementationExecutor Integration\n\nCreate shared `_execute_improvements()` helper method:\n\n```python\ndef _execute_improvements(\n    self,\n    plan_path: Path,\n    repo_path: Path,\n    auto_implement: bool,\n    dry_run: bool = False\n) -> ExecutionResult:\n    if not auto_implement:\n        logger.info('Skipping auto-implementation (use --auto-implement)')\n        return ExecutionResult(success=False, message='Skipped')\n    \n    executor = ImplementationExecutor(repo_path=repo_path)\n    result = executor.execute_from_plan(plan_path, dry_run=dry_run)\n    \n    if result.success:\n        self._commit_changes(repo_path, result.changes)\n    return result\n```\n\n### 3. Wire into Each Refine Method\n\n**For `refine()` (single-pass):**\n```python\n# After plan_writer.write_plan()\nif config.auto_implement:\n    exec_result = self._execute_improvements(\n        plan_path, config.repo_path, config.auto_implement\n    )\n    history.add_execution(exec_result)\n```\n\n**For `refine_iterative()` (existing - enhance):**\n```python\n# In main loop, after each iteration's plan generation:\nexec_result = self._execute_improvements(\n    iteration_plan_path, config.repo_path, auto_implement\n)\nif exec_result.success:\n    logger.info('✅ Iteration completed with successful implementation')\n    break  # Optional: early exit on success\n```\n\n**For `refine_with_stage_triage()` (new logic):**\n```python\n# After triage determines implementation stage:\nif triage_result.should_implement and auto_implement:\n    exec_result = self._execute_improvements(\n        triage_plan_path, config.repo_path, auto_implement\n    )\n```\n\n### 4. Update CLI Flag Handling\n\nIn `src/metaagent/cli.py`, add `--auto-implement` to all commands:\n\n```python\n@app.command()\ndef refine(\n    repo: Path = typer.Argument(...),\n    profile: str = 'default',\n    auto_implement: bool = typer.Option(False, '--auto-implement'),\n    # ...\n):\n    config = Config.from_env(repo_path=repo)\n    config.auto_implement = auto_implement  # NEW\n    orchestrator = Orchestrator(config)\n    return orchestrator.refine(profile, auto_implement=auto_implement)\n\n# Repeat for refine-iterative and refine-with-stage-triage\n```\n\n### 5. Add ExecutionResult Dataclass\n\n```python\n@dataclass\nclass ExecutionResult:\n    success: bool\n    changes: list[Path]\n    message: str\n    commit_hash: str | None = None\n```\n\n### 6. Safety & Best Practices\n- Add `dry_run` flag for testing\n- Implement change validation before commit\n- Add max_changes limit (default: 50 files)\n- Log all executor actions with file diffs\n- Graceful fallback if executor fails",
        "testStrategy": "## Comprehensive Test Strategy\n\n### 1. Unit Tests (`tests/test_orchestrator.py`)\n```bash\npytest tests/test_orchestrator.py::TestImplementationExecutor -v\n```\n- Test `_execute_improvements()` skips when `auto_implement=False`\n- Test executor called with correct paths when `auto_implement=True`\n- Test `ExecutionResult` parsing from mock executor\n- Test all 3 refine methods pass `auto_implement` correctly\n- Test CLI config sets `config.auto_implement` properly\n\n### 2. Integration Tests (`tests/test_refine_methods.py`)\n- Mock `ImplementationExecutor.execute_from_plan()`\n- Verify each refine method → plan generation → executor call chain\n- Test dry_run mode doesn't modify files\n\n### 3. CLI End-to-End Tests (`tests/test_cli_auto_implement.py`)\n```python\ndef test_all_refine_modes_with_auto_implement():\n    for mode in ['refine', 'refine-iterative', 'refine-with-stage-triage']:\n        result = runner.invoke(app, [mode, sample_repo, '--auto-implement'])\n        assert result.exit_code == 0\n        assert 'ImplementationExecutor' in result.stdout\n        assert 'Changes applied' in result.stdout\n```\n\n### 4. Real-World E2E Test (clipvid repo)\n```bash\n# Setup clipvid test repo\ngit clone https://github.com/user/clipvid.git /tmp/clipvid-test\ncd /tmp/clipvid-test\n\n# Test each mode\nmetaagent refine . --auto-implement --dry-run\nmetaagent refine-iterative . --auto-implement --dry-run\nmetaagent refine-with-stage-triage . --auto-implement --dry-run\n\n# Verify:\n- No actual file changes (dry-run)\n- Executor logs show correct plan parsing\n- CLI help shows --auto-implement for all modes\n- Coverage >90%: pytest --cov=metaagent/orchestrator --cov-report=html\n```\n\n### 5. Smoke Test Matrix\n| Mode | auto_implement=False | auto_implement=True | dry_run=True |\n|------|---------------------|-------------------|-------------|\n| refine | ✅ Plan only | ✅ Plan+Executor | ✅ No changes |\n| iterative | ✅ | ✅ Loop+Executor | ✅ |\n| triage | ✅ | ✅ Triage+Executor | ✅ |\n\n### 6. CI Verification\n- All tests pass: `pytest tests/ -v`\n- No file modifications in dry-run\n- Executor integration coverage >95%\n- CLI flag consistency across modes",
        "status": "done",
        "dependencies": [
          "7",
          "8",
          "18"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Add auto_implement Parameter to Refine Methods",
            "description": "Update method signatures in Orchestrator class for refine(), refine_iterative(), and refine_with_stage_triage() to include auto_implement: bool parameter.",
            "dependencies": [],
            "details": "Modify src/metaagent/orchestrator.py to add auto_implement: bool = False to all three refine method signatures as shown in implementation step 1. Ensure default value is False for backward compatibility.",
            "status": "done",
            "testStrategy": "Unit test each method signature with pytest to verify parameter acceptance and default values. Test refine() and refine_with_stage_triage() parameter passing.",
            "parentId": "undefined",
            "updatedAt": "2025-12-14T12:39:22.100Z"
          },
          {
            "id": 2,
            "title": "Implement Shared _execute_improvements Helper Method",
            "description": "Create private helper method in Orchestrator to handle ImplementationExecutor calls with auto_implement logic and change committing.",
            "dependencies": [
              1
            ],
            "details": "Add def _execute_improvements(self, plan_path: Path, repo_path: Path, auto_implement: bool, dry_run: bool = False) -> ExecutionResult: in orchestrator.py per step 2. Include skip logic, executor instantiation, execution, and _commit_changes() call.",
            "status": "pending",
            "testStrategy": "Unit tests for _execute_improvements(): test skip when auto_implement=False, test executor call with correct paths when True, test commit on success, test dry_run mode.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Wire ImplementationExecutor into All Refine Methods",
            "description": "Integrate _execute_improvements() calls into refine(), enhance refine_iterative(), and add to refine_with_stage_triage() after plan generation.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement wiring per step 3: single-pass call in refine(), loop integration in refine_iterative() with optional early exit, triage-based call in refine_with_stage_triage(). Add history.add_execution() where applicable.",
            "status": "pending",
            "testStrategy": "Integration tests calling each refine method with auto_implement=True/False, verify executor called at correct points, check RunHistory captures ExecutionResult.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Update CLI Commands with --auto-implement Flag Support",
            "description": "Add --auto-implement typer.Option to refine, refine-iterative, refine-with-stage-triage commands and pass to Orchestrator methods.",
            "dependencies": [
              1
            ],
            "details": "In src/metaagent/cli.py, add auto_implement: bool = typer.Option(False, '--auto-implement') to each command. Set config.auto_implement = auto_implement and pass to orchestrator.{method}(..., auto_implement=auto_implement).",
            "status": "pending",
            "testStrategy": "CLI tests using typer.testing.CliRunner: test --help shows flag, test flag parsing sets config correctly, test end-to-end refine commands with/without flag.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add ExecutionResult Dataclass and Safety Features",
            "description": "Define ExecutionResult dataclass and implement safety features: dry_run flag, change validation, max_changes limit, and logging.",
            "dependencies": [
              2
            ],
            "details": "Create @dataclass ExecutionResult(success: bool, changes: list[Path], message: str, commit_hash: str | None = None). Add dry_run to CLI, validate changes before commit (max 50 files), log diffs.",
            "status": "pending",
            "testStrategy": "Unit tests for dataclass serialization, integration tests for safety: test dry_run skips commits, test max_changes rejection, test validation failures, full CLI test on clipvid repo.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-14T12:39:29.046Z"
      },
      {
        "id": "39",
        "title": "Refactor ClaudeCodeRunner to Return Structured Reports Instead of Subprocess Spawning",
        "description": "Refactor ClaudeCodeRunner in claude_runner.py to remove subprocess.run() calls that spawn Claude Code CLI, replacing them with structured data classes (TaskAnalysis, ImplementationRecommendation, ImplementationReport) and an _extract_tasks_from_plan() parser that returns structured reports for in-process consumption.",
        "details": "## Problem Statement\n\nThe current `ClaudeCodeRunner` implementation in `src/metaagent/claude_runner.py:25-208` spawns a separate Claude Code process via `subprocess.run(['claude', ...])` (lines 93-106). This breaks the intended architecture where meta-agent should orchestrate analysis and return structured data for consumption by other components, not delegate implementation to an external process.\n\n## Current Implementation Issues\n\n1. **Subprocess spawning at lines 52-58 and 93-106**: Calls `subprocess.run([\"claude\", ...])` which spawns external processes\n2. **ClaudeCodeResult dataclass** (lines 14-22): Returns opaque `output: str` instead of structured analysis\n3. **No task extraction**: The `_build_prompt()` method (lines 152-175) creates prompts but doesn't parse them back into structured data\n4. **Git subprocess at lines 188-204**: Uses subprocess for `git status` to track modified files\n\n## New Data Classes Required\n\nAdd the following dataclasses to `src/metaagent/claude_runner.py`:\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Any\nfrom enum import Enum\n\nclass TaskPriority(Enum):\n    CRITICAL = \"critical\"\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n\n@dataclass\nclass TaskAnalysis:\n    \"\"\"Analysis of a single task from the improvement plan.\"\"\"\n    task_id: str\n    title: str\n    description: str\n    priority: TaskPriority\n    file_path: Optional[str] = None\n    estimated_complexity: str = \"medium\"  # low, medium, high\n    dependencies: list[str] = field(default_factory=list)\n    acceptance_criteria: list[str] = field(default_factory=list)\n\n@dataclass\nclass ImplementationRecommendation:\n    \"\"\"Recommended implementation approach for a task.\"\"\"\n    task_id: str\n    approach: str\n    code_changes: list[dict[str, Any]] = field(default_factory=list)\n    # Each code_change: {file: str, change_type: str, description: str}\n    test_requirements: list[str] = field(default_factory=list)\n    risks: list[str] = field(default_factory=list)\n\n@dataclass\nclass ImplementationReport:\n    \"\"\"Complete structured report from plan analysis.\"\"\"\n    success: bool\n    tasks: list[TaskAnalysis] = field(default_factory=list)\n    recommendations: list[ImplementationRecommendation] = field(default_factory=list)\n    summary: str = \"\"\n    error: Optional[str] = None\n    total_tasks: int = 0\n    by_priority: dict[str, int] = field(default_factory=dict)\n```\n\n## Implementation Steps\n\n### Step 1: Add New Data Classes\nAdd `TaskAnalysis`, `ImplementationRecommendation`, and `ImplementationReport` dataclasses after the existing `ClaudeCodeResult` class.\n\n### Step 2: Implement `_extract_tasks_from_plan()`\nAdd a new method to parse improvement plans:\n\n```python\ndef _extract_tasks_from_plan(self, plan_content: str) -> list[TaskAnalysis]:\n    \"\"\"Extract structured tasks from an improvement plan markdown file.\n    \n    Parses the markdown structure generated by PlanWriter to extract:\n    - Task titles (from ### headers or checkbox items)\n    - Priorities (from section headers like '[CRITICAL]', '[HIGH]', etc.)\n    - Descriptions (from indented content)\n    - File references (from backtick-wrapped paths)\n    \n    Args:\n        plan_content: Raw markdown content of the improvement plan.\n        \n    Returns:\n        List of TaskAnalysis objects.\n    \"\"\"\n    # Implementation using regex patterns matching PlanWriter output format\n    # See plan_writer.py:130-172 for the format to parse\n```\n\n### Step 3: Create `analyze_plan()` Method\nReplace the subprocess-based `implement()` with a new method:\n\n```python\ndef analyze_plan(\n    self,\n    repo_path: Path,\n    plan_file: Optional[Path] = None,\n) -> ImplementationReport:\n    \"\"\"Analyze an improvement plan and return structured implementation report.\n    \n    This method replaces subprocess spawning with in-process analysis that:\n    1. Reads and parses the improvement plan file\n    2. Extracts structured tasks using _extract_tasks_from_plan()\n    3. Generates implementation recommendations\n    4. Returns a structured report for consumption by orchestrator\n    \n    Args:\n        repo_path: Path to the target repository.\n        plan_file: Optional path to the improvement plan file.\n        \n    Returns:\n        ImplementationReport with structured analysis.\n    \"\"\"\n```\n\n### Step 4: Update `ImplementationExecutor` Integration\nModify `orchestrator.py:726-944` (`ImplementationExecutor` class) to use the new structured API:\n\n- Change `execute()` method to call `analyze_plan()` instead of `implement()`\n- Update `_run_claude_code()` to work with `ImplementationReport`\n- Keep `ClaudeCodeResult` for backward compatibility but deprecate it\n\n### Step 5: Update MockClaudeCodeRunner\nUpdate `MockClaudeCodeRunner` (lines 210-248) to support both old and new APIs for testing.\n\n## Files to Modify\n\n1. **`src/metaagent/claude_runner.py`** (primary changes):\n   - Add new dataclasses (TaskAnalysis, ImplementationRecommendation, ImplementationReport)\n   - Add `_extract_tasks_from_plan()` method\n   - Add `analyze_plan()` method\n   - Deprecate `implement()` method (keep for backward compatibility)\n   - Update `MockClaudeCodeRunner` with new mock methods\n\n2. **`src/metaagent/orchestrator.py`**:\n   - Update `ImplementationExecutor.execute()` to use new API\n   - Update `_run_claude_code()` method signature\n\n3. **`tests/test_claude_runner.py`**:\n   - Add tests for new dataclasses\n   - Add tests for `_extract_tasks_from_plan()`\n   - Add tests for `analyze_plan()`\n   - Keep existing tests for backward compatibility\n\n## Backward Compatibility\n\n- Keep `ClaudeCodeResult` and `implement()` method but mark as deprecated\n- Add deprecation warnings when old API is used\n- Document migration path in docstrings",
        "testStrategy": "## Unit Tests (`tests/test_claude_runner.py`)\n\n### 1. Test New Data Classes\n```python\nclass TestTaskAnalysis:\n    def test_create_task_analysis(self):\n        \"\"\"Test TaskAnalysis dataclass creation with all fields.\"\"\"\n        \n    def test_task_priority_enum(self):\n        \"\"\"Test TaskPriority enum values map correctly.\"\"\"\n        \nclass TestImplementationReport:\n    def test_create_empty_report(self):\n        \"\"\"Test creating an empty ImplementationReport.\"\"\"\n        \n    def test_report_with_tasks(self):\n        \"\"\"Test report correctly aggregates task counts by priority.\"\"\"\n```\n\n### 2. Test `_extract_tasks_from_plan()`\n```python\nclass TestExtractTasksFromPlan:\n    def test_extract_from_valid_plan(self):\n        \"\"\"Test extraction from a properly formatted plan (matches PlanWriter output).\"\"\"\n        \n    def test_extract_handles_empty_plan(self):\n        \"\"\"Test graceful handling of empty plan content.\"\"\"\n        \n    def test_extract_handles_malformed_plan(self):\n        \"\"\"Test partial extraction from malformed markdown.\"\"\"\n        \n    def test_extracts_all_priorities(self):\n        \"\"\"Test tasks are correctly categorized by priority level.\"\"\"\n        \n    def test_extracts_file_references(self):\n        \"\"\"Test file paths in backticks are extracted correctly.\"\"\"\n```\n\n### 3. Test `analyze_plan()`\n```python\nclass TestAnalyzePlan:\n    def test_analyze_with_valid_plan_file(self, tmp_path):\n        \"\"\"Test analysis returns structured report from valid plan.\"\"\"\n        \n    def test_analyze_missing_plan_file(self, tmp_path):\n        \"\"\"Test error handling when plan file doesn't exist.\"\"\"\n        \n    def test_analyze_returns_recommendations(self, tmp_path):\n        \"\"\"Test that recommendations are generated for each task.\"\"\"\n        \n    def test_by_priority_counts_correct(self, tmp_path):\n        \"\"\"Test priority counts in report match extracted tasks.\"\"\"\n```\n\n### 4. Test Mock Implementation\n```python\nclass TestMockClaudeCodeRunnerNew:\n    def test_mock_analyze_plan(self, tmp_path):\n        \"\"\"Test mock returns valid ImplementationReport.\"\"\"\n        \n    def test_mock_tracks_analyze_calls(self, tmp_path):\n        \"\"\"Test mock tracks call count for new API.\"\"\"\n```\n\n### 5. Backward Compatibility Tests\n```python\nclass TestBackwardCompatibility:\n    def test_implement_still_works(self, tmp_path):\n        \"\"\"Test old implement() method still functions (with deprecation warning).\"\"\"\n        \n    def test_claude_code_result_unchanged(self):\n        \"\"\"Test ClaudeCodeResult dataclass is unchanged.\"\"\"\n```\n\n## Integration Tests\n\n### 1. Test with Real Plan Files\n```python\nclass TestPlanIntegration:\n    def test_analyze_plan_written_by_plan_writer(self, tmp_path):\n        \"\"\"Test ClaudeCodeRunner can parse plans created by PlanWriter.\"\"\"\n        # Create a StageResult with real tasks\n        # Use PlanWriter to generate a plan\n        # Use ClaudeCodeRunner.analyze_plan() to parse it\n        # Verify tasks round-trip correctly\n```\n\n### 2. Test Orchestrator Integration\n```python\nclass TestOrchestratorIntegration:\n    def test_implementation_executor_uses_new_api(self):\n        \"\"\"Test ImplementationExecutor.execute() works with analyze_plan().\"\"\"\n        \n    def test_refine_with_auto_implement_uses_new_api(self):\n        \"\"\"Test full refine pipeline uses structured reports.\"\"\"\n```\n\n## Manual Verification Checklist\n\n- [ ] Run `pytest tests/test_claude_runner.py -v` - all tests pass\n- [ ] Run `metaagent refine --profile automation_agent --mock` - completes without subprocess spawning\n- [ ] Verify no `subprocess.run(['claude'` calls remain in production code paths\n- [ ] Check deprecation warnings appear when using old API\n- [ ] Verify structured report contains all task data from improvement plan",
        "status": "done",
        "dependencies": [
          "7",
          "6",
          "38"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-14T12:39:46.326Z"
      },
      {
        "id": "40",
        "title": "Modify Orchestrator refine() Methods to Return RefinementResult with implementation_report Field",
        "description": "Update the `RefinementResult` dataclass and all refine methods (`refine()`, `refine_iterative()`, `refine_with_stage_triage()`) in orchestrator.py to include an `implementation_report` field containing structured `TaskAnalysis` and `ImplementationRecommendation` data, enabling direct consumption by the current Claude Code session instead of subprocess spawning.",
        "details": "## Overview\n\nThis task modifies the orchestrator's refinement methods to return structured implementation reports that can be consumed by the CURRENT Claude Code session, completing the architectural shift away from subprocess spawning established in Task 39.\n\n## Implementation Steps\n\n### 1. Update RefinementResult Dataclass (`src/metaagent/orchestrator.py:123-150`)\n\nAdd a new `implementation_report` field to the existing `RefinementResult` dataclass:\n\n```python\nfrom .claude_runner import (\n    ClaudeCodeRunner,\n    ClaudeCodeResult,\n    ImplementationReport,  # Already imported\n    MockClaudeCodeRunner,\n    TaskAnalysis,\n    ImplementationRecommendation,  # Add this import\n)\n\n@dataclass\nclass RefinementResult:\n    \"\"\"Result from a complete refinement run.\"\"\"\n    \n    success: bool\n    profile_name: str\n    stages_completed: int\n    stages_failed: int\n    plan_path: Optional[Path] = None\n    error: Optional[str] = None\n    stage_results: list[StageResult] = field(default_factory=list)\n    iterations: list[IterationResult] = field(default_factory=list)\n    planned_calls: list[PlannedCall] = field(default_factory=list)\n    partial_success: bool = False\n    # NEW FIELD: Structured report for direct Claude Code consumption\n    implementation_report: Optional[ImplementationReport] = None\n    \n    @property\n    def status(self) -> str:\n        \"\"\"Get human-readable status.\"\"\"\n        if self.success:\n            return \"success\"\n        elif self.partial_success:\n            return \"partial_success\"\n        else:\n            return \"failure\"\n    \n    def to_markdown(self) -> str:\n        \"\"\"Generate markdown summary including implementation report.\"\"\"\n        lines = [\n            f\"# Refinement Result: {self.status.upper()}\",\n            \"\",\n            f\"**Profile:** {self.profile_name}\",\n            f\"**Stages Completed:** {self.stages_completed}\",\n            f\"**Stages Failed:** {self.stages_failed}\",\n        ]\n        \n        if self.plan_path:\n            lines.append(f\"**Plan Path:** {self.plan_path}\")\n        \n        if self.error:\n            lines.append(f\"\\n**Error:** {self.error}\")\n        \n        if self.implementation_report:\n            lines.append(\"\\n---\\n\")\n            lines.append(self.implementation_report.to_markdown())\n        \n        return \"\\n\".join(lines)\n```\n\n### 2. Modify `refine()` Method (`src/metaagent/orchestrator.py:1241-1326`)\n\nUpdate the `refine()` method to generate and include `implementation_report`:\n\n```python\ndef refine(self, profile_id: str) -> RefinementResult:\n    \"\"\"Run the refinement pipeline for a profile.\"\"\"\n    logger.info(f\"Starting refinement with profile: {profile_id}\")\n    # ... existing code for loading PRD, profile, packing codebase ...\n\n    # Run all stages using the StageRunner\n    history = RunHistory()\n    stage_results, stages_completed, stages_failed = self.stage_runner.run_stages(\n        prompts=prompts,\n        prd_content=prd_content,\n        code_context=code_context,\n        history=history,\n    )\n\n    # Write plan to TARGET repo\n    plan_path = None\n    if stage_results:\n        logger.info(\"Writing improvement plan...\")\n        plan_path = self.plan_writer.write_plan(\n            prd_content=prd_content,\n            profile_name=profile.name,\n            stage_results=stage_results,\n        )\n        logger.info(f\"Plan written to: {plan_path}\")\n\n    # Generate structured implementation report for Claude Code consumption\n    implementation_report = None\n    if stage_results:\n        implementation_report = self.implementation_executor.analyze_and_report(stage_results)\n        logger.info(f\"Generated implementation report with {len(implementation_report.tasks)} tasks\")\n\n    # Execute implementation if auto_implement is enabled\n    if self.config.auto_implement and stage_results:\n        logger.info(\"Auto-implementing changes with Claude Code...\")\n        changes_made = self.implementation_executor.execute(stage_results)\n        if changes_made:\n            logger.info(\"Implementation completed successfully\")\n        else:\n            logger.warning(\"No changes were made during implementation\")\n\n    return RefinementResult(\n        success=stages_failed == 0 and stages_completed > 0,\n        profile_name=profile.name,\n        stages_completed=stages_completed,\n        stages_failed=stages_failed,\n        plan_path=plan_path,\n        stage_results=stage_results,\n        partial_success=stages_failed > 0 and stages_completed > 0,\n        implementation_report=implementation_report,  # NEW\n    )\n```\n\n### 3. Modify `refine_iterative()` Method (`src/metaagent/orchestrator.py:1392-1524`)\n\nUpdate to aggregate implementation reports across iterations:\n\n```python\ndef refine_iterative(self, max_iterations: int = 10) -> RefinementResult:\n    \"\"\"Run the iterative refinement loop with AI-driven triage.\"\"\"\n    logger.info(\"Starting iterative refinement loop\")\n    \n    # ... existing initialization code ...\n    \n    all_tasks: list[TaskAnalysis] = []  # Aggregate tasks across iterations\n    \n    for iteration in range(1, max_iterations + 1):\n        # ... existing iteration logic ...\n        \n        # After running stages, generate iteration report\n        if iteration_results:\n            iter_report = self.implementation_executor.analyze_and_report(iteration_results)\n            all_tasks.extend(iter_report.tasks)\n        \n        # ... rest of existing iteration logic ...\n\n    # Generate final aggregated implementation report\n    implementation_report = None\n    if all_stage_results:\n        implementation_report = self.implementation_executor.analyze_and_report(all_stage_results)\n        logger.info(f\"Final report: {len(implementation_report.tasks)} total tasks\")\n\n    # Write final plan\n    plan_path = None\n    if all_stage_results:\n        logger.info(\"Writing final improvement plan...\")\n        plan_path = self.plan_writer.write_plan(\n            prd_content=prd_content,\n            profile_name=\"Iterative Refinement\",\n            stage_results=all_stage_results,\n        )\n\n    return RefinementResult(\n        success=stages_failed == 0 and stages_completed > 0,\n        profile_name=\"iterative\",\n        stages_completed=stages_completed,\n        stages_failed=stages_failed,\n        plan_path=plan_path,\n        stage_results=all_stage_results,\n        iterations=iterations,\n        partial_success=stages_failed > 0 and stages_completed > 0,\n        implementation_report=implementation_report,  # NEW\n    )\n```\n\n### 4. Modify `refine_with_stage_triage()` Method (`src/metaagent/orchestrator.py:1526-1645`)\n\nUpdate to include implementation report:\n\n```python\ndef refine_with_stage_triage(self, stages: list[str]) -> RefinementResult:\n    \"\"\"Run refinement with AI-driven prompt selection per stage.\"\"\"\n    logger.info(f\"Starting stage-aware refinement with stages: {stages}\")\n    \n    # ... existing stage processing logic ...\n    \n    # Generate structured implementation report\n    implementation_report = None\n    if all_stage_results:\n        implementation_report = self.implementation_executor.analyze_and_report(all_stage_results)\n        logger.info(f\"Generated report with {len(implementation_report.tasks)} tasks\")\n\n    # Write plan\n    plan_path = None\n    if all_stage_results:\n        logger.info(\"Writing improvement plan...\")\n        plan_path = self.plan_writer.write_plan(\n            prd_content=prd_content,\n            profile_name=f\"Stage Triage ({', '.join(stages)})\",\n            stage_results=all_stage_results,\n        )\n\n    return RefinementResult(\n        success=stages_failed == 0 and stages_completed > 0,\n        profile_name=\"stage_triage\",\n        stages_completed=stages_completed,\n        stages_failed=stages_failed,\n        plan_path=plan_path,\n        stage_results=all_stage_results,\n        partial_success=stages_failed > 0 and stages_completed > 0,\n        implementation_report=implementation_report,  # NEW\n    )\n```\n\n### 5. Update Import Statement (`src/metaagent/orchestrator.py:31-37`)\n\nEnsure `ImplementationRecommendation` is imported:\n\n```python\nfrom .claude_runner import (\n    ClaudeCodeRunner,\n    ClaudeCodeResult,\n    ImplementationReport,\n    ImplementationRecommendation,  # Add this\n    MockClaudeCodeRunner,\n    TaskAnalysis,\n)\n```\n\n### 6. Add `to_markdown()` Method to RefinementResult\n\nAdd a convenience method for generating markdown output that includes the implementation report (shown in step 1 above).\n\n## Key Architectural Points\n\n1. **No subprocess spawning**: The `implementation_report` contains all data needed for the CURRENT Claude Code session to implement changes\n2. **Backward compatibility**: Existing fields remain unchanged; `implementation_report` is optional (defaults to `None`)\n3. **Aggregation in iterative mode**: Tasks are collected across all iterations into a single final report\n4. **Consistent pattern**: All three refine methods follow the same pattern of calling `analyze_and_report()` after stage execution\n\n## Files Modified\n\n- `src/metaagent/orchestrator.py` - Main changes to `RefinementResult` dataclass and all three refine methods",
        "testStrategy": "## Unit Tests (`tests/test_orchestrator.py`)\n\n### 1. Test RefinementResult Has implementation_report Field\n```python\ndef test_refinement_result_has_implementation_report_field():\n    \"\"\"Test that RefinementResult includes implementation_report field.\"\"\"\n    from metaagent.orchestrator import RefinementResult\n    from metaagent.claude_runner import ImplementationReport, TaskAnalysis\n    \n    report = ImplementationReport(\n        tasks=[TaskAnalysis(task_id=\"1\", title=\"Test\", description=\"Desc\")],\n        success=True,\n    )\n    \n    result = RefinementResult(\n        success=True,\n        profile_name=\"test\",\n        stages_completed=1,\n        stages_failed=0,\n        implementation_report=report,\n    )\n    \n    assert result.implementation_report is not None\n    assert len(result.implementation_report.tasks) == 1\n```\n\n### 2. Test refine() Returns implementation_report\n```python\ndef test_refine_returns_implementation_report(tmp_path: Path):\n    \"\"\"Test that refine() populates implementation_report field.\"\"\"\n    from metaagent.orchestrator import Orchestrator\n    from metaagent.config import Config\n    \n    # Setup test config with mock mode\n    config = Config(\n        repo_path=tmp_path,\n        mock_mode=True,\n    )\n    # Create test PRD file\n    prd_file = tmp_path / \"prd.txt\"\n    prd_file.write_text(\"Test PRD content\")\n    config.prd_path = prd_file\n    \n    orchestrator = Orchestrator(config)\n    result = orchestrator.refine(\"default\")\n    \n    # Verify implementation_report is populated when stages complete\n    if result.stages_completed > 0:\n        assert result.implementation_report is not None\n        assert isinstance(result.implementation_report.tasks, list)\n```\n\n### 3. Test refine_iterative() Returns implementation_report\n```python\ndef test_refine_iterative_returns_implementation_report(tmp_path: Path):\n    \"\"\"Test that refine_iterative() populates implementation_report.\"\"\"\n    config = Config(repo_path=tmp_path, mock_mode=True)\n    prd_file = tmp_path / \"prd.txt\"\n    prd_file.write_text(\"Test PRD\")\n    config.prd_path = prd_file\n    \n    orchestrator = Orchestrator(config)\n    result = orchestrator.refine_iterative(max_iterations=2)\n    \n    if result.stages_completed > 0:\n        assert result.implementation_report is not None\n```\n\n### 4. Test refine_with_stage_triage() Returns implementation_report\n```python\ndef test_refine_with_stage_triage_returns_implementation_report(tmp_path: Path):\n    \"\"\"Test that refine_with_stage_triage() populates implementation_report.\"\"\"\n    config = Config(repo_path=tmp_path, mock_mode=True)\n    prd_file = tmp_path / \"prd.txt\"\n    prd_file.write_text(\"Test PRD\")\n    config.prd_path = prd_file\n    \n    orchestrator = Orchestrator(config)\n    result = orchestrator.refine_with_stage_triage(stages=[\"architecture\"])\n    \n    if result.stages_completed > 0:\n        assert result.implementation_report is not None\n```\n\n### 5. Test to_markdown() Includes Implementation Report\n```python\ndef test_refinement_result_to_markdown():\n    \"\"\"Test that to_markdown() includes implementation report content.\"\"\"\n    from metaagent.orchestrator import RefinementResult\n    from metaagent.claude_runner import ImplementationReport, TaskAnalysis\n    \n    report = ImplementationReport(\n        tasks=[TaskAnalysis(task_id=\"1\", title=\"Fix bug\", description=\"Fix the bug\", priority=\"high\")],\n        implementation_plan=\"## Plan\\n1. Fix bug\",\n        success=True,\n    )\n    \n    result = RefinementResult(\n        success=True,\n        profile_name=\"test\",\n        stages_completed=1,\n        stages_failed=0,\n        implementation_report=report,\n    )\n    \n    markdown = result.to_markdown()\n    \n    assert \"Fix bug\" in markdown\n    assert \"Implementation Report\" in markdown or \"high\" in markdown.lower()\n```\n\n### 6. Test implementation_report Is None When No Stages Complete\n```python\ndef test_implementation_report_none_when_no_stages():\n    \"\"\"Test implementation_report is None when no stages completed.\"\"\"\n    result = RefinementResult(\n        success=False,\n        profile_name=\"test\",\n        stages_completed=0,\n        stages_failed=0,\n        error=\"PRD not found\",\n    )\n    \n    assert result.implementation_report is None\n```\n\n## Integration Tests\n\n### 7. CLI Integration Test\n```bash\n# Test that CLI can access implementation report\nmeta-agent refine --profile default --mock --verbose 2>&1 | grep -i \"implementation\"\n```\n\n### 8. End-to-End Mock Test\n```python\ndef test_end_to_end_with_implementation_report(tmp_path: Path):\n    \"\"\"Full E2E test verifying implementation_report flow.\"\"\"\n    # Setup complete test environment\n    config = Config(\n        repo_path=tmp_path,\n        mock_mode=True,\n        config_dir=tmp_path / \"config\",\n    )\n    \n    # Create minimal required files\n    (tmp_path / \"prd.txt\").write_text(\"Build a CLI tool\")\n    (tmp_path / \"src\").mkdir()\n    (tmp_path / \"src\" / \"main.py\").write_text(\"print('hello')\")\n    \n    orchestrator = Orchestrator(config)\n    result = orchestrator.refine(\"automation_agent\")\n    \n    # Verify complete flow\n    assert result.success or result.partial_success\n    if result.stages_completed > 0:\n        assert result.implementation_report is not None\n        assert result.implementation_report.success\n        # Report should have tasks from mock analysis\n        assert len(result.implementation_report.tasks) >= 0\n```\n\n## Run Tests\n```bash\npytest tests/test_orchestrator.py -v -k \"implementation_report\"\npytest tests/test_orchestrator.py::test_refine_returns_implementation_report -v\npytest tests/test_orchestrator.py::test_refinement_result_to_markdown -v\n```",
        "status": "done",
        "dependencies": [
          "7",
          "39"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-14T12:57:49.422Z"
      },
      {
        "id": "41",
        "title": "Add CodeImplementationStrategy Abstraction with ClaudeCodeStrategy and CurrentSessionStrategy",
        "description": "Create a Strategy pattern abstraction (CodeImplementationStrategy) with two concrete implementations: ClaudeCodeStrategy (wraps existing subprocess logic for backwards compatibility) and CurrentSessionStrategy (returns structured ImplementationReport for the current Claude session to implement manually).",
        "details": "## Overview\n\nThis task introduces the Strategy pattern to enable the meta-agent to operate in two distinct modes:\n1. **Auto-implement mode** (ClaudeCodeStrategy): Spawns Claude Code subprocess to implement changes automatically\n2. **Report mode** (CurrentSessionStrategy): Returns structured ImplementationReport for the current Claude Code session to implement\n\n## Implementation Steps\n\n### 1. Define Abstract Strategy Protocol (`src/metaagent/strategies.py`)\n\nCreate a new module with the abstract strategy protocol:\n\n```python\n\"\"\"Strategy pattern for code implementation modes.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Protocol, Optional\n\nfrom .claude_runner import ImplementationReport, ClaudeCodeResult\nfrom .plan_writer import StageResult\n\n\nclass CodeImplementationStrategy(Protocol):\n    \"\"\"Protocol for code implementation strategies.\n    \n    Defines the interface for different implementation modes:\n    - ClaudeCodeStrategy: Subprocess-based auto-implementation\n    - CurrentSessionStrategy: Returns reports for manual implementation\n    \"\"\"\n    \n    def execute(\n        self,\n        stage_results: list[StageResult],\n        repo_path: Path,\n        plan_file: Optional[Path] = None,\n    ) -> ImplementationResult:\n        \"\"\"Execute the implementation strategy.\n        \n        Args:\n            stage_results: Analysis results containing tasks to implement.\n            repo_path: Path to the target repository.\n            plan_file: Optional path to the improvement plan file.\n            \n        Returns:\n            ImplementationResult with outcome and optional report.\n        \"\"\"\n        ...\n\n\n@dataclass\nclass ImplementationResult:\n    \"\"\"Result from executing an implementation strategy.\"\"\"\n    \n    success: bool\n    mode: str  # \"subprocess\" or \"report\"\n    changes_made: bool = False\n    files_modified: list[str] = None\n    implementation_report: Optional[ImplementationReport] = None\n    error: Optional[str] = None\n    \n    def __post_init__(self):\n        if self.files_modified is None:\n            self.files_modified = []\n```\n\n### 2. Implement ClaudeCodeStrategy (`src/metaagent/strategies.py`)\n\nWrap existing subprocess logic from `ClaudeCodeRunner.implement()`:\n\n```python\nfrom .claude_runner import ClaudeCodeRunner, ClaudeCodeResult\n\n\nclass ClaudeCodeStrategy:\n    \"\"\"Strategy that spawns Claude Code subprocess for auto-implementation.\n    \n    This provides backwards compatibility with the existing subprocess-based\n    implementation workflow. Use when --auto-implement flag is set.\n    \"\"\"\n    \n    def __init__(\n        self,\n        claude_runner: ClaudeCodeRunner,\n        auto_commit: bool = True,\n    ):\n        \"\"\"Initialize with Claude Code runner.\n        \n        Args:\n            claude_runner: ClaudeCodeRunner instance for subprocess calls.\n            auto_commit: Whether to commit changes after implementation.\n        \"\"\"\n        self.claude_runner = claude_runner\n        self.auto_commit = auto_commit\n    \n    def execute(\n        self,\n        stage_results: list[StageResult],\n        repo_path: Path,\n        plan_file: Optional[Path] = None,\n    ) -> ImplementationResult:\n        \"\"\"Execute implementation via Claude Code subprocess.\n        \n        This delegates to ClaudeCodeRunner.implement() which spawns\n        `claude --print` subprocess with the implementation prompt.\n        \"\"\"\n        if not stage_results:\n            return ImplementationResult(\n                success=True,\n                mode=\"subprocess\",\n                changes_made=False,\n            )\n        \n        # Build implementation prompt from tasks\n        prompt = self._build_prompt(stage_results)\n        \n        # Run Claude Code subprocess\n        result = self.claude_runner.implement(\n            repo_path=repo_path,\n            prompt=prompt,\n            plan_file=plan_file,\n        )\n        \n        return ImplementationResult(\n            success=result.success,\n            mode=\"subprocess\",\n            changes_made=len(result.files_modified) > 0,\n            files_modified=result.files_modified,\n            error=result.error,\n        )\n    \n    def _build_prompt(self, stage_results: list[StageResult]) -> str:\n        \"\"\"Build implementation prompt from stage results.\"\"\"\n        tasks = []\n        for result in stage_results:\n            tasks.extend(result.tasks)\n        \n        prompt = \"Please implement the following improvements:\\n\\n\"\n        for i, task in enumerate(tasks, 1):\n            if isinstance(task, dict):\n                title = task.get(\"title\", \"\")\n                desc = task.get(\"description\", str(task))\n                prompt += f\"{i}. **{title}**: {desc}\\n\\n\"\n            else:\n                prompt += f\"{i}. {task}\\n\\n\"\n        \n        return prompt\n```\n\n### 3. Implement CurrentSessionStrategy (`src/metaagent/strategies.py`)\n\nCreate strategy that returns structured reports:\n\n```python\nclass CurrentSessionStrategy:\n    \"\"\"Strategy that returns structured reports for current session implementation.\n    \n    Instead of spawning a subprocess, this strategy returns an ImplementationReport\n    that the CURRENT Claude Code session (the one running this code) can use to\n    implement changes directly. This is the preferred mode for interactive use.\n    \"\"\"\n    \n    def __init__(self, write_task_file: bool = True):\n        \"\"\"Initialize the strategy.\n        \n        Args:\n            write_task_file: Whether to write tasks to .meta-agent-tasks.md\n        \"\"\"\n        self.write_task_file = write_task_file\n    \n    def execute(\n        self,\n        stage_results: list[StageResult],\n        repo_path: Path,\n        plan_file: Optional[Path] = None,\n    ) -> ImplementationResult:\n        \"\"\"Generate structured implementation report.\n        \n        Returns ImplementationReport with TaskAnalysis and\n        ImplementationRecommendation objects that can be consumed\n        by the current Claude Code session.\n        \"\"\"\n        if not stage_results:\n            report = ImplementationReport(\n                success=True,\n                tasks=[],\n                implementation_plan=\"No tasks to implement.\",\n            )\n            return ImplementationResult(\n                success=True,\n                mode=\"report\",\n                changes_made=False,\n                implementation_report=report,\n            )\n        \n        # Convert stage results to TaskAnalysis objects\n        tasks = self._extract_tasks(stage_results)\n        recommendations = self._generate_recommendations(tasks)\n        plan_text = self._create_plan(tasks)\n        effort = self._estimate_effort(tasks)\n        \n        # Optionally write task file\n        if self.write_task_file:\n            self._write_task_file(tasks, repo_path)\n        \n        report = ImplementationReport(\n            tasks=tasks,\n            recommendations=recommendations,\n            implementation_plan=plan_text,\n            success=True,\n            total_estimated_effort=effort,\n        )\n        \n        return ImplementationResult(\n            success=True,\n            mode=\"report\",\n            changes_made=False,  # No changes made yet - report only\n            implementation_report=report,\n        )\n    \n    def _extract_tasks(self, stage_results: list[StageResult]) -> list[TaskAnalysis]:\n        \"\"\"Convert stage results to TaskAnalysis objects.\"\"\"\n        # Implementation similar to ImplementationExecutor.analyze_and_report()\n        ...\n    \n    def _generate_recommendations(self, tasks: list[TaskAnalysis]) -> list[ImplementationRecommendation]:\n        \"\"\"Generate implementation recommendations from tasks.\"\"\"\n        ...\n    \n    def _create_plan(self, tasks: list[TaskAnalysis]) -> str:\n        \"\"\"Create markdown implementation plan.\"\"\"\n        ...\n    \n    def _estimate_effort(self, tasks: list[TaskAnalysis]) -> str:\n        \"\"\"Estimate total effort.\"\"\"\n        ...\n    \n    def _write_task_file(self, tasks: list[TaskAnalysis], repo_path: Path) -> None:\n        \"\"\"Write tasks to .meta-agent-tasks.md for reference.\"\"\"\n        ...\n```\n\n### 4. Create Strategy Factory (`src/metaagent/strategies.py`)\n\nAdd factory function for strategy selection:\n\n```python\ndef create_implementation_strategy(\n    mode: str,\n    claude_runner: Optional[ClaudeCodeRunner] = None,\n    auto_commit: bool = True,\n    write_task_file: bool = True,\n) -> CodeImplementationStrategy:\n    \"\"\"Factory function to create the appropriate implementation strategy.\n    \n    Args:\n        mode: Either \"subprocess\" (auto-implement) or \"report\" (current session).\n        claude_runner: Required for subprocess mode.\n        auto_commit: Whether to auto-commit (subprocess mode only).\n        write_task_file: Whether to write task file (report mode only).\n        \n    Returns:\n        Appropriate CodeImplementationStrategy implementation.\n        \n    Raises:\n        ValueError: If mode is invalid or dependencies missing.\n    \"\"\"\n    if mode == \"subprocess\":\n        if claude_runner is None:\n            raise ValueError(\"ClaudeCodeRunner required for subprocess mode\")\n        return ClaudeCodeStrategy(\n            claude_runner=claude_runner,\n            auto_commit=auto_commit,\n        )\n    elif mode == \"report\":\n        return CurrentSessionStrategy(\n            write_task_file=write_task_file,\n        )\n    else:\n        raise ValueError(f\"Unknown implementation mode: {mode}\")\n```\n\n### 5. Update Config (`src/metaagent/config.py`)\n\nAdd new configuration option at line ~34:\n\n```python\n@dataclass\nclass Config:\n    # ... existing fields ...\n    \n    # Implementation Strategy Settings\n    implementation_mode: str = \"report\"  # \"subprocess\" or \"report\"\n    # When \"subprocess\": spawns Claude Code process (existing behavior)\n    # When \"report\": returns ImplementationReport for current session\n```\n\nUpdate `from_env()` to read from environment:\n\n```python\nimplementation_mode=os.getenv(\"METAAGENT_IMPLEMENTATION_MODE\", \"report\"),\n```\n\n### 6. Update ImplementationExecutor (`src/metaagent/orchestrator.py:767-825`)\n\nRefactor to use strategy pattern:\n\n```python\nclass ImplementationExecutor:\n    \"\"\"Handles code implementation using configurable strategy.\"\"\"\n    \n    def __init__(\n        self,\n        config: Config,\n        claude_runner: ClaudeCodeRunner,\n        strategy: Optional[CodeImplementationStrategy] = None,\n    ):\n        self.config = config\n        self.claude_runner = claude_runner\n        \n        # Create strategy based on config if not provided\n        if strategy is None:\n            mode = \"subprocess\" if config.auto_implement else \"report\"\n            self.strategy = create_implementation_strategy(\n                mode=mode,\n                claude_runner=claude_runner,\n                auto_commit=config.auto_commit,\n            )\n        else:\n            self.strategy = strategy\n    \n    def execute(self, stage_results: list[StageResult]) -> ImplementationResult:\n        \"\"\"Execute implementation using configured strategy.\"\"\"\n        return self.strategy.execute(\n            stage_results=stage_results,\n            repo_path=self.config.repo_path,\n            plan_file=self.config.repo_path / \"docs\" / \"mvp_improvement_plan.md\",\n        )\n```\n\n### 7. Update CLI (`src/metaagent/cli.py`)\n\nAdd `--report-only` flag at ~line 141:\n\n```python\n@app.command()\ndef refine(\n    # ... existing options ...\n    report_only: bool = typer.Option(\n        False,\n        \"--report-only\",\n        help=\"Generate implementation report without auto-implementing (default behavior).\",\n    ),\n    auto_implement: bool = typer.Option(\n        False,\n        \"--auto-implement\",\n        \"-a\",\n        help=\"Spawn Claude Code subprocess to implement changes automatically.\",\n    ),\n):\n    # ... existing logic ...\n    \n    # Set implementation mode based on flags\n    if auto_implement:\n        config.implementation_mode = \"subprocess\"\n        config.auto_implement = True\n    else:\n        config.implementation_mode = \"report\"\n```\n\n### 8. Update Module Exports (`src/metaagent/__init__.py`)\n\nExport new strategy classes:\n\n```python\nfrom .strategies import (\n    CodeImplementationStrategy,\n    ClaudeCodeStrategy,\n    CurrentSessionStrategy,\n    ImplementationResult,\n    create_implementation_strategy,\n)\n```\n\n## Files to Create/Modify\n\n| File | Action | Description |\n|------|--------|-------------|\n| `src/metaagent/strategies.py` | Create | New module with Strategy pattern implementation |\n| `src/metaagent/config.py` | Modify | Add `implementation_mode` field |\n| `src/metaagent/orchestrator.py` | Modify | Update ImplementationExecutor to use strategy |\n| `src/metaagent/cli.py` | Modify | Add `--report-only` flag, update help text |\n| `src/metaagent/__init__.py` | Modify | Export new classes |\n| `tests/test_strategies.py` | Create | Unit tests for strategies |\n\n## Design Rationale\n\n1. **Strategy Pattern**: Clean separation of concerns between subprocess spawning and report generation\n2. **Protocol-based**: Uses Python Protocol for duck typing, allowing future strategy implementations\n3. **Backwards Compatible**: ClaudeCodeStrategy preserves existing subprocess behavior exactly\n4. **Factory Function**: Simplifies strategy creation and centralizes mode selection logic\n5. **Configuration-driven**: Mode can be set via CLI flags or environment variables",
        "testStrategy": "## Unit Tests (`tests/test_strategies.py`)\n\n### 1. Test CodeImplementationStrategy Protocol\n```python\ndef test_strategy_protocol_has_execute_method():\n    \"\"\"Verify protocol defines execute method signature.\"\"\"\n    from metaagent.strategies import CodeImplementationStrategy\n    import inspect\n    assert hasattr(CodeImplementationStrategy, 'execute')\n    sig = inspect.signature(CodeImplementationStrategy.execute)\n    params = list(sig.parameters.keys())\n    assert 'stage_results' in params\n    assert 'repo_path' in params\n```\n\n### 2. Test ClaudeCodeStrategy\n```python\ndef test_claude_code_strategy_calls_runner(mock_claude_runner, tmp_path):\n    \"\"\"Test ClaudeCodeStrategy delegates to ClaudeCodeRunner.implement().\"\"\"\n    from metaagent.strategies import ClaudeCodeStrategy\n    strategy = ClaudeCodeStrategy(claude_runner=mock_claude_runner)\n    \n    stage_results = [StageResult(stage_id=\"test\", tasks=[{\"title\": \"Test\"}])]\n    result = strategy.execute(stage_results, tmp_path)\n    \n    assert result.mode == \"subprocess\"\n    assert mock_claude_runner.implement.called\n\ndef test_claude_code_strategy_returns_files_modified(mock_claude_runner, tmp_path):\n    \"\"\"Test ClaudeCodeStrategy returns modified files list.\"\"\"\n    mock_claude_runner.implement.return_value = ClaudeCodeResult(\n        success=True,\n        files_modified=[\"src/foo.py\", \"src/bar.py\"],\n    )\n    strategy = ClaudeCodeStrategy(claude_runner=mock_claude_runner)\n    result = strategy.execute([StageResult(tasks=[{}])], tmp_path)\n    \n    assert result.files_modified == [\"src/foo.py\", \"src/bar.py\"]\n    assert result.changes_made is True\n```\n\n### 3. Test CurrentSessionStrategy\n```python\ndef test_current_session_strategy_returns_report(tmp_path):\n    \"\"\"Test CurrentSessionStrategy returns ImplementationReport.\"\"\"\n    from metaagent.strategies import CurrentSessionStrategy\n    strategy = CurrentSessionStrategy(write_task_file=False)\n    \n    stage_results = [StageResult(\n        stage_id=\"test\",\n        tasks=[{\"title\": \"Test Task\", \"description\": \"Do something\"}]\n    )]\n    result = strategy.execute(stage_results, tmp_path)\n    \n    assert result.mode == \"report\"\n    assert result.implementation_report is not None\n    assert len(result.implementation_report.tasks) == 1\n    assert result.implementation_report.tasks[0].title == \"Test Task\"\n\ndef test_current_session_strategy_writes_task_file(tmp_path):\n    \"\"\"Test CurrentSessionStrategy writes .meta-agent-tasks.md.\"\"\"\n    strategy = CurrentSessionStrategy(write_task_file=True)\n    stage_results = [StageResult(tasks=[{\"title\": \"Test\"}])]\n    \n    result = strategy.execute(stage_results, tmp_path)\n    \n    task_file = tmp_path / \".meta-agent-tasks.md\"\n    assert task_file.exists()\n    assert \"Test\" in task_file.read_text()\n\ndef test_current_session_strategy_empty_results():\n    \"\"\"Test CurrentSessionStrategy handles empty results.\"\"\"\n    strategy = CurrentSessionStrategy()\n    result = strategy.execute([], Path(\"/tmp\"))\n    \n    assert result.success is True\n    assert result.changes_made is False\n    assert result.implementation_report.tasks == []\n```\n\n### 4. Test Strategy Factory\n```python\ndef test_create_strategy_subprocess_mode(mock_claude_runner):\n    \"\"\"Test factory creates ClaudeCodeStrategy for subprocess mode.\"\"\"\n    from metaagent.strategies import create_implementation_strategy, ClaudeCodeStrategy\n    \n    strategy = create_implementation_strategy(\n        mode=\"subprocess\",\n        claude_runner=mock_claude_runner,\n    )\n    \n    assert isinstance(strategy, ClaudeCodeStrategy)\n\ndef test_create_strategy_report_mode():\n    \"\"\"Test factory creates CurrentSessionStrategy for report mode.\"\"\"\n    from metaagent.strategies import create_implementation_strategy, CurrentSessionStrategy\n    \n    strategy = create_implementation_strategy(mode=\"report\")\n    \n    assert isinstance(strategy, CurrentSessionStrategy)\n\ndef test_create_strategy_subprocess_requires_runner():\n    \"\"\"Test factory raises error if subprocess mode without runner.\"\"\"\n    from metaagent.strategies import create_implementation_strategy\n    \n    with pytest.raises(ValueError, match=\"ClaudeCodeRunner required\"):\n        create_implementation_strategy(mode=\"subprocess\", claude_runner=None)\n\ndef test_create_strategy_invalid_mode():\n    \"\"\"Test factory raises error for unknown mode.\"\"\"\n    from metaagent.strategies import create_implementation_strategy\n    \n    with pytest.raises(ValueError, match=\"Unknown implementation mode\"):\n        create_implementation_strategy(mode=\"invalid\")\n```\n\n### 5. Test ImplementationResult Dataclass\n```python\ndef test_implementation_result_defaults():\n    \"\"\"Test ImplementationResult has sensible defaults.\"\"\"\n    from metaagent.strategies import ImplementationResult\n    \n    result = ImplementationResult(success=True, mode=\"report\")\n    \n    assert result.files_modified == []\n    assert result.implementation_report is None\n    assert result.error is None\n    assert result.changes_made is False\n```\n\n### 6. Integration Test with ImplementationExecutor\n```python\ndef test_executor_uses_strategy(mock_config, mock_claude_runner):\n    \"\"\"Test ImplementationExecutor delegates to strategy.\"\"\"\n    from metaagent.orchestrator import ImplementationExecutor\n    from metaagent.strategies import CurrentSessionStrategy\n    \n    strategy = CurrentSessionStrategy(write_task_file=False)\n    executor = ImplementationExecutor(\n        config=mock_config,\n        claude_runner=mock_claude_runner,\n        strategy=strategy,\n    )\n    \n    result = executor.execute([StageResult(tasks=[{\"title\": \"Test\"}])])\n    \n    assert result.mode == \"report\"\n    assert result.implementation_report is not None\n```\n\n### 7. CLI Integration Tests\n```bash\n# Test report-only mode (default)\nmetaagent refine --profile automation_agent --mock --repo .\n\n# Verify no subprocess spawned, report returned\n# Check .meta-agent-tasks.md written\n\n# Test auto-implement mode\nmetaagent refine --profile automation_agent --auto-implement --mock --repo .\n\n# Verify subprocess logic invoked (mocked)\n```\n\n### 8. Run Tests\n```bash\n# Run all strategy tests\npytest tests/test_strategies.py -v\n\n# Run with coverage\npytest tests/test_strategies.py --cov=metaagent.strategies --cov-report=term-missing\n\n# Run integration tests\npytest tests/test_orchestrator.py::TestImplementationExecutor -v\n```\n\n## Acceptance Criteria\n\n1. Both modes produce valid ImplementationReport objects\n2. ClaudeCodeStrategy preserves exact existing subprocess behavior\n3. CurrentSessionStrategy returns report without subprocess calls\n4. Factory correctly creates strategies based on mode\n5. CLI `--auto-implement` triggers subprocess mode\n6. Default behavior (no flag) uses report mode\n7. All existing tests continue to pass",
        "status": "deferred",
        "dependencies": [
          "7",
          "39",
          "40"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-14T13:00:37.394Z"
      },
      {
        "id": "42",
        "title": "Implement CurrentSessionStrategy with Abstract CodeImplementationStrategy Base Class",
        "description": "Create an abstract CodeImplementationStrategy base class and implement CurrentSessionStrategy that returns structured ImplementationReport results for the current Claude session without spawning subprocesses, following the strategy pattern from Report 01.",
        "details": "**Overview**\n\nThis task implements the Strategy design pattern[1][2][5] to decouple code implementation logic from the orchestrator, enabling runtime selection between subprocess-based (ClaudeCodeRunner) and in-process (CurrentSessionStrategy) implementations. The abstract base class defines the contract, while CurrentSessionStrategy simulates Claude session execution using existing structured data classes from Task 39.\n\n**File Structure**\n\nCreate `src/metaagent/strategies.py`:\n\n```python\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\nimport logging\n\nfrom metaagent.claude_runner import ImplementationReport  # from Task 39\n\nlogger = logging.getLogger(__name__)\n\nclass CodeImplementationStrategy(ABC):\n    \"\"\"Abstract base class for code implementation strategies.\"\"\"\n    \n    @abstractmethod\n    def execute_implementation(\n        self, \n        task_plan: str, \n        workdir: str, \n        max_turns: int = 50\n    ) -> ImplementationReport:\n        \"\"\"Execute implementation strategy and return structured report.\n        \n        Args:\n            task_plan: Markdown task plan (.meta-agent-tasks.md content)\n            workdir: Working directory path\n            max_turns: Maximum interaction turns\n        \n        Returns:\n            ImplementationReport with structured results\n        \"\"\"\n        pass\n\nclass CurrentSessionStrategy(CodeImplementationStrategy):\n    \"\"\"In-process strategy that simulates current Claude session implementation.\"\"\"\n    \n    def __init__(self, session_context: Optional[str] = None):\n        self.session_context = session_context or \"Current development session\"\n    \n    def execute_implementation(\n        self, \n        task_plan: str, \n        workdir: str, \n        max_turns: int = 50\n    ) -> ImplementationReport:\n        \"\"\"Simulate Claude session implementation without subprocess spawning.\"\"\"\n        logger.info(f\"Executing CurrentSessionStrategy in {workdir}\")\n        logger.info(f\"Task plan length: {len(task_plan)} chars\")\n        \n        # Simulate structured implementation report (in real impl, would use current session state)\n        # For now, return success report with mock changes\n        mock_changes = [\n            {\n                'file': f'{workdir}/orchestrator.py',\n                'status': 'modified',\n                'summary': 'Integrated strategy pattern for implementation',\n                'diff_preview': '```diff\\n+ strategy: CodeImplementationStrategy\\n```'\n            }\n        ]\n        \n        return ImplementationReport(\n            success=True,\n            summary=\"Current session strategy executed successfully\",\n            changes_applied=mock_changes,\n            turns_used=3,\n            session_context=self.session_context\n        )\n```\n\n**Integration Steps**\n\n1. **Update Orchestrator** (`src/metaagent/orchestrator.py`):\n```python\nfrom metaagent.strategies import CodeImplementationStrategy, CurrentSessionStrategy\n\nclass MetaAgentOrchestrator:\n    def __init__(self, config):\n        self.config = config\n        self.implementation_strategy: Optional[CodeImplementationStrategy] = None\n    \n    def set_implementation_strategy(self, strategy: CodeImplementationStrategy):\n        \"\"\"Set runtime implementation strategy per Strategy pattern[1].\"\"\"\n        self.implementation_strategy = strategy\n    \n    def _implement_with_strategy(self, task_plan: str, workdir: str) -> bool:\n        if not self.implementation_strategy:\n            # Default to subprocess strategy (from Task 16)\n            from metaagent.claude_runner import ClaudeCodeRunner\n            self.implementation_strategy = ClaudeCodeRunner()\n        \n        report = self.implementation_strategy.execute_implementation(\n            task_plan=task_plan,\n            workdir=workdir,\n            max_turns=self.config.get('max_turns', 50)\n        )\n        \n        logger.info(f\"Implementation report: {report.summary}\")\n        return report.success\n```\n\n2. **Configuration Support** (`config/profiles.yaml` update):\n```yaml\nprofiles:\n  default:\n    implementation_strategy: \"current_session\"  # or \"subprocess\"\n```\n\n3. **Factory Pattern for Strategy Selection**:\n```python\n# In orchestrator.py\ndef create_strategy(strategy_name: str, **kwargs) -> CodeImplementationStrategy:\n    strategies = {\n        'current_session': CurrentSessionStrategy,\n        'subprocess': ClaudeCodeRunner,  # from Task 16/39\n    }\n    strategy_cls = strategies.get(strategy_name)\n    if not strategy_cls:\n        raise ValueError(f\"Unknown strategy: {strategy_name}\")\n    return strategy_cls(**kwargs)\n```\n\n**Best Practices Applied**:\n- Follows Strategy pattern exactly per Auth0[1] and Refactoring.Guru[5]: abstract interface + concrete implementations + runtime selection\n- Uses `abc.ABC` for proper abstract base class enforcement\n- Type hints throughout for static analysis\n- Structured return types using existing `ImplementationReport` dataclass\n- Logging for observability\n- Configuration-driven strategy selection\n\n**Considerations**:\n- Backward compatible with existing ClaudeCodeRunner (Task 16/39)\n- Mock implementation returns realistic `ImplementationReport` structure\n- Extensible for future strategies (e.g., remote API, parallel execution)\n- No external dependencies beyond existing codebase",
        "testStrategy": "**Comprehensive Test Suite** (`tests/test_strategies.py`)\n\n**1. Unit Tests for Abstract Base Class**:\n```python\ndef test_abstract_base_class_cannot_instantiate():\n    with pytest.raises(TypeError):\n        CodeImplementationStrategy()  # Should fail\n```\n\n**2. Unit Tests for CurrentSessionStrategy**:\n```python\ndef test_current_session_strategy_execution():\n    strategy = CurrentSessionStrategy(\"test-session\")\n    report = strategy.execute_implementation(\n        task_plan=\"# Test Task\",\n        workdir=\"/tmp/test\",\n        max_turns=10\n    )\n    assert report.success is True\n    assert \"Current session strategy\" in report.summary\n    assert len(report.changes_applied) == 1\n    assert report.turns_used == 3\n    assert report.session_context == \"test-session\"\n\n\ndef test_current_session_strategy_defaults():\n    strategy = CurrentSessionStrategy()\n    assert strategy.session_context == \"Current development session\"\n```\n\n**3. Integration Tests for Orchestrator Integration**:\n```python\ndef test_orchestrator_strategy_switching(temp_dir):\n    orchestrator = MetaAgentOrchestrator(config={})\n    \n    # Test CurrentSessionStrategy\n    orchestrator.set_implementation_strategy(CurrentSessionStrategy())\n    success = orchestrator._implement_with_strategy(\"# test\", str(temp_dir))\n    assert success is True\n    \n    # Test default fallback (requires ClaudeCodeRunner mock)\n    orchestrator.implementation_strategy = None\n    with patch('metaagent.orchestrator.ClaudeCodeRunner'):\n        success = orchestrator._implement_with_strategy(\"# test\", str(temp_dir))\n        assert success is True\n```\n\n**4. Strategy Factory Tests**:\n```python\ndef test_strategy_factory():\n    strategy = create_strategy('current_session')\n    assert isinstance(strategy, CurrentSessionStrategy)\n    \n    with pytest.raises(ValueError):\n        create_strategy('invalid')\n```\n\n**Execution & Coverage**:\n```bash\n# Run strategy tests\npytest tests/test_strategies.py -v\n\n# Coverage\npytest tests/test_strategies.py --cov=src/metaagent/strategies --cov-report=term-missing\n```\n\n**Expected Coverage**: >95% for `strategies.py`\n\n**Manual Verification**:\n1. Run orchestrator with `implementation_strategy: current_session` in profile\n2. Verify no subprocess spawning occurs (check process list)\n3. Confirm structured `ImplementationReport` is returned and logged\n4. Test strategy switching at runtime via `set_implementation_strategy()`",
        "status": "done",
        "dependencies": [
          "39",
          "16"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-15T01:00:04.071Z"
      },
      {
        "id": "43",
        "title": "Update CLI refine Command to Stream ImplementationReport to Current Session",
        "description": "Extend the meta-agent CLI refine command so that when running metaagent refine, the structured ImplementationReport returned by the orchestrator is rendered to stdout in a machine-readable, JSON-based format that the current Claude Code session can immediately consume and act on, completing Phase 1 of the Report 01 coupling/cohesion recommendations.",
        "details": "## Goal\nWire the orchestrator’s new `implementation_report` output (from `RefinementResult`) into the CLI layer so that `metaagent refine` prints a structured, contract-compliant representation of the report to stdout for direct consumption by the current Claude Code session.\n\n## High-Level Design\n- **Source of truth**: Use the `implementation_report` field introduced in Task 40’s `RefinementResult` as the single source of structured implementation data.\n- **Output contract**: Align the CLI output with the JSON response contract and schema machinery introduced in Tasks 19, 22, and documented in Task 32, while allowing for an \"implementation-report\"-specific envelope if needed.\n- **Session visibility**: Ensure output is written to stdout (not files or subprocesses), with a clearly delimited, parseable JSON block so the surrounding Claude Code session can reliably detect and parse it.\n\n## Implementation Steps\n1. **Review Existing Types and Results**\n   - Inspect `RefinementResult` in `src/metaagent/orchestrator.py` (Task 40) to confirm the structure and type of `implementation_report` (e.g., whether it is a dataclass, Pydantic model, or plain dict/list structure of `TaskAnalysis` and `ImplementationRecommendation`).\n   - Identify the refine entrypoints used by the CLI (e.g., `refine()`, `refine_iterative()`, `refine_with_stage_triage()`) and which are bound to `metaagent refine` vs `metaagent refine-loop`.\n\n2. **Define CLI Output Shape for ImplementationReport**\n   - Design a **minimal wrapper** JSON structure that cleanly exposes the `implementation_report` while remaining consistent with the documented JSON response contract:\n     ```json\n     {\n       \"type\": \"implementation_report\",\n       \"version\": 1,\n       \"summary\": \"...optional summary of this refinement run...\",\n       \"report\": {\n         \"tasks\": [\n           {\n             \"title\": \"...\",\n             \"description\": \"...\",\n             \"priority\": \"high|medium|low\",\n             \"files\": [\"path1.py\", \"path2.py\"],\n             \"recommendations\": [\"...\"],\n             \"analysis\": {\n               \"coupling\": \"...\",\n               \"cohesion\": \"...\",\n               \"rationale\": \"...\"\n             }\n           }\n         ]\n       }\n     }\n     ```\n   - Keep the structure close to the existing analysis JSON schema (Task 19/22) so downstream tooling can reuse parsing logic and types where reasonable.\n   - Ensure the chosen field names are stable and documented in a small comment block in `cli.py` for maintainability.\n\n3. **Update CLI refine Command to Emit ImplementationReport**\n   - In `src/metaagent/cli.py`, locate the `refine` command implementation created/refined in Tasks 13 and 18.\n   - After invoking the orchestrator (e.g., `result = orchestrator.refine(...)` or similar), access `result.implementation_report`.\n   - Implement a helper function in `cli.py` or a small utility module, e.g., `format_implementation_report_for_cli(result: RefinementResult) -> dict`, that:\n     - Validates that `implementation_report` is present; if missing or empty, prints a clear warning to stderr and exits with non-zero or falls back to legacy behavior, depending on project conventions.\n     - Converts dataclass / Pydantic models to plain Python dictionaries using `.dict()` / `asdict()` / `jsonable_encoder` patterns.\n     - Wraps the raw report data into the agreed JSON envelope (`{\"type\": \"implementation_report\", ...}`) while preserving all task details.\n   - Serialize to JSON using `json.dumps(..., ensure_ascii=False, indent=None)` and print to **stdout**.\n   - Guard the output with optional markers if the project already uses them for machine parsing, for example:\n     ```python\n     print(\"===METAAGENT_IMPLEMENTATION_REPORT_START===\")\n     print(json.dumps(payload, ensure_ascii=False))\n     print(\"===METAAGENT_IMPLEMENTATION_REPORT_END===\")\n     ```\n     but only if consistent with existing conventions; otherwise, rely on a single well-formed JSON object.\n\n4. **Honor JSON Response Contract and Robust Parsing Practices**\n   - Reuse the JSON schema patterns and validation utilities from `prompts.py` (Task 19) and `analysis.py` (Task 22) where appropriate:\n     - Either define a dedicated `IMPLEMENTATION_REPORT_SCHEMA` constant, or\n     - Validate that the generated `payload` conforms to an internal structure (lightweight validation) before printing, logging any validation errors to stderr.\n   - Follow best practices from the analysis parsing layer (Task 22):\n     - Avoid mixing human-readable text and JSON on the same line.\n     - If additional human-facing logs are required, send them to stderr (`click.echo(..., err=True)` or `print(..., file=sys.stderr)`), keeping stdout strictly for machine-readable JSON.\n\n5. **Configuration and Backwards Compatibility**\n   - Add a `--output-mode` or `--report-format` option to `metaagent refine` only if needed to maintain backward compatibility (e.g., `human`, `json`, defaulting to `json` for the new architecture):\n     - In `human` mode, preserve current logging behavior and optionally show a summarized view of the implementation report.\n     - In `json` mode, suppress non-essential stdout logs and emit only the JSON payload.\n   - Clearly document the new behavior in CLI help strings (short description and example usage) and ensure it aligns with the JSON contract documentation added in Task 32.\n\n6. **Logging, Errors, and Exit Codes**\n   - If `implementation_report` is `None` or empty:\n     - Log a structured error to stderr.\n     - Exit with a non-zero exit code to signal to the calling session that no actionable tasks were produced.\n   - If JSON serialization fails (e.g., due to non-serializable objects), catch `TypeError` and:\n     - Apply a generic encoder (e.g., convert enums to `value`, datetimes to ISO strings) or\n     - Fall back to a best-effort minimal JSON containing an error description.\n   - Ensure that unhandled exceptions in the refine path still produce a useful message to stderr and non-zero exit code, without printing malformed JSON.\n\n7. **Documentation Touchpoint (Optional/Small)**\n   - Add a brief note to either the existing README JSON contract section or CLI help describing that `metaagent refine` now emits an `implementation_report` envelope for Claude Code to consume, referencing the `type: \"implementation_report\"` field.\n\n## Implementation Best Practices\n- Keep CLI responsibilities slim: orchestrator computes the report; CLI only formats and outputs it.\n- Avoid tight coupling between CLI and internal `TaskAnalysis`/`ImplementationRecommendation` types by converting to plain dicts at the boundary.\n- Ensure tests do not depend on exact key ordering in JSON; compare parsed dicts instead of raw strings.\n- Consider future extensibility for additional report types (e.g., `\"type\": \"diagnostic_report\"`) by centralizing envelope construction in a helper function.",
        "testStrategy": "1. **Unit Tests for CLI Formatting** (e.g., `tests/test_cli_refine_output.py`)\n   - Add tests using `click.testing.CliRunner` (or the project’s existing CLI test utilities) to invoke `metaagent refine` with a mocked orchestrator:\n     - Patch the orchestrator’s `refine()` to return a `RefinementResult` instance with a non-empty `implementation_report` containing one or more task entries.\n     - Assert that `result.exit_code == 0`.\n     - Parse `result.stdout` as JSON and assert:\n       - `payload[\"type\"] == \"implementation_report\"`.\n       - `payload[\"version\"] == 1`.\n       - `\"report\" in payload` and `\"tasks\" in payload[\"report\"]`.\n       - At least one task entry has expected keys (e.g., `title`, `description`, `priority`, `files`).\n   - If envelope markers are used, assert that:\n     - The markers appear exactly once.\n     - The JSON content between markers parses correctly.\n\n2. **Unit Tests for Missing or Empty implementation_report**\n   - Mock `RefinementResult` with `implementation_report=None` or an empty structure.\n   - Run `metaagent refine` and assert:\n     - Non-zero exit code.\n     - No JSON object printed to stdout (stdout either empty or contains only a minimal error JSON if that pattern is chosen).\n     - A clear error message appears on stderr.\n\n3. **Serialization Robustness Tests**\n   - Construct `implementation_report` test objects containing enums or simple custom dataclasses.\n   - Verify that the CLI either:\n     - Successfully serializes them (e.g., via custom encoder or `asdict()`), or\n     - Emits a structured error JSON that is still parseable and includes an `error` field.\n\n4. **Output Mode / Backwards Compatibility Tests** (if `--output-mode` or similar is introduced)\n   - Test `metaagent refine --output-mode json`:\n     - Stdout contains only valid JSON (no extra logging text).\n   - Test `metaagent refine --output-mode human`:\n     - Stdout contains a human-readable summary and *no* strict JSON envelope is required.\n     - Exit code handling remains consistent.\n\n5. **Integration Test with Orchestrator**\n   - Add or extend a test (e.g., in `tests/test_orchestrator_cli_integration.py`) that:\n     - Uses a temporary directory and minimal config/PRD to run `metaagent refine` end-to-end with the real orchestrator in mock/analysis-only mode (no external API keys or subprocess Claude Code invocations).\n     - Confirms that, when the orchestrator produces a real `implementation_report`, the CLI prints the expected JSON envelope.\n\n6. **Contract Consistency Checks**\n   - Write a test that validates the emitted CLI JSON (or its `report` portion) against a lightweight schema or expectations derived from the existing `JSON_RESPONSE_SCHEMA` to ensure field names and types remain stable over time.\n   - Include this test in the standard `pytest` run so regressions in the report shape are caught early.",
        "status": "done",
        "dependencies": [
          "13",
          "18",
          "19",
          "22",
          "32",
          "40"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-15T01:02:42.084Z"
      },
      {
        "id": "44",
        "title": "Fix Shell Injection Vulnerability in repomix.py by Using List-Based Subprocess Commands",
        "description": "Refactor the subprocess execution in repomix.py to eliminate shell injection vulnerability by replacing shell=True with string formatting to list-based command arguments passed directly to subprocess.run(), following OWASP secure coding practices.",
        "details": "## Problem Statement\n\nThe current implementation in `src/metaagent/repomix.py:65-82` uses shell=True with string-formatted commands, creating a potential shell injection vulnerability:\n\n```python\n# VULNERABLE CODE (lines 65-68)\ncmd_options = [\n    \"repomix --output {} --style markdown\".format(str(output_path)),\n    \"npx repomix --output {} --style markdown\".format(str(output_path)),\n]\n```\n\nIf `output_path` contains shell metacharacters (e.g., `; rm -rf /` or `$(malicious_command)`), arbitrary code could be executed.\n\n## Implementation Steps\n\n### 1. Convert Command Strings to List Format\n\nReplace the string-formatted commands with proper list-based command arrays:\n\n```python\n# SECURE REPLACEMENT\ncmd_options = [\n    ['repomix', '--output', str(output_path), '--style', 'markdown'],\n    ['npx', 'repomix', '--output', str(output_path), '--style', 'markdown'],\n]\n```\n\n### 2. Remove shell=True Where Possible\n\nOn Unix/Linux/macOS, `shell=True` is not needed when using list-based commands. The subprocess module will directly execute the command without shell interpretation.\n\n### 3. Handle Windows-Specific PATH Resolution\n\nOn Windows, `shell=True` is sometimes needed to resolve commands in PATH (especially for npm/npx). Implement a cross-platform solution:\n\n```python\nimport platform\nimport shutil\n\ndef _find_executable(name: str) -> Optional[str]:\n    \"\"\"Find executable path, handling Windows PATH issues.\"\"\"\n    return shutil.which(name)\n\n# In pack() method:\nuse_shell = platform.system() == \"Windows\"\n\nif use_shell:\n    # On Windows, use shell=True but with list commands (safer than string)\n    cmd_options = [\n        ['repomix', '--output', str(output_path), '--style', 'markdown'],\n        ['npx', 'repomix', '--output', str(output_path), '--style', 'markdown'],\n    ]\nelse:\n    # On Unix, try to find the executable first\n    repomix_path = shutil.which('repomix')\n    npx_path = shutil.which('npx')\n    cmd_options = []\n    if repomix_path:\n        cmd_options.append([repomix_path, '--output', str(output_path), '--style', 'markdown'])\n    if npx_path:\n        cmd_options.append([npx_path, 'repomix', '--output', str(output_path), '--style', 'markdown'])\n    if not cmd_options:\n        cmd_options = [\n            ['repomix', '--output', str(output_path), '--style', 'markdown'],\n            ['npx', 'repomix', '--output', str(output_path), '--style', 'markdown'],\n        ]\n```\n\n### 4. Update subprocess.run() Call\n\nModify the subprocess.run() call at line 75-82:\n\n```python\nfor cmd in cmd_options:\n    try:\n        result = subprocess.run(\n            cmd,  # Now a list, not a string\n            cwd=repo_path,\n            capture_output=True,\n            text=True,\n            timeout=self.timeout,\n            shell=use_shell,  # Only True on Windows\n        )\n        if result.returncode == 0:\n            break\n        last_error = result.stderr\n    except FileNotFoundError:\n        last_error = f\"Command not found: {cmd[0]}\"  # Access first element of list\n        continue\n```\n\n### 5. Update Error Message Handling\n\nFix line 87 to handle list-based commands:\n\n```python\nexcept FileNotFoundError:\n    last_error = f\"Command not found: {cmd[0]}\"  # Changed from cmd.split()[0]\n    continue\n```\n\n### 6. Alternative: Avoid shell=True Entirely on Windows\n\nA more secure approach for Windows is to locate the executable using `shutil.which()` and pass the full path:\n\n```python\nimport shutil\n\ndef _get_command_path(cmd_name: str) -> Optional[str]:\n    \"\"\"Get full path to command, handling Windows .cmd/.bat extensions.\"\"\"\n    # On Windows, npm/npx are often .cmd files\n    path = shutil.which(cmd_name)\n    if path:\n        return path\n    # Try with common Windows extensions\n    for ext in ['.cmd', '.bat', '.exe']:\n        path = shutil.which(cmd_name + ext)\n        if path:\n            return path\n    return None\n```\n\n## Security Considerations\n\n1. **Path Validation**: Even with list-based commands, validate that `output_path` doesn't contain unexpected characters\n2. **Temporary File Location**: Ensure tempfile is created in a secure location (already handled by `tempfile.NamedTemporaryFile`)\n3. **Working Directory**: The `cwd=repo_path` parameter should also be validated to prevent directory traversal\n\n## Files to Modify\n\n- `src/metaagent/repomix.py`: Main implementation changes (lines 62-88)\n\n## Backwards Compatibility\n\nThis change maintains full backwards compatibility:\n- Same public API (`RepomixRunner.pack()`)\n- Same `RepomixResult` dataclass\n- Same timeout and max_chars parameters\n- Same fallback behavior (repomix → npx repomix)",
        "testStrategy": "## Test Strategy\n\n### 1. Create/Update Test File `tests/test_repomix.py`\n\n#### Unit Tests for Secure Command Building\n\n```python\nimport pytest\nfrom unittest.mock import patch, MagicMock\nfrom pathlib import Path\nfrom metaagent.repomix import RepomixRunner\n\nclass TestRepomixSecurityFix:\n    \"\"\"Tests for shell injection vulnerability fix.\"\"\"\n\n    def test_command_is_list_not_string(self):\n        \"\"\"Verify subprocess receives list-based command, not string.\"\"\"\n        runner = RepomixRunner()\n        with patch('subprocess.run') as mock_run:\n            mock_run.return_value = MagicMock(returncode=0, stderr='')\n            with patch('tempfile.NamedTemporaryFile'):\n                with patch('pathlib.Path.exists', return_value=True):\n                    with patch('pathlib.Path.read_text', return_value='content'):\n                        runner.pack(Path('/test/repo'))\n            \n            # Verify subprocess.run was called with a list, not string\n            call_args = mock_run.call_args[0][0]\n            assert isinstance(call_args, list), \"Command should be a list, not a string\"\n            assert 'repomix' in call_args[0] or call_args[0] == 'repomix'\n\n    def test_shell_injection_path_is_escaped(self):\n        \"\"\"Test that malicious paths don't cause shell injection.\"\"\"\n        runner = RepomixRunner()\n        with patch('subprocess.run') as mock_run:\n            mock_run.return_value = MagicMock(returncode=0, stderr='')\n            with patch('tempfile.NamedTemporaryFile') as mock_tmp:\n                # Simulate a path with shell metacharacters\n                mock_tmp.return_value.__enter__.return_value.name = '/tmp/test; rm -rf /'\n                with patch('pathlib.Path.exists', return_value=True):\n                    with patch('pathlib.Path.read_text', return_value='content'):\n                        runner.pack(Path('/test/repo'))\n            \n            # Verify the dangerous string is passed as a single argument, not interpreted\n            call_args = mock_run.call_args[0][0]\n            # The path should be a separate list element, not part of the command string\n            assert '--output' in call_args\n            output_index = call_args.index('--output')\n            # The next element should be the path as a single string\n            assert isinstance(call_args[output_index + 1], str)\n\n    def test_command_arguments_are_separate_list_elements(self):\n        \"\"\"Verify each argument is a separate list element.\"\"\"\n        runner = RepomixRunner()\n        with patch('subprocess.run') as mock_run:\n            mock_run.return_value = MagicMock(returncode=0, stderr='')\n            with patch('tempfile.NamedTemporaryFile'):\n                with patch('pathlib.Path.exists', return_value=True):\n                    with patch('pathlib.Path.read_text', return_value='content'):\n                        runner.pack(Path('/test/repo'))\n            \n            call_args = mock_run.call_args[0][0]\n            # Check structure: ['repomix', '--output', '<path>', '--style', 'markdown']\n            assert '--output' in call_args\n            assert '--style' in call_args\n            assert 'markdown' in call_args\n```\n\n#### Integration Tests\n\n```python\nclass TestRepomixIntegration:\n    \"\"\"Integration tests for repomix execution.\"\"\"\n\n    @pytest.mark.skipif(not shutil.which('repomix'), reason=\"repomix not installed\")\n    def test_real_repomix_execution(self, tmp_path):\n        \"\"\"Test actual repomix execution with list-based command.\"\"\"\n        # Create a minimal test repo\n        (tmp_path / 'test.py').write_text('print(\"hello\")')\n        \n        runner = RepomixRunner(timeout=60)\n        result = runner.pack(tmp_path)\n        \n        # Should succeed without shell injection issues\n        assert result.success or 'not found' in (result.error or '').lower()\n\n    def test_path_with_spaces(self, tmp_path):\n        \"\"\"Test that paths with spaces work correctly.\"\"\"\n        space_path = tmp_path / 'path with spaces'\n        space_path.mkdir()\n        (space_path / 'test.py').write_text('print(\"test\")')\n        \n        runner = RepomixRunner()\n        with patch('subprocess.run') as mock_run:\n            mock_run.return_value = MagicMock(returncode=0, stderr='')\n            with patch('pathlib.Path.read_text', return_value='content'):\n                result = runner.pack(space_path)\n            \n            # Path with spaces should be handled as single argument\n            call_args = mock_run.call_args[0][0]\n            assert any('path with spaces' in str(arg) for arg in call_args)\n```\n\n### 2. Manual Security Verification\n\n1. **Test with malicious path names**:\n   - Create temp file paths containing `;`, `|`, `$()`, backticks\n   - Verify no shell command execution occurs\n   \n2. **Cross-platform testing**:\n   - Test on Windows (where shell=True may still be used)\n   - Test on Linux/macOS (where shell=False should be used)\n\n3. **Run existing test suite**:\n   ```bash\n   pytest tests/ -v --tb=short\n   ```\n\n### 3. Static Analysis\n\nRun security linters to verify the fix:\n```bash\nbandit -r src/metaagent/repomix.py\nsemgrep --config=p/python src/metaagent/repomix.py\n```\n\n### 4. Code Review Checklist\n\n- [ ] No string formatting of shell commands\n- [ ] subprocess.run() receives list argument\n- [ ] shell=True only used on Windows (if needed)\n- [ ] Error messages handle list-based commands correctly\n- [ ] All tests pass",
        "status": "done",
        "dependencies": [
          "4"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-15T01:07:03.677Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-15T01:07:03.680Z",
      "taskCount": 43,
      "completedCount": 41,
      "tags": [
        "master"
      ]
    }
  }
}