{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Initialize Python Project Structure",
        "description": "Set up the Python project with pyproject.toml, create the directory structure as specified in the PRD, and configure development dependencies.",
        "details": "Create the following structure:\n\n1. Create `pyproject.toml` with:\n   - Project metadata (name='metaagent', version='0.1.0')\n   - Python 3.10+ requirement\n   - Dependencies: click or typer, pyyaml, httpx, python-dotenv, jinja2\n   - Dev dependencies: pytest, pytest-cov, pytest-asyncio\n   - Entry point: metaagent = 'metaagent.cli:main'\n\n2. Create directory structure:\n   ```\n   src/metaagent/__init__.py\n   src/metaagent/cli.py (stub)\n   src/metaagent/orchestrator.py (stub)\n   src/metaagent/repomix.py (stub)\n   src/metaagent/prompts.py (stub)\n   src/metaagent/analysis.py (stub)\n   src/metaagent/plan_writer.py (stub)\n   src/metaagent/config.py (stub)\n   config/prompts.yaml (empty structure)\n   config/profiles.yaml (empty structure)\n   tests/__init__.py\n   tests/test_cli.py (stub)\n   tests/test_orchestrator.py (stub)\n   docs/\n   ```\n\n3. Update .gitignore for Python:\n   - __pycache__/, *.pyc, *.pyo\n   - .venv/, venv/, .env\n   - dist/, build/, *.egg-info/\n   - .pytest_cache/, .coverage\n\n4. Create README.md with basic project description and setup instructions.",
        "testStrategy": "Verify project structure exists with `ls -R`. Verify `uv pip install -e .` or `pip install -e .` succeeds. Verify `metaagent --help` runs without errors (will show help from stub CLI).",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Author modern pyproject.toml with src-layout and dependency groups",
            "description": "Create pyproject.toml file with project metadata, Python 3.10+ requirement, runtime and dev dependencies, and console_script entry point using modern standards.",
            "dependencies": [],
            "details": "Use [tool.uv] or [build-system] with hatchling; define [project] table with name='metaagent', version='0.1.0', requires-python='>=3.10'; dependencies=['click', 'pyyaml', 'httpx', 'python-dotenv', 'jinja2']; optional-dependencies.dev=['pytest', 'pytest-cov', 'pytest-asyncio']; [project.scripts] metaagent='metaagent.cli:main'; enable src-layout with packages=['src/metaagent'].",
            "status": "pending",
            "testStrategy": "Verify with `uv pip install -e .` or `pip install -e .` succeeds without errors; check `metaagent --help` displays CLI help from stub; validate TOML syntax and metadata with `uv project info`.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create src/, config/, tests/, and docs/ directory skeletons",
            "description": "Set up complete directory structure with all specified empty stub files as per PRD requirements.",
            "dependencies": [
              1
            ],
            "details": "Create directories: src/metaagent/, config/, tests/, docs/; add __init__.py files to src/metaagent/ and tests/; create stub Python files: cli.py, orchestrator.py, repomix.py, prompts.py, analysis.py, plan_writer.py, config.py; create empty YAML files: config/prompts.yaml, config/profiles.yaml.",
            "status": "pending",
            "testStrategy": "Run `ls -R src config tests docs` to verify exact structure matches spec; ensure all stub files exist and contain pass or basic if __name__ == '__main__' guards.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Set up .gitignore for Python projects and common tooling",
            "description": "Create comprehensive .gitignore covering Python caches, virtualenvs, build artifacts, and testing caches.",
            "dependencies": [
              1
            ],
            "details": "Include patterns: __pycache__/, *.pyc, *.pyo, *.pyd; .venv/, venv/, ENV/, env/; .env, .env.*; dist/, build/, *.egg-info/, *.whl; .pytest_cache/, .coverage, .coverage.*; recommend adding .uv/, uv.lock if using uv tooling.",
            "status": "pending",
            "testStrategy": "Verify git ignores files by creating test files matching patterns and running `git status --ignored`; ensure common ignore files like .env are properly excluded.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create minimal README.md with install, usage, and dev notes",
            "description": "Write basic project README including project description, installation instructions, basic usage, and development setup.",
            "dependencies": [
              1,
              2
            ],
            "details": "Include sections: # MetaAgent (AI-powered repo refinement); ## Installation (`uv venv; uv pip install -e .` or pip); ## Usage (`metaagent --help`); ## Development (pytest, pre-commit if added); ## Structure overview; link to PRD/docs.",
            "status": "pending",
            "testStrategy": "Verify README renders correctly in GitHub/Markdown viewer; check all commands in install/usage sections execute successfully after project setup.",
            "parentId": "undefined"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down Task 1 (Initialize Python Project Structure) into 4 concrete subtasks covering: (1) authoring a modern pyproject.toml with src-layout, dependency groups for dev, and console_script entry point; (2) creating the specified src/, config/, tests/, and docs/ directory/files skeletons; (3) setting up .gitignore following Python and common tooling patterns; (4) creating a minimal but correct README.md with install, usage, and development notes. For each subtask, specify clear acceptance criteria and any tooling conventions (e.g., uv, pytest).",
        "updatedAt": "2025-12-14T01:59:42.237Z"
      },
      {
        "id": "2",
        "title": "Implement Configuration Management",
        "description": "Create the config.py module to handle environment variables, application settings, and provide a centralized configuration interface.",
        "details": "Implement `src/metaagent/config.py`:\n\n```python\nimport os\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\n\n@dataclass\nclass Config:\n    perplexity_api_key: str | None\n    anthropic_api_key: str | None\n    log_level: str\n    timeout: int\n    max_tokens: int\n    repo_path: Path\n    prd_path: Path\n    config_dir: Path\n    output_dir: Path\n\n    @classmethod\n    def from_env(cls, repo_path: Path | None = None) -> 'Config':\n        load_dotenv()\n        repo = repo_path or Path.cwd()\n        return cls(\n            perplexity_api_key=os.getenv('PERPLEXITY_API_KEY'),\n            anthropic_api_key=os.getenv('ANTHROPIC_API_KEY'),\n            log_level=os.getenv('METAAGENT_LOG_LEVEL', 'INFO'),\n            timeout=int(os.getenv('METAAGENT_TIMEOUT', '120')),\n            max_tokens=int(os.getenv('METAAGENT_MAX_TOKENS', '100000')),\n            repo_path=repo,\n            prd_path=repo / 'docs' / 'prd.md',\n            config_dir=Path(__file__).parent.parent.parent / 'config',\n            output_dir=repo / 'docs'\n        )\n\n    def validate(self) -> list[str]:\n        \"\"\"Return list of validation errors, empty if valid.\"\"\"\n        errors = []\n        if not self.repo_path.exists():\n            errors.append(f'Repository path does not exist: {self.repo_path}')\n        if not self.config_dir.exists():\n            errors.append(f'Config directory does not exist: {self.config_dir}')\n        return errors\n```\n\nKey features:\n- Load from environment variables with defaults\n- Dataclass for type safety and immutability\n- Path resolution for repo, PRD, config, and output directories\n- Validation method for required paths\n- Support for both installed package and development mode paths",
        "testStrategy": "Unit tests in `tests/test_config.py`:\n1. Test Config.from_env() with mocked environment variables\n2. Test default values when env vars not set\n3. Test validate() returns errors for missing paths\n4. Test validate() returns empty list for valid paths\n5. Test path resolution works correctly",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Config dataclass and from_env() factory with dotenv loading and robust defaults",
            "description": "Create the Config dataclass and the from_env() classmethod to load configuration from environment variables using python-dotenv, including sensible defaults and safe type conversion.",
            "dependencies": [],
            "details": "- Define the Config dataclass exactly as specified (API keys, log_level, timeout, max_tokens, repo_path, prd_path, config_dir, output_dir) with appropriate type hints.\n- Implement from_env() to call load_dotenv() first so that .env files are respected.\n- Accept an optional repo_path argument; if not provided, default to Path.cwd().\n- Read PERPLEXITY_API_KEY and ANTHROPIC_API_KEY from the environment; allow them to be missing (set to None) without raising.\n- Read METAAGENT_LOG_LEVEL, METAAGENT_TIMEOUT, METAAGENT_MAX_TOKENS with robust defaulting and type conversion (e.g., fall back to safe defaults if env values are malformed integers).\n- Compute prd_path as repo_path / 'docs' / 'prd.md'.\n- Compute config_dir relative to the installed package layout (e.g., Path(__file__).parent.parent.parent / 'config') so it works in both editable and installed modes.\n- Compute output_dir as repo_path / 'docs', creating only paths in later code, not here.\n- Edge cases to consider and later test: missing env vars (ensure defaults/None used), invalid integer values for timeout/max_tokens (decide on fallback behavior, e.g., catch ValueError and revert to defaults rather than crash), unusual CWD when repo_path is not passed.\n- Do not perform filesystem existence checks here; leave that to validate().",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement validate() with repo/config/output/PRD path checks and dev vs installed behavior",
            "description": "Implement the validate() method to check existence and correctness of key paths (repo_path, config_dir, output_dir, prd_path), handling differences between development and installed package modes.",
            "dependencies": [
              1
            ],
            "details": "- Extend validate() to return a list of human-readable error strings describing configuration problems; keep empty list on success.\n- Check that repo_path exists and is a directory; if not, add an error.\n- Check that config_dir exists and is a directory; if not, add an error describing that configuration assets are missing.\n- Check that output_dir exists or, if your design requires it, that its parent exists; decide whether absence of output_dir is an error (e.g., might be created later) or only a warning (still represented as an error string if you choose to enforce creation upfront).\n- Check that prd_path exists and is a file; add an error if the PRD is missing so downstream components (orchestrator, plan writer) can fail early.\n- For dev vs installed modes, base behavior only on the resolved config_dir path: for example, treat a config_dir located inside the source tree (e.g., under src/metaagent/../config) as dev mode, and a site-packages-like path as installed mode; ensure validate() behaves consistently in both, without hard-coding environment-specific assumptions.\n- Ensure validate() never raises; all issues must be represented as error messages so callers can decide how to handle them.\n- Edge cases and expectations: non-existent repo_path passed explicitly; repo_path that exists but lacks docs/ or prd.md; config_dir missing because package assets not installed; output_dir pointing to a file instead of a directory; paths that are symlinks (still treated as existing).",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Wire Config into a minimal caller (stub CLI or script) to exercise usage patterns",
            "description": "Create a small entrypoint (e.g., a stub CLI command or script) that constructs Config.from_env(), runs validate(), and reports configuration status, to verify real-world usage patterns.",
            "dependencies": [
              1,
              2
            ],
            "details": "- Implement a minimal callable entrypoint, such as a function main() or a tiny Typer/Rich-based CLI stub under src/metaagent, that imports and uses Config.\n- In the entrypoint, call Config.from_env() with an optional repo_path argument (e.g., from CLI flag or default Path.cwd()).\n- Immediately call config.validate() and, if errors are returned, print them in a user-friendly way and exit with a non-zero status code.\n- On success (no validation errors), print or log key configuration values (e.g., repo_path, prd_path, config_dir, output_dir, timeout, log_level) without exposing sensitive API keys.\n- Ensure the entrypoint demonstrates how higher-level components (orchestrator, CLI) will interact with Config, including typical error-handling flow.\n- Edge cases and expectations: behavior when repo_path points to a non-existent directory (entrypoint should show validation errors, not crash); behavior with missing PRD or config_dir (clear error output); running from different working directories to confirm repo_path resolution is intuitive.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create focused pytest unit tests for Config using monkeypatch and tmp_path",
            "description": "Add pytest unit tests covering environment-variable handling, default values, path resolution, and validate() behavior across various filesystem scenarios using monkeypatch and tmp_path.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "- Implement tests in tests/test_config.py structured around the provided test strategy.\n- Use monkeypatch to control os.environ for Config.from_env() tests, including setting PERPLEXITY_API_KEY, ANTHROPIC_API_KEY, METAAGENT_LOG_LEVEL, METAAGENT_TIMEOUT, and METAAGENT_MAX_TOKENS, and clearing them to test defaults.\n- Test edge cases of env parsing: missing API keys (must yield None), missing optional settings (must use defaults), and invalid integer strings for timeout/max_tokens (must fall back safely instead of raising).\n- Use tmp_path to create ephemeral directory structures representing a fake repo with docs/, prd.md, and optional config/ directory, passing these paths into Config instances for validation tests.\n- Write tests where repo_path does not exist, config_dir does not exist, output_dir points to a file instead of a directory, and prd.md is missing; assert that validate() returns clear error messages for each condition.\n- Add a test to confirm that path resolution from from_env() (repo_path, prd_path, config_dir, output_dir) matches expectations when run from different working directories or with an explicit repo_path.\n- Optionally add an integration-style test that uses the minimal caller entrypoint to confirm end-to-end behavior (construct, validate, and report), asserting correct exit codes and output using pytest’s capsys or CliRunner if using Typer.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down Task 2 (Implement Configuration Management) into 4 subtasks: (1) implement Config dataclass and from_env factory using python-dotenv and robust type conversion with defaults; (2) implement validate() including repo/config/output/PRD path checks and behavior in dev vs installed modes; (3) wire Config into a minimal caller (e.g., stub CLI) to verify usage patterns; (4) create focused pytest unit tests using monkeypatch/tmp_path for environment and filesystem scenarios. For each subtask, call out edge cases (missing env vars, non-existent paths) and test expectations.",
        "updatedAt": "2025-12-14T01:59:48.825Z"
      },
      {
        "id": "3",
        "title": "Implement Prompt and Profile Loading",
        "description": "Create the prompts.py module to load and render prompt templates from YAML configuration, and load profile definitions that map stages to prompts.",
        "details": "Implement `src/metaagent/prompts.py`:\n\n```python\nfrom pathlib import Path\nfrom dataclasses import dataclass\nimport yaml\nfrom jinja2 import Template\n\n@dataclass\nclass Prompt:\n    id: str\n    goal: str\n    stage: str\n    template: str\n\n    def render(self, prd: str, code_context: str, history: str, current_stage: str) -> str:\n        \"\"\"Render template with provided variables.\"\"\"\n        tmpl = Template(self.template)\n        return tmpl.render(\n            prd=prd,\n            code_context=code_context,\n            history=history,\n            current_stage=current_stage\n        )\n\n@dataclass\nclass Profile:\n    name: str\n    description: str\n    stages: list[str]\n\nclass PromptLibrary:\n    def __init__(self, config_dir: Path):\n        self.config_dir = config_dir\n        self._prompts: dict[str, Prompt] = {}\n        self._profiles: dict[str, Profile] = {}\n        self._load()\n\n    def _load(self):\n        # Load prompts.yaml\n        prompts_file = self.config_dir / 'prompts.yaml'\n        if prompts_file.exists():\n            with open(prompts_file) as f:\n                data = yaml.safe_load(f)\n            for pid, pdata in data.get('prompts', {}).items():\n                self._prompts[pid] = Prompt(\n                    id=pdata['id'],\n                    goal=pdata['goal'],\n                    stage=pdata['stage'],\n                    template=pdata['template']\n                )\n\n        # Load profiles.yaml\n        profiles_file = self.config_dir / 'profiles.yaml'\n        if profiles_file.exists():\n            with open(profiles_file) as f:\n                data = yaml.safe_load(f)\n            for pname, pdata in data.get('profiles', {}).items():\n                self._profiles[pname] = Profile(\n                    name=pdata['name'],\n                    description=pdata['description'],\n                    stages=pdata['stages']\n                )\n\n    def get_prompt(self, prompt_id: str) -> Prompt | None:\n        return self._prompts.get(prompt_id)\n\n    def get_profile(self, profile_name: str) -> Profile | None:\n        return self._profiles.get(profile_name)\n\n    def list_profiles(self) -> list[str]:\n        return list(self._profiles.keys())\n\n    def get_prompts_for_profile(self, profile_name: str) -> list[Prompt]:\n        profile = self.get_profile(profile_name)\n        if not profile:\n            return []\n        return [self._prompts[s] for s in profile.stages if s in self._prompts]\n```\n\nPopulate `config/prompts.yaml` and `config/profiles.yaml` with the exact content from PRD Section 7.1 and 7.2.",
        "testStrategy": "Unit tests in `tests/test_prompts.py`:\n1. Test loading prompts from YAML file\n2. Test loading profiles from YAML file\n3. Test Prompt.render() with template variables\n4. Test get_prompts_for_profile() returns correct ordered list\n5. Test handling of missing files gracefully\n6. Test list_profiles() returns all profile names",
        "priority": "high",
        "dependencies": [
          "1",
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Prompt and Profile dataclasses and PromptLibrary public interface",
            "description": "Create the Prompt and Profile dataclasses and sketch the PromptLibrary API surface that will manage loading and accessing prompts and profiles.",
            "dependencies": [
              3
            ],
            "details": "Implement Prompt and Profile as @dataclass structures with the fields specified in the PRD (id, goal, stage, template for Prompt; name, description, stages for Profile). Define the PromptLibrary __init__(config_dir: Path) signature and internal dictionaries for prompts and profiles. Stub out _load(), get_prompt(), get_profile(), list_profiles(), and get_prompts_for_profile() with type hints and docstrings but no logic yet, ensuring the interface is stable for later use by orchestrator and CLI.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement YAML loading with safe_load, validation, and missing-file behavior",
            "description": "Add YAML loading logic to PromptLibrary that reads prompts.yaml and profiles.yaml, using yaml.safe_load with graceful handling of missing files and malformed content.",
            "dependencies": [
              1
            ],
            "details": "In PromptLibrary._load(), implement reading config_dir / 'prompts.yaml' and config_dir / 'profiles.yaml' using context managers and yaml.safe_load. If a file does not exist, skip loading without raising, so construction of PromptLibrary has minimal side effects and is deterministic. Add minimal schema validation (e.g., ensure top-level keys 'prompts' and 'profiles' are dicts, required fields exist) and either log or raise clear exceptions for invalid structures while keeping side effects confined to object state. Avoid any global state; all loaded data should be stored only in self._prompts and self._profiles so tests can inject a temporary config_dir with fixture YAML files.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Prompt.render() using Jinja2 with fixed variable contract and error guardrails",
            "description": "Complete the Prompt.render() method so it renders templates with a fixed set of variables via Jinja2, handling template errors safely.",
            "dependencies": [
              1,
              2
            ],
            "details": "Use jinja2.Template to compile self.template and render it with a fixed context including prd, code_context, history, and current_stage. Decide on behavior for template syntax or rendering errors (e.g., catch TemplateError and either re-raise a custom exception or return a fallback string) to prevent hard crashes in callers like the orchestrator. Keep rendering pure and side-effect free: no file I/O or logging inside render, so unit tests can call it deterministically. Document the variable contract in the render docstring so YAML templates in prompts.yaml can rely on a stable set of fields.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement profile and prompt lookup helpers with ordering guarantees",
            "description": "Fill in PromptLibrary helper methods for querying prompts and profiles, ensuring get_prompts_for_profile preserves the profile-defined stage order and handles missing mappings robustly.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement get_prompt(prompt_id) and get_profile(profile_name) as simple dictionary lookups returning None when not found. Implement list_profiles() to return profile names in deterministic order (e.g., sorted or insertion order, documented explicitly). Implement get_prompts_for_profile(profile_name) to resolve the profile, then map profile.stages entries to Prompt instances, skipping unknown prompt IDs safely while preserving the original stage order. Keep logic purely in-memory so tests can construct PromptLibrary against temporary YAML directories without global side effects.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write unit tests for YAML loading, rendering, helpers, and error cases",
            "description": "Create tests in tests/test_prompts.py that cover YAML fixtures, prompt rendering, helper behaviors, missing files, and invalid data, while keeping loading side effects isolated.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Add pytest-based tests that use temporary directories to host prompts.yaml and profiles.yaml fixtures derived from PRD sections 7.1 and 7.2. Test that PromptLibrary loads valid YAML into correct Prompt and Profile objects, that Prompt.render() correctly interpolates all supported variables, and that get_prompts_for_profile() returns prompts in the expected stage order. Add tests for behavior when prompts.yaml and/or profiles.yaml are missing (PromptLibrary still constructs and methods return empty/None appropriately) and when YAML contains invalid structures or missing required fields. Ensure each test constructs its own PromptLibrary instance pointing at an isolated temp config_dir so there are no cross-test side effects or reliance on global filesystem state.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down Task 3 (Implement Prompt and Profile Loading) into 5 subtasks: (1) define Prompt and Profile dataclasses plus PromptLibrary interface; (2) implement YAML loading with safe_load, error handling, and missing-file behavior; (3) implement Prompt.render() using Jinja2 with a fixed variable contract and guardrails for template errors; (4) implement profile/query helpers (get_prompt, get_profile, list_profiles, get_prompts_for_profile) with ordering guarantees; (5) write unit tests covering YAML fixtures, rendering, missing files, and invalid data cases. Explicitly note how to keep loading side effects contained for testability.",
        "updatedAt": "2025-12-14T01:59:55.377Z"
      },
      {
        "id": "4",
        "title": "Implement Repomix Integration",
        "description": "Create the repomix.py module to run Repomix CLI as a subprocess and return the packed codebase content.",
        "details": "Implement `src/metaagent/repomix.py`:\n\n```python\nimport subprocess\nimport tempfile\nfrom pathlib import Path\nfrom dataclasses import dataclass\n\n@dataclass\nclass RepomixResult:\n    success: bool\n    content: str\n    error: str | None = None\n\nclass RepomixRunner:\n    def __init__(self, timeout: int = 120):\n        self.timeout = timeout\n\n    def pack(self, repo_path: Path) -> RepomixResult:\n        \"\"\"\n        Run Repomix on the given repository and return packed content.\n        \n        Args:\n            repo_path: Path to the repository to pack\n            \n        Returns:\n            RepomixResult with success status and content or error\n        \"\"\"\n        with tempfile.NamedTemporaryFile(suffix='.txt', delete=False) as tmp:\n            output_file = Path(tmp.name)\n        \n        try:\n            result = subprocess.run(\n                ['npx', '-y', 'repomix', '--output', str(output_file)],\n                cwd=str(repo_path),\n                capture_output=True,\n                text=True,\n                timeout=self.timeout\n            )\n            \n            if result.returncode != 0:\n                return RepomixResult(\n                    success=False,\n                    content='',\n                    error=f'Repomix failed: {result.stderr}'\n                )\n            \n            content = output_file.read_text(encoding='utf-8')\n            return RepomixResult(success=True, content=content)\n            \n        except subprocess.TimeoutExpired:\n            return RepomixResult(\n                success=False,\n                content='',\n                error=f'Repomix timed out after {self.timeout}s'\n            )\n        except FileNotFoundError:\n            return RepomixResult(\n                success=False,\n                content='',\n                error='npx/repomix not found. Ensure Node.js is installed.'\n            )\n        finally:\n            output_file.unlink(missing_ok=True)\n\n    def truncate_content(self, content: str, max_tokens: int) -> str:\n        \"\"\"Truncate content to fit within token budget (rough char estimate).\"\"\"\n        # Rough estimate: 1 token ≈ 4 characters\n        max_chars = max_tokens * 4\n        if len(content) <= max_chars:\n            return content\n        return content[:max_chars] + '\\n\\n[Content truncated to fit token limit]'\n```\n\nKey considerations:\n- Use npx to run repomix without global installation\n- Capture output to temp file and read content\n- Handle timeout errors gracefully\n- Handle missing npx/node gracefully\n- Provide truncation utility for context budget",
        "testStrategy": "Unit tests in `tests/test_repomix.py`:\n1. Test successful pack with mock subprocess (mock subprocess.run)\n2. Test timeout handling\n3. Test error handling for failed subprocess\n4. Test FileNotFoundError handling\n5. Test truncate_content() with content under limit\n6. Test truncate_content() with content over limit\n7. Integration test with real Repomix on small test repo (optional, mark as slow)",
        "priority": "high",
        "dependencies": [
          "1",
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design RepomixResult dataclass and RepomixRunner API",
            "description": "Define the data class for results and the main class structure with init and method signatures.",
            "dependencies": [],
            "details": "Create RepomixResult dataclass with success, content, error fields. Define RepomixRunner __init__ with timeout param and pack() method signature accepting Path returning RepomixResult. Add truncate_content signature.",
            "status": "pending",
            "testStrategy": "Verify dataclass field types and defaults. Test __init__ sets timeout correctly. Test method signatures via inspect.signature.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement pack() method core logic with subprocess and temp file",
            "description": "Implement the main subprocess.run call using npx repomix with temp file output and cwd set to repo_path.",
            "dependencies": [
              1
            ],
            "details": "Use tempfile.NamedTemporaryFile for output. Run subprocess.run(['npx', '-y', 'repomix', '--output', str(output_file)], cwd=str(repo_path), capture_output=True, text=True, timeout=self.timeout). Read content on success. Ensure cleanup in finally block with unlink(missing_ok=True).",
            "status": "pending",
            "testStrategy": "Mock subprocess.run returning success (returncode=0). Verify temp file path passed correctly to --output. Verify cwd set to repo_path. Verify content read from temp file after success.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add comprehensive error handling branches",
            "description": "Handle subprocess failure, timeout, and missing npx/Node.js cases with descriptive error messages in RepomixResult.",
            "dependencies": [
              2
            ],
            "details": "Check result.returncode != 0 and return error with result.stderr. Catch subprocess.TimeoutExpired with timeout message. Catch FileNotFoundError with 'npx/repomix not found' message ensuring Node.js check.",
            "status": "pending",
            "testStrategy": "Mock subprocess.run with returncode=1 and stderr. Mock TimeoutExpired exception. Mock FileNotFoundError. Verify each returns RepomixResult(success=False, error=expected_message).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement truncate_content() with token estimation",
            "description": "Add utility method to truncate content based on max_tokens using 4 chars per token heuristic.",
            "dependencies": [
              1
            ],
            "details": "Calculate max_chars = max_tokens * 4. Return content unchanged if under limit. Otherwise truncate and append '[Content truncated to fit token limit]' suffix. Document the 1 token ≈ 4 chars approximation.",
            "status": "pending",
            "testStrategy": "Test content under limit returns unchanged. Test exact boundary (max_chars). Test over limit truncates correctly with suffix. Test empty string and max_tokens=0 edge cases.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write comprehensive unit tests with mocks for CI stability",
            "description": "Create test_repomix.py with pytest monkeypatch covering all success/failure paths without requiring Node.js.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Use monkeypatch to mock subprocess.run for success/failure/timeout cases. Mock Path.read_text() and tempfile.NamedTemporaryFile. Test truncate_content boundaries. Ensure tests pass in CI without Node/repomix installed by mocking all external calls.",
            "status": "pending",
            "testStrategy": "Coverage: success pack, failed subprocess, timeout, FileNotFoundError, truncate under/over limit. Verify temp file cleanup called. Use pytest tmp_path for filesystem isolation. Mock Path.unlink().",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down Task 4 (Implement Repomix Integration) into 5 subtasks: (1) design RepomixResult dataclass and RepomixRunner API; (2) implement pack() using subprocess.run with cwd, temp file handling, and robust cleanup in finally; (3) implement error handling branches for non-zero exit, TimeoutExpired, and FileNotFoundError with clear messages; (4) implement truncate_content() with a well-documented token-to-char heuristic and tests around boundaries; (5) write unit tests using monkeypatch to mock subprocess.run and filesystem interactions, covering success, failure, timeout, and missing binary cases. Include notes on making this stable in CI where Node/repomix may not be installed.",
        "updatedAt": "2025-12-14T02:33:18.775Z"
      },
      {
        "id": "5",
        "title": "Implement Analysis Engine with Mock Mode",
        "description": "Create the analysis.py module to wrap LLM API calls for analysis. Include a mock mode for testing and a clear extension point for future API integration.",
        "details": "Implement `src/metaagent/analysis.py`:\n\n```python\nimport json\nimport httpx\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Protocol\n\n@dataclass\nclass AnalysisResult:\n    summary: str\n    recommendations: list[str]\n    tasks: list[dict]  # Each task: {title, description, priority, files}\n\nclass AnalysisEngine(Protocol):\n    def analyze(self, prompt: str) -> AnalysisResult:\n        \"\"\"Run analysis with rendered prompt and return structured result.\"\"\"\n        ...\n\nclass MockAnalysisEngine:\n    \"\"\"Mock engine for testing without API calls.\"\"\"\n    \n    def analyze(self, prompt: str) -> AnalysisResult:\n        return AnalysisResult(\n            summary='Mock analysis completed. This is a placeholder result.',\n            recommendations=[\n                'Implement core functionality first',\n                'Add comprehensive error handling',\n                'Write unit tests for critical paths'\n            ],\n            tasks=[\n                {'title': 'Sample Task 1', 'description': 'Placeholder task', 'priority': 'high', 'files': []},\n                {'title': 'Sample Task 2', 'description': 'Another placeholder', 'priority': 'medium', 'files': []}\n            ]\n        )\n\nclass PerplexityAnalysisEngine:\n    \"\"\"Perplexity API integration for analysis.\"\"\"\n    \n    def __init__(self, api_key: str, timeout: int = 120):\n        self.api_key = api_key\n        self.timeout = timeout\n        self.base_url = 'https://api.perplexity.ai'\n    \n    def analyze(self, prompt: str) -> AnalysisResult:\n        \"\"\"Extension point for LLM analysis calls.\"\"\"\n        headers = {\n            'Authorization': f'Bearer {self.api_key}',\n            'Content-Type': 'application/json'\n        }\n        \n        payload = {\n            'model': 'llama-3.1-sonar-large-128k-online',\n            'messages': [\n                {'role': 'system', 'content': 'You are a code analysis expert. Always respond with valid JSON containing keys: summary, recommendations, tasks.'},\n                {'role': 'user', 'content': prompt}\n            ]\n        }\n        \n        with httpx.Client(timeout=self.timeout) as client:\n            response = client.post(\n                f'{self.base_url}/chat/completions',\n                headers=headers,\n                json=payload\n            )\n            response.raise_for_status()\n            data = response.json()\n        \n        content = data['choices'][0]['message']['content']\n        return self._parse_response(content)\n    \n    def _parse_response(self, content: str) -> AnalysisResult:\n        \"\"\"Parse JSON response from LLM.\"\"\"\n        try:\n            # Try to extract JSON from response\n            parsed = json.loads(content)\n        except json.JSONDecodeError:\n            # Fallback: try to find JSON block\n            import re\n            match = re.search(r'```json\\s*(.+?)\\s*```', content, re.DOTALL)\n            if match:\n                parsed = json.loads(match.group(1))\n            else:\n                return AnalysisResult(\n                    summary=content[:500],\n                    recommendations=[],\n                    tasks=[]\n                )\n        \n        return AnalysisResult(\n            summary=parsed.get('summary', ''),\n            recommendations=parsed.get('recommendations', []),\n            tasks=parsed.get('tasks', [])\n        )\n\ndef create_analysis_engine(api_key: str | None, use_mock: bool = False) -> AnalysisEngine:\n    \"\"\"Factory function to create appropriate analysis engine.\"\"\"\n    if use_mock or not api_key:\n        return MockAnalysisEngine()\n    return PerplexityAnalysisEngine(api_key)\n```",
        "testStrategy": "Unit tests in `tests/test_analysis.py`:\n1. Test MockAnalysisEngine.analyze() returns valid AnalysisResult\n2. Test create_analysis_engine() returns mock when use_mock=True\n3. Test create_analysis_engine() returns mock when api_key is None\n4. Test _parse_response() with valid JSON\n5. Test _parse_response() with JSON in code block\n6. Test _parse_response() with invalid JSON (fallback)\n7. Integration test with real Perplexity API (optional, mark as integration, skip in CI)",
        "priority": "high",
        "dependencies": [
          "1",
          "2",
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define AnalysisResult dataclass and AnalysisEngine protocol in analysis.py",
            "description": "Create the AnalysisResult dataclass and AnalysisEngine protocol to formalize structured analysis outputs and the engine interface.",
            "dependencies": [],
            "details": "Implement AnalysisResult with fields: summary: str, recommendations: list[str], tasks: list[dict] where each task dict includes at least title, description, priority, files. Define an AnalysisEngine Protocol with a single method analyze(self, prompt: str) -> AnalysisResult and a clear docstring describing its contract as the extension point for different LLM providers.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement MockAnalysisEngine with deterministic analyze() output",
            "description": "Create MockAnalysisEngine that implements AnalysisEngine and returns fixed, deterministic results for tests.",
            "dependencies": [
              1
            ],
            "details": "Add MockAnalysisEngine class with analyze(self, prompt: str) -> AnalysisResult returning a constant AnalysisResult instance matching the provided example values so tests can rely on stable outputs. Ensure it does not perform any network calls or depend on external state. Keep implementation simple and side-effect free so unit tests can assert exact summaries, recommendations, and tasks.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement PerplexityAnalysisEngine HTTP client with secure configuration",
            "description": "Create PerplexityAnalysisEngine that calls the Perplexity API using httpx with proper headers, payload, timeout, and error handling while avoiding API key leaks.",
            "dependencies": [
              1
            ],
            "details": "Implement __init__(self, api_key: str, timeout: int = 120) storing api_key, timeout, and base_url. Implement analyze(self, prompt: str) -> AnalysisResult using httpx.Client with configured timeout to POST to /chat/completions. Build headers including Authorization: Bearer <api_key> and Content-Type: application/json, and construct the payload with the specified model and system/user messages. Call response.raise_for_status() to surface HTTP errors, then parse JSON and extract the LLM message content. Never log or print the raw api_key, and avoid logging full request/response bodies that may contain sensitive data; if logging is needed, redact keys and truncate content.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement robust _parse_response() JSON extraction and graceful fallback",
            "description": "Add private _parse_response() helper on PerplexityAnalysisEngine to convert raw LLM content into AnalysisResult, handling invalid or wrapped JSON safely.",
            "dependencies": [
              3
            ],
            "details": "Implement _parse_response(self, content: str) -> AnalysisResult that first attempts json.loads(content). On json.JSONDecodeError, search for a ```json ... ``` fenced block via regex, attempt to json.loads on the captured block, and if that also fails, return an AnalysisResult with summary set to a truncated slice of the raw content (e.g., first 500 chars) and empty recommendations and tasks. Ensure missing keys in parsed JSON are handled via .get with sensible defaults. Do not execute or eval any content, and avoid logging full untrusted content; if logging parse failures, log only short snippets or generic error messages to reduce risk of leaking sensitive data.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement create_analysis_engine() factory for mock vs real engines",
            "description": "Create the create_analysis_engine() factory function to choose between MockAnalysisEngine and PerplexityAnalysisEngine based on api_key and use_mock flag.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement create_analysis_engine(api_key: str | None, use_mock: bool = False) -> AnalysisEngine such that it returns MockAnalysisEngine when use_mock is True or when api_key is falsy/None, and returns PerplexityAnalysisEngine(api_key) otherwise. Document this behavior clearly so callers (e.g., configuration/orchestrator) understand how to enable mock mode. Ensure the function does not log API keys and only logs high-level selection decisions if logging is added.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Write unit tests for analysis engines and JSON parsing with mocked httpx",
            "description": "Add comprehensive unit tests in tests/test_analysis.py covering mock engine, factory behavior, PerplexityAnalysisEngine HTTP interactions, and JSON parsing fallbacks without real network calls.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "Create tests verifying: (1) MockAnalysisEngine.analyze() returns an AnalysisResult with expected deterministic fields; (2) create_analysis_engine() returns MockAnalysisEngine when use_mock=True; (3) create_analysis_engine() returns MockAnalysisEngine when api_key is None or empty; (4) _parse_response() correctly parses valid JSON content; (5) _parse_response() extracts JSON from ```json fenced blocks; (6) _parse_response() falls back to truncated summary with empty lists on invalid JSON. Fully mock httpx.Client using monkeypatch or unittest.mock to simulate successful responses, HTTP error responses (ensuring raise_for_status propagates), and malformed model responses where choices/message/content is missing or non-JSON. Assert that no real HTTP requests are made and that error paths do not expose API keys or full sensitive payloads in exception messages or logs if any logging is present.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down Task 5 (Implement Analysis Engine with Mock Mode) into 6 subtasks: (1) define AnalysisResult dataclass and AnalysisEngine protocol; (2) implement MockAnalysisEngine with deterministic outputs for tests; (3) implement PerplexityAnalysisEngine HTTP client using httpx, including headers, payload shape, timeout, and error handling; (4) implement _parse_response() to robustly extract JSON from raw content or ```json``` blocks and degrade gracefully on invalid JSON; (5) implement create_analysis_engine() factory with clear rules for mock vs real engine; (6) write unit tests that fully mock httpx.Client to cover success, HTTP errors, malformed model responses, and JSON parse fallbacks, ensuring no real network access. Highlight security considerations around API keys and logging.",
        "updatedAt": "2025-12-14T02:00:08.782Z"
      },
      {
        "id": "6",
        "title": "Implement Plan Writer",
        "description": "Create the plan_writer.py module to generate the mvp_improvement_plan.md file from aggregated analysis results.",
        "details": "Implement `src/metaagent/plan_writer.py`:\n\n```python\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom .analysis import AnalysisResult\n\n@dataclass\nclass StageResult:\n    stage_name: str\n    prompt_id: str\n    result: AnalysisResult\n\nclass PlanWriter:\n    def __init__(self, output_dir: Path):\n        self.output_dir = output_dir\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n    \n    def write_plan(self, prd_content: str, stage_results: list[StageResult], profile_name: str) -> Path:\n        \"\"\"\n        Generate mvp_improvement_plan.md from analysis results.\n        \n        Args:\n            prd_content: Original PRD content\n            stage_results: Results from each analysis stage\n            profile_name: Name of the profile used\n            \n        Returns:\n            Path to the generated plan file\n        \"\"\"\n        output_path = self.output_dir / 'mvp_improvement_plan.md'\n        \n        lines = [\n            '# MVP Improvement Plan',\n            '',\n            f'**Generated:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}',\n            f'**Profile:** {profile_name}',\n            '',\n            '---',\n            '',\n            '## PRD Summary',\n            '',\n            self._extract_prd_summary(prd_content),\n            '',\n            '---',\n            ''\n        ]\n        \n        # Add stage summaries\n        lines.append('## Analysis Summaries')\n        lines.append('')\n        for sr in stage_results:\n            lines.append(f'### {sr.stage_name}')\n            lines.append('')\n            lines.append(sr.result.summary)\n            lines.append('')\n            if sr.result.recommendations:\n                lines.append('**Recommendations:**')\n                for rec in sr.result.recommendations:\n                    lines.append(f'- {rec}')\n                lines.append('')\n        \n        # Aggregate and prioritize tasks\n        lines.append('---')\n        lines.append('')\n        lines.append('## Prioritized Task List')\n        lines.append('')\n        lines.append('Complete tasks in order. Check off as completed.')\n        lines.append('')\n        \n        all_tasks = self._aggregate_tasks(stage_results)\n        for i, task in enumerate(all_tasks, 1):\n            priority_badge = self._priority_badge(task.get('priority', 'medium'))\n            lines.append(f'- [ ] **{i}. {task[\"title\"]}** {priority_badge}')\n            lines.append(f'  - {task[\"description\"]}')\n            if task.get('files'):\n                lines.append(f'  - Files: {\", \".join(task[\"files\"])}')\n            lines.append('')\n        \n        # Claude Code instruction block\n        lines.extend(self._claude_code_instructions())\n        \n        output_path.write_text('\\n'.join(lines), encoding='utf-8')\n        return output_path\n    \n    def _extract_prd_summary(self, prd_content: str) -> str:\n        \"\"\"Extract first 500 chars as summary.\"\"\"\n        lines = prd_content.strip().split('\\n')\n        summary_lines = []\n        char_count = 0\n        for line in lines:\n            if char_count + len(line) > 500:\n                break\n            summary_lines.append(line)\n            char_count += len(line)\n        return '\\n'.join(summary_lines) + '...'\n    \n    def _aggregate_tasks(self, stage_results: list[StageResult]) -> list[dict]:\n        \"\"\"Aggregate tasks from all stages, sorted by priority.\"\"\"\n        all_tasks = []\n        for sr in stage_results:\n            all_tasks.extend(sr.result.tasks)\n        \n        priority_order = {'high': 0, 'medium': 1, 'low': 2}\n        return sorted(all_tasks, key=lambda t: priority_order.get(t.get('priority', 'medium'), 1))\n    \n    def _priority_badge(self, priority: str) -> str:\n        badges = {'high': '🔴', 'medium': '🟡', 'low': '🟢'}\n        return badges.get(priority, '🟡')\n    \n    def _claude_code_instructions(self) -> list[str]:\n        return [\n            '---',\n            '',\n            '## Instructions for Claude Code',\n            '',\n            '1. Read this entire document before starting',\n            '2. Work through tasks in order, checking off each as completed',\n            '3. Run tests after each significant change',\n            '4. Commit changes incrementally with descriptive messages',\n            '5. If blocked, document the blocker and move to the next task',\n            ''\n        ]\n```",
        "testStrategy": "Unit tests in `tests/test_plan_writer.py`:\n1. Test write_plan() creates file at expected path\n2. Test output contains PRD summary section\n3. Test output contains stage summaries for all stages\n4. Test tasks are aggregated and sorted by priority\n5. Test Claude Code instruction block is included\n6. Test _priority_badge() returns correct emojis\n7. Test with empty stage_results list",
        "priority": "medium",
        "dependencies": [
          "1",
          "2",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define StageResult dataclass and PlanWriter constructor",
            "description": "Implement the basic class structure with StageResult dataclass and PlanWriter __init__ that creates output directory.",
            "dependencies": [],
            "details": "Create src/metaagent/plan_writer.py with imports (Path, dataclass, datetime, AnalysisResult), define StageResult with stage_name, prompt_id, result fields, and PlanWriter __init__ that sets self.output_dir and calls mkdir(parents=True, exist_ok=True).",
            "status": "pending",
            "testStrategy": "Test StageResult instantiation with sample data and PlanWriter __init__ creates directory using tmp_path fixture.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement write_plan() method with Markdown sections",
            "description": "Create the main write_plan method that assembles metadata, PRD summary, stage summaries, prioritized tasks, and Claude instructions into mvp_improvement_plan.md.",
            "dependencies": [
              1
            ],
            "details": "Implement write_plan(prd_content, stage_results, profile_name) that builds lines list with header, timestamp/profile metadata, PRD summary call, stage summaries loop, prioritized tasks section with _aggregate_tasks call, and _claude_code_instructions, then writes UTF-8 file.",
            "status": "pending",
            "testStrategy": "Test file creation at expected path, verify all sections present in output (header, metadata, PRD summary, stage summaries, tasks, instructions) using file content assertions.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement _extract_prd_summary with line-aware truncation",
            "description": "Create helper method to extract first ~500 characters of PRD content while preserving complete lines and adding ellipsis.",
            "dependencies": [
              1
            ],
            "details": "Implement _extract_prd_summary(prd_content) that splits by lines, accumulates until 500 char limit, joins lines, appends '...' for truncation. Handle empty/whitespace PRD gracefully.",
            "status": "pending",
            "testStrategy": "Test truncation at ~500 chars preserves lines, test short content returns full, test empty PRD returns empty string or minimal output.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement task aggregation and priority utilities",
            "description": "Add _aggregate_tasks to collect/sort tasks by priority and _priority_badge for emoji mapping.",
            "dependencies": [
              1
            ],
            "details": "Implement _aggregate_tasks(stage_results) that flattens all tasks and sorts by priority_order={'high':0,'medium':1,'low':2}, _priority_badge(priority) mapping to emojis (🔴🟡🟢). Ensure UTF-8 compatibility for emojis.",
            "status": "pending",
            "testStrategy": "Test task aggregation collects from multiple stages, verify priority sorting (high>medium>low), test badge emojis render correctly in UTF-8 output.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write comprehensive unit tests for PlanWriter",
            "description": "Create tests/test_plan_writer.py with pytest tests covering file creation, content sections, priority sorting, edge cases.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Use tmp_path fixture to test: file creation/path, all Markdown sections present, stage summaries rendered, tasks sorted by priority, empty tasks/stages handled, UTF-8 emoji encoding, PRD truncation. Mock AnalysisResult and StageResult.",
            "status": "pending",
            "testStrategy": "Run pytest with coverage: verify 100% pass rate, test empty inputs (no stages/no tasks), mixed priorities, long PRD truncation, Unicode emoji preservation in output file.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down Task 6 (Implement Plan Writer) into 5 subtasks: (1) define StageResult dataclass and PlanWriter constructor creating output_dir; (2) implement write_plan() to assemble sections (metadata, PRD summary, stage summaries, prioritized tasks, instructions) into Markdown; (3) implement _extract_prd_summary() with length limits and line-aware truncation; (4) implement _aggregate_tasks() and _priority_badge() with clear priority ordering and emoji mapping; (5) write unit tests using tmp_path to verify file creation, content sections, priority sorting, and behavior with empty tasks/stages. Note any i18n/encoding considerations for emojis and UTF-8 writes.",
        "updatedAt": "2025-12-14T02:00:13.300Z"
      },
      {
        "id": "7",
        "title": "Implement Orchestrator",
        "description": "Create the orchestrator.py module that coordinates the entire refinement workflow: loading config, running stages, calling analysis engine, and generating the plan.",
        "details": "Implement `src/metaagent/orchestrator.py`:\n\n```python\nimport logging\nfrom pathlib import Path\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\n\nfrom .config import Config\nfrom .prompts import PromptLibrary, Prompt\nfrom .repomix import RepomixRunner, RepomixResult\nfrom .analysis import create_analysis_engine, AnalysisResult\nfrom .plan_writer import PlanWriter, StageResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RunHistory:\n    \"\"\"Tracks analysis history for context in subsequent stages.\"\"\"\n    entries: list[dict] = field(default_factory=list)\n    \n    def add(self, stage: str, summary: str):\n        self.entries.append({\n            'stage': stage,\n            'timestamp': datetime.now().isoformat(),\n            'summary': summary\n        })\n    \n    def to_string(self) -> str:\n        if not self.entries:\n            return 'No previous analysis.'\n        lines = ['Previous analysis results:']\n        for entry in self.entries:\n            lines.append(f\"\\n## {entry['stage']} ({entry['timestamp']})\")\n            lines.append(entry['summary'])\n        return '\\n'.join(lines)\n\n@dataclass  \nclass RefinementResult:\n    success: bool\n    plan_path: Path | None\n    stage_results: list[StageResult]\n    errors: list[str]\n\nclass Orchestrator:\n    def __init__(self, config: Config, use_mock: bool = False):\n        self.config = config\n        self.use_mock = use_mock\n        self.prompt_library = PromptLibrary(config.config_dir)\n        self.repomix = RepomixRunner(timeout=config.timeout)\n        self.analysis_engine = create_analysis_engine(\n            config.perplexity_api_key,\n            use_mock=use_mock\n        )\n        self.plan_writer = PlanWriter(config.output_dir)\n        self.history = RunHistory()\n    \n    def refine(self, profile_name: str) -> RefinementResult:\n        \"\"\"\n        Run the full refinement workflow for the given profile.\n        \n        Args:\n            profile_name: Name of the profile to use\n            \n        Returns:\n            RefinementResult with success status and plan path\n        \"\"\"\n        errors = []\n        stage_results = []\n        \n        # Validate profile exists\n        profile = self.prompt_library.get_profile(profile_name)\n        if not profile:\n            available = self.prompt_library.list_profiles()\n            return RefinementResult(\n                success=False,\n                plan_path=None,\n                stage_results=[],\n                errors=[f'Profile \"{profile_name}\" not found. Available: {available}']\n            )\n        \n        logger.info(f'Starting refinement with profile: {profile_name}')\n        \n        # Load PRD\n        prd_content = self._load_prd()\n        if not prd_content:\n            return RefinementResult(\n                success=False,\n                plan_path=None,\n                stage_results=[],\n                errors=[f'PRD not found at {self.config.prd_path}']\n            )\n        \n        # Run Repomix\n        logger.info('Packing codebase with Repomix...')\n        repomix_result = self.repomix.pack(self.config.repo_path)\n        if not repomix_result.success:\n            errors.append(f'Repomix warning: {repomix_result.error}')\n            code_context = '[Repomix failed - limited code context available]'\n        else:\n            code_context = self.repomix.truncate_content(\n                repomix_result.content,\n                self.config.max_tokens\n            )\n        \n        # Run each stage\n        prompts = self.prompt_library.get_prompts_for_profile(profile_name)\n        for prompt in prompts:\n            logger.info(f'Running stage: {prompt.id}')\n            try:\n                result = self._run_stage(prompt, prd_content, code_context)\n                stage_results.append(StageResult(\n                    stage_name=prompt.id,\n                    prompt_id=prompt.id,\n                    result=result\n                ))\n                self.history.add(prompt.id, result.summary)\n            except Exception as e:\n                logger.error(f'Stage {prompt.id} failed: {e}')\n                errors.append(f'Stage {prompt.id} failed: {str(e)}')\n        \n        # Generate plan\n        if stage_results:\n            logger.info('Generating improvement plan...')\n            plan_path = self.plan_writer.write_plan(\n                prd_content,\n                stage_results,\n                profile_name\n            )\n            logger.info(f'Plan written to {plan_path}')\n        else:\n            plan_path = None\n            errors.append('No stages completed successfully')\n        \n        return RefinementResult(\n            success=len(errors) == 0,\n            plan_path=plan_path,\n            stage_results=stage_results,\n            errors=errors\n        )\n    \n    def _load_prd(self) -> str | None:\n        if self.config.prd_path.exists():\n            return self.config.prd_path.read_text(encoding='utf-8')\n        return None\n    \n    def _run_stage(self, prompt: Prompt, prd: str, code_context: str) -> AnalysisResult:\n        rendered = prompt.render(\n            prd=prd,\n            code_context=code_context,\n            history=self.history.to_string(),\n            current_stage=prompt.id\n        )\n        return self.analysis_engine.analyze(rendered)\n```",
        "testStrategy": "Unit tests in `tests/test_orchestrator.py`:\n1. Test refine() with valid profile returns success\n2. Test refine() with invalid profile returns error\n3. Test refine() with missing PRD returns error\n4. Test refine() continues after Repomix failure with warning\n5. Test stages are run in profile order\n6. Test history is accumulated across stages\n7. Test plan is generated after successful stages\n8. Use MockAnalysisEngine and mock Repomix for isolation",
        "priority": "high",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define RunHistory and RefinementResult dataclasses",
            "description": "Implement RunHistory class with add() and to_string() methods, and RefinementResult dataclass for workflow results.",
            "dependencies": [],
            "details": "Use @dataclass with field(default_factory=list) for RunHistory.entries. Implement timestamped history tracking and string formatting for prompt context. RefinementResult holds success flag, plan_path, stage_results list, and errors list.",
            "status": "pending",
            "testStrategy": "Test RunHistory.add() appends correctly, to_string() formats multi-entry history, empty history returns 'No previous analysis.' Test RefinementResult instantiation and field access.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Orchestrator.__init__ dependency wiring",
            "description": "Wire Config, PromptLibrary, RepomixRunner, analysis_engine factory, PlanWriter, and RunHistory in constructor.",
            "dependencies": [
              1
            ],
            "details": "Initialize self.config, self.prompt_library(config.config_dir), self.repomix(config.timeout), self.analysis_engine=create_analysis_engine(config.perplexity_api_key, use_mock), self.plan_writer(config.output_dir), self.history=RunHistory(). Support use_mock flag.",
            "status": "pending",
            "testStrategy": "Mock all dependencies, verify __init__ passes correct params to each component, test use_mock=True creates mock analysis engine.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement _load_prd() with file existence check",
            "description": "Load PRD content from config.prd_path with clear None return on missing file.",
            "dependencies": [
              2
            ],
            "details": "Use self.config.prd_path.exists() check, read_text(encoding='utf-8') on success, return None on failure. No exceptions, clean failure path for orchestrator.",
            "status": "pending",
            "testStrategy": "Mock Path.exists()=False returns None, mock Path.read_text() returns content, verify encoding handling.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Repomix integration in refine()",
            "description": "Run repomix.pack(), handle failure with warning, truncate content if successful.",
            "dependencies": [
              2,
              3
            ],
            "details": "Call self.repomix.pack(self.config.repo_path), on failure set code_context='[Repomix failed...]', on success truncate with self.repomix.truncate_content(result.content, self.config.max_tokens). Log progress.",
            "status": "pending",
            "testStrategy": "Mock repomix.pack() success with long content (verify truncation), mock failure (verify warning context), verify logging calls.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement stage execution loop with history",
            "description": "Fetch prompts for profile, render each with history/PRD/code_context, run analysis, accumulate StageResult, update history.",
            "dependencies": [
              1,
              2,
              4
            ],
            "details": "Get prompts = self.prompt_library.get_prompts_for_profile(profile_name), loop: render=prompt.render(prd, code_context, history=self.history.to_string(), current_stage=prompt.id), result=self.analysis_engine.analyze(rendered), create StageResult, self.history.add(). Catch exceptions per stage.",
            "status": "pending",
            "testStrategy": "Mock prompt_library.get_prompts_for_profile() returns 2 prompts, verify sequential execution, history updates after each stage, StageResult list accumulates correctly, partial failure continues.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement plan generation and RefinementResult",
            "description": "Generate plan if stages succeeded, assemble final RefinementResult with success/errors semantics.",
            "dependencies": [
              1,
              2,
              5
            ],
            "details": "After stage loop, if stage_results: plan_path=self.plan_writer.write_plan(prd_content, stage_results, profile_name), else plan_path=None. Return RefinementResult(success=len(errors)==0, plan_path, stage_results, errors). Early returns for profile/PRD validation.",
            "status": "pending",
            "testStrategy": "Mock plan_writer.write_plan() returns Path, verify success=True when errors=[], success=False with errors, test no-stages-empty-results case.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Write comprehensive unit tests for Orchestrator",
            "description": "Create tests/test_orchestrator.py with heavy mocking covering all paths: success, missing profile/PRD, Repomix/stage failures.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Use pytest, unittest.mock: Mock PromptLibrary.get_profile()/get_prompts_for_profile(), RepomixRunner.pack(), create_analysis_engine(), PlanWriter.write_plan(). Test 10+ scenarios: valid profile success path, invalid profile early return, missing PRD, Repomix fail continues, stage exceptions accumulate, history chaining, empty profile prompts.",
            "status": "pending",
            "testStrategy": "Aim for 90%+ coverage. Verify orchestration logic isolation from I/O. Test history.to_string() called sequentially, StageResult.prompt_id==stage_name consistency, error accumulation without crashing.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Break down Task 7 (Implement Orchestrator) into 7 subtasks: (1) define RunHistory and RefinementResult data structures; (2) implement Orchestrator.__init__ wiring Config, PromptLibrary, RepomixRunner, AnalysisEngine factory, and PlanWriter; (3) implement _load_prd() with clear failure behavior; (4) implement Repomix integration inside refine(), including warning handling and context truncation; (5) implement stage loop: fetching prompts for a profile, rendering with history, invoking analysis, accumulating StageResult, and updating RunHistory; (6) implement plan generation and final RefinementResult assembly with success/error semantics; (7) write unit tests that heavily mock PromptLibrary, RepomixRunner, and AnalysisEngine to cover success path, missing profile, missing PRD, Repomix failures, partial stage failures, history accumulation, and no-stages cases. Emphasize separation of orchestration logic from I/O for testability.",
        "updatedAt": "2025-12-14T02:00:17.617Z"
      },
      {
        "id": "8",
        "title": "Implement CLI Entrypoint",
        "description": "Create the CLI using Typer with the `metaagent refine` command that validates arguments and invokes the orchestrator.",
        "details": "Implement `src/metaagent/cli.py`:\n\n```python\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Optional\n\nimport typer\nfrom rich.console import Console\nfrom rich.logging import RichHandler\n\nfrom .config import Config\nfrom .orchestrator import Orchestrator\nfrom .prompts import PromptLibrary\n\napp = typer.Typer(\n    name='metaagent',\n    help='Meta-agent for automated codebase refinement from v0 to MVP'\n)\nconsole = Console()\n\ndef setup_logging(level: str):\n    logging.basicConfig(\n        level=level,\n        format='%(message)s',\n        handlers=[RichHandler(rich_tracebacks=True)]\n    )\n\n@app.command()\ndef refine(\n    profile: str = typer.Option(\n        ...,\n        '--profile', '-p',\n        help='Profile to use for refinement (e.g., automation_agent, backend_service)'\n    ),\n    repo: Path = typer.Option(\n        Path('.'),\n        '--repo', '-r',\n        help='Path to the repository to refine'\n    ),\n    prd: Optional[Path] = typer.Option(\n        None,\n        '--prd',\n        help='Path to PRD file (default: docs/prd.md in repo)'\n    ),\n    mock: bool = typer.Option(\n        False,\n        '--mock',\n        help='Use mock analysis engine (no API calls)'\n    ),\n    verbose: bool = typer.Option(\n        False,\n        '--verbose', '-v',\n        help='Enable verbose output'\n    )\n):\n    \"\"\"\n    Refine a codebase from v0 to MVP using automated analysis and planning.\n    \n    Example:\n        metaagent refine --profile automation_agent --repo /path/to/repo\n    \"\"\"\n    # Setup logging\n    log_level = 'DEBUG' if verbose else 'INFO'\n    setup_logging(log_level)\n    \n    # Resolve paths\n    repo_path = repo.resolve()\n    if not repo_path.exists():\n        console.print(f'[red]Error: Repository path does not exist: {repo_path}[/red]')\n        raise typer.Exit(1)\n    \n    # Load config\n    config = Config.from_env(repo_path)\n    if prd:\n        config.prd_path = prd.resolve()\n    \n    # Validate config\n    errors = config.validate()\n    if errors:\n        for err in errors:\n            console.print(f'[red]Configuration error: {err}[/red]')\n        raise typer.Exit(1)\n    \n    # Show available profiles if requested profile doesn't exist\n    prompt_library = PromptLibrary(config.config_dir)\n    if not prompt_library.get_profile(profile):\n        available = prompt_library.list_profiles()\n        console.print(f'[red]Error: Profile \"{profile}\" not found.[/red]')\n        console.print(f'Available profiles: {available}')\n        raise typer.Exit(1)\n    \n    # Run refinement\n    console.print(f'[bold blue]Starting refinement...[/bold blue]')\n    console.print(f'  Profile: {profile}')\n    console.print(f'  Repository: {repo_path}')\n    console.print(f'  Mock mode: {mock}')\n    console.print()\n    \n    orchestrator = Orchestrator(config, use_mock=mock)\n    result = orchestrator.refine(profile)\n    \n    # Report results\n    if result.success:\n        console.print('[bold green]Refinement completed successfully![/bold green]')\n        console.print(f'Plan written to: {result.plan_path}')\n    else:\n        console.print('[bold yellow]Refinement completed with warnings:[/bold yellow]')\n        for err in result.errors:\n            console.print(f'  - {err}')\n        if result.plan_path:\n            console.print(f'Plan written to: {result.plan_path}')\n    \n    console.print(f'\\nStages completed: {len(result.stage_results)}')\n\n@app.command()\ndef list_profiles(\n    config_dir: Optional[Path] = typer.Option(\n        None,\n        '--config-dir', '-c',\n        help='Path to config directory'\n    )\n):\n    \"\"\"List available refinement profiles.\"\"\"\n    config = Config.from_env()\n    if config_dir:\n        config.config_dir = config_dir.resolve()\n    \n    prompt_library = PromptLibrary(config.config_dir)\n    profiles = prompt_library.list_profiles()\n    \n    if not profiles:\n        console.print('[yellow]No profiles found.[/yellow]')\n        return\n    \n    console.print('[bold]Available Profiles:[/bold]')\n    for name in profiles:\n        profile = prompt_library.get_profile(name)\n        console.print(f'  {name}: {profile.description}')\n        console.print(f'    Stages: {profile.stages}')\n\ndef main():\n    app()\n\nif __name__ == '__main__':\n    main()\n```\n\nAdd to `src/metaagent/__init__.py`:\n```python\n__version__ = '0.1.0'\n```",
        "testStrategy": "Unit tests in `tests/test_cli.py`:\n1. Test `refine` command with valid args using CliRunner\n2. Test `refine` command fails gracefully with invalid profile\n3. Test `refine` command fails gracefully with non-existent repo\n4. Test `list-profiles` command outputs available profiles\n5. Test `--mock` flag passes through to orchestrator\n6. Test `--verbose` flag sets logging level correctly\n7. Test help text is displayed with `--help`",
        "priority": "high",
        "dependencies": [
          "2",
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Typer app, main() entrypoint, and package __version__",
            "description": "Create the Typer application object, wire up the main() entrypoint compatible with the pyproject console_script, and expose the package version in __init__.py.",
            "dependencies": [],
            "details": "Implement src/metaagent/cli.py with a module-level typer.Typer instance (name='metaagent') and a main() function that calls app(), suitable for use as the console_script entrypoint. Ensure __init__.py defines __version__ = '0.1.0' and that the CLI module can be imported without side effects beyond defining commands. Confirm the structure aligns with expected project layout so that `metaagent` runs the Typer app when installed.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement logging setup using RichHandler with configurable levels",
            "description": "Add a reusable logging setup function that configures logging with RichHandler and supports INFO/DEBUG levels via a parameter.",
            "dependencies": [
              1
            ],
            "details": "In cli.py, implement setup_logging(level: str) that calls logging.basicConfig with RichHandler(rich_tracebacks=True) and a simple message format. Use the provided code skeleton as reference. Ensure that calling setup_logging with 'DEBUG' or 'INFO' correctly changes the global log level and that it does not re-add duplicate handlers on multiple invocations in typical CLI use. Prepare for verbose flag integration in the refine command.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement `refine` command with argument parsing and orchestrator integration",
            "description": "Create the refine Typer command that parses CLI options, resolves paths, loads and validates config, checks profile existence, calls the orchestrator, and reports results.",
            "dependencies": [
              1,
              2
            ],
            "details": "Define @app.command() refine(...) with options: --profile/-p (required str), --repo/-r (Path, default '.'), --prd (Optional[Path]), --mock (bool), and --verbose/-v (bool). Inside, set log_level to 'DEBUG' when verbose is True, else 'INFO', and call setup_logging. Resolve repo path; if it does not exist, print a red Rich error message and exit with typer.Exit(1). Call Config.from_env(repo_path) to load configuration, apply prd override if provided, and run config.validate(); on validation errors, print each as a Rich red configuration error and exit 1. Instantiate PromptLibrary with config.config_dir, verify the requested profile exists via get_profile(), and if missing, print a red error plus a list of available profiles before exiting 1. On success, log a blue starting message with profile, repo, and mock mode. Create Orchestrator(config, use_mock=mock), call orchestrator.refine(profile), and then print green success output with plan path on success, or yellow warnings listing each error and plan path when present. Always print the count of completed stages at the end.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement `list_profiles` command with optional config_dir override",
            "description": "Add the list_profiles Typer command that loads configuration, applies an optional config directory override, and prints available profiles using PromptLibrary.",
            "dependencies": [
              1,
              3
            ],
            "details": "Define @app.command() list_profiles(config_dir: Optional[Path] = Option(None, '--config-dir', '-c', ...)). Inside, call Config.from_env() with default parameters, and if config_dir is provided, resolve it and assign to config.config_dir. Instantiate PromptLibrary(config.config_dir), call list_profiles(), and if the list is empty print a yellow notice and return without error. Otherwise, print a bold header and for each profile name, fetch the profile object with get_profile(name) and print its description and stages in a readable, indented format using Rich markup. Ensure this command exits with code 0 on normal completion.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Ensure consistent CLI error handling and exit codes with Rich messages",
            "description": "Review and refine CLI error handling so that all failure scenarios return appropriate exit codes and user-friendly Rich-formatted messages.",
            "dependencies": [
              3,
              4
            ],
            "details": "Audit refine and list_profiles commands to make sure all early-return error states use typer.Exit with non-zero codes (e.g., 1) for invalid repo paths, configuration validation failures, missing profiles, and other user errors, while successful paths exit with 0. Standardize Rich output styles for errors (red), warnings (yellow), and success (green/blue). Confirm no unhandled exceptions leak to the user in common failure scenarios, and that messages remain concise and informative across platforms. Avoid using sys.exit directly; rely on typer.Exit for consistency.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Write CLI tests with typer.testing.CliRunner covering success and failure paths",
            "description": "Create comprehensive tests for the CLI using CliRunner to exercise refine and list_profiles commands across success, failure, mock, verbose, and help scenarios without hitting real external APIs or subprocesses.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "In tests/test_cli.py, instantiate CliRunner and write tests that invoke the Typer app (e.g., runner.invoke(cli.app, [...])). Cover: (a) successful refine with valid args, mocking Config.from_env, PromptLibrary, and Orchestrator to avoid real file system, APIs, or subprocesses; (b) refine with invalid profile, ensuring proper error message and non-zero exit code; (c) refine with non-existent repo path, checking path handling is cross-platform safe (e.g., using tmp_path / 'missing'); (d) refine with --mock and --verbose ensuring they alter behavior/log level and output; (e) list_profiles showing available profiles, using mocked PromptLibrary; and (f) help text for top-level and refine/list_profiles commands. Use monkeypatch or similar to inject test doubles, and assert output strings and exit codes without relying on actual environment variables or network calls.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down Task 8 (Implement CLI Entrypoint) into 6 subtasks: (1) set up Typer app structure and main() entrypoint compatible with pyproject console_script; (2) implement logging setup with RichHandler and configurable levels; (3) implement refine command: argument parsing, repo path resolution, Config.from_env usage, optional PRD override, config validation, profile existence check, orchestrator invocation, and result reporting; (4) implement list_profiles command: config_dir override, PromptLibrary usage, and formatted output; (5) ensure CLI error handling exits with proper codes and user-friendly Rich messages; (6) write tests using typer.testing.CliRunner that exercise success and failure paths, mock mode, verbose flag, invalid repo/profile, and help text. Note cross-platform path considerations and how to avoid hitting real APIs/subprocesses in tests.",
        "updatedAt": "2025-12-14T02:00:22.408Z"
      },
      {
        "id": "9",
        "title": "Create Complete Configuration Files",
        "description": "Populate config/prompts.yaml and config/profiles.yaml with all prompt templates and profile definitions from the PRD.",
        "details": "Create `config/prompts.yaml` with exact content from PRD Section 7.1:\n\n```yaml\nprompts:\n  alignment_with_prd:\n    id: alignment_with_prd\n    goal: \"Identify gaps between current implementation and PRD requirements\"\n    stage: alignment\n    template: |\n      You are analyzing a codebase against its PRD.\n\n      ## PRD:\n      {{prd}}\n\n      ## Current Codebase:\n      {{code_context}}\n\n      ## Previous Analysis (if any):\n      {{history}}\n\n      Current Stage: {{current_stage}}\n\n      Please analyze and provide:\n      1. Summary of alignment gaps\n      2. Missing features or incomplete implementations\n      3. Prioritized task list to close gaps\n\n      Format your response as JSON with keys: summary, recommendations, tasks\n\n  architecture_sanity:\n    id: architecture_sanity\n    goal: \"Review architecture for best practices and maintainability\"\n    stage: architecture\n    template: |\n      Review this codebase for architectural quality.\n\n      ## PRD Context:\n      {{prd}}\n\n      ## Codebase:\n      {{code_context}}\n\n      Analyze:\n      1. Code organization and modularity\n      2. Separation of concerns\n      3. Error handling patterns\n      4. Dependency management\n\n      Format your response as JSON with keys: summary, recommendations, tasks\n\n  core_flow_hardening:\n    id: core_flow_hardening\n    goal: \"Identify robustness improvements for core flows\"\n    stage: hardening\n    template: |\n      Analyze core flows for robustness.\n\n      ## PRD:\n      {{prd}}\n\n      ## Codebase:\n      {{code_context}}\n\n      ## Analysis History:\n      {{history}}\n\n      Focus on:\n      1. Error handling and recovery\n      2. Retry logic for external calls\n      3. Input validation\n      4. Edge cases\n\n      Format your response as JSON with keys: summary, recommendations, tasks\n\n  test_suite_mvp:\n    id: test_suite_mvp\n    goal: \"Identify critical tests needed for MVP quality\"\n    stage: testing\n    template: |\n      Review test coverage for this codebase.\n\n      ## PRD:\n      {{prd}}\n\n      ## Codebase:\n      {{code_context}}\n\n      Identify:\n      1. Missing unit tests for core functions\n      2. Missing integration tests for main flows\n      3. Edge cases without test coverage\n\n      Format your response as JSON with keys: summary, recommendations, tasks\n```\n\nCreate `config/profiles.yaml` with content from PRD Section 7.2:\n\n```yaml\nprofiles:\n  automation_agent:\n    name: \"Automation Agent\"\n    description: \"Profile for CLI tools and automation agents\"\n    stages:\n      - alignment_with_prd\n      - architecture_sanity\n      - core_flow_hardening\n      - test_suite_mvp\n\n  backend_service:\n    name: \"Backend Service\"\n    description: \"Profile for API backends and services\"\n    stages:\n      - alignment_with_prd\n      - architecture_sanity\n      - core_flow_hardening\n      - test_suite_mvp\n\n  internal_tool:\n    name: \"Internal Tool\"\n    description: \"Profile for internal developer tools\"\n    stages:\n      - alignment_with_prd\n      - core_flow_hardening\n```",
        "testStrategy": "Validation tests:\n1. Validate prompts.yaml is valid YAML and parses correctly\n2. Validate profiles.yaml is valid YAML and parses correctly\n3. Test PromptLibrary loads all 4 prompts\n4. Test PromptLibrary loads all 3 profiles\n5. Test each prompt template renders without Jinja errors\n6. Verify all stages referenced in profiles exist in prompts",
        "priority": "medium",
        "dependencies": [
          "1",
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Author config/prompts.yaml with PRD templates",
            "description": "Create prompts.yaml file with all 4 prompt templates from PRD Section 7.1 including required Jinja placeholders",
            "dependencies": [],
            "details": "Copy exact content for alignment_with_prd, architecture_sanity, core_flow_hardening, test_suite_mvp prompts. Ensure all templates include {{prd}}, {{code_context}}, {{history}}, {{current_stage}} placeholders where appropriate. Verify YAML indentation and structure.",
            "status": "pending",
            "testStrategy": "Validate YAML parses correctly, check all 4 prompts load via PromptLibrary, test Jinja rendering with sample context for each template",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Author config/profiles.yaml with complete profiles",
            "description": "Create profiles.yaml with all 3 profiles and their stage sequences from PRD Section 7.2",
            "dependencies": [
              1
            ],
            "details": "Implement automation_agent, backend_service, internal_tool profiles with exact stage sequences. Ensure all referenced stage IDs exist in prompts.yaml (alignment_with_prd, architecture_sanity, core_flow_hardening, test_suite_mvp). Keep configs environment-agnostic.",
            "status": "pending",
            "testStrategy": "Validate YAML parses correctly, verify PromptLibrary loads all 3 profiles, check referential integrity between profiles.stages and prompts keys",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement validation tests for config files",
            "description": "Write tests/test_prompts.py to validate both YAML files load correctly and maintain integrity",
            "dependencies": [
              1,
              2
            ],
            "details": "Create pytest suite that: 1) Loads both YAML files via PromptLibrary, 2) Asserts 4 prompts and 3 profiles found, 3) Tests all prompt templates render without Jinja errors using mock context, 4) Verifies profile stage references exist in prompts, 5) Uses relative paths for CI compatibility.",
            "status": "pending",
            "testStrategy": "Run pytest tests/test_prompts.py. Should pass: YAML parsing, prompt/profile counts, template renderability, referential integrity checks. Tests must work in CI without environment-specific paths.",
            "parentId": "undefined"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down Task 9 (Create Complete Configuration Files) into 3 subtasks: (1) author prompts.yaml exactly per PRD, adding required Jinja placeholders (prd, code_context, history, current_stage) and validate formatting; (2) author profiles.yaml with all profiles and stage sequences, ensuring all referenced stages exist in prompts.yaml; (3) write small validation tests or scripts (e.g., in tests/test_prompts.py) to load both YAML files via PromptLibrary and assert prompt/profile counts, renderability, and referential integrity. Note how to keep these configs environment-agnostic for CI.",
        "updatedAt": "2025-12-14T02:00:33.045Z"
      },
      {
        "id": "10",
        "title": "Add Comprehensive Test Suite",
        "description": "Create a complete test suite covering all modules with unit tests, integration tests, and fixtures for testing.",
        "details": "Create comprehensive test files with pytest fixtures and mocks:\n\n`tests/conftest.py`:\n```python\nimport pytest\nfrom pathlib import Path\nimport tempfile\nimport shutil\n\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Create a temporary directory for tests.\"\"\"\n    d = Path(tempfile.mkdtemp())\n    yield d\n    shutil.rmtree(d, ignore_errors=True)\n\n@pytest.fixture\ndef sample_prd():\n    return '''# Test PRD\n## Overview\nThis is a test PRD for unit testing.\n## Requirements\n- REQ1: Feature one\n- REQ2: Feature two\n'''\n\n@pytest.fixture\ndef sample_code_context():\n    return '''# Packed Codebase\n## src/main.py\ndef main():\n    print(\"Hello World\")\n'''\n\n@pytest.fixture\ndef mock_prompts_yaml():\n    return '''prompts:\n  test_prompt:\n    id: test_prompt\n    goal: Test goal\n    stage: test\n    template: |\n      PRD: {{prd}}\n      Code: {{code_context}}\n'''\n\n@pytest.fixture\ndef mock_profiles_yaml():\n    return '''profiles:\n  test_profile:\n    name: Test Profile\n    description: For testing\n    stages:\n      - test_prompt\n'''\n\n@pytest.fixture\ndef config_dir(temp_dir, mock_prompts_yaml, mock_profiles_yaml):\n    \"\"\"Create a config directory with test YAML files.\"\"\"\n    config = temp_dir / 'config'\n    config.mkdir()\n    (config / 'prompts.yaml').write_text(mock_prompts_yaml)\n    (config / 'profiles.yaml').write_text(mock_profiles_yaml)\n    return config\n\n@pytest.fixture\ndef test_repo(temp_dir, sample_prd):\n    \"\"\"Create a test repository structure.\"\"\"\n    repo = temp_dir / 'test_repo'\n    repo.mkdir()\n    docs = repo / 'docs'\n    docs.mkdir()\n    (docs / 'prd.md').write_text(sample_prd)\n    (repo / 'src').mkdir()\n    (repo / 'src' / 'main.py').write_text('print(\"hello\")')\n    return repo\n```\n\nTest files to create:\n- `tests/test_config.py` - Config loading and validation\n- `tests/test_prompts.py` - Prompt/profile loading and rendering\n- `tests/test_repomix.py` - Repomix integration with mocked subprocess\n- `tests/test_analysis.py` - Analysis engine with mock and parsing\n- `tests/test_plan_writer.py` - Plan generation\n- `tests/test_orchestrator.py` - Full workflow with mocks\n- `tests/test_cli.py` - CLI commands with typer.testing.CliRunner\n\nEach test file should:\n- Use pytest fixtures from conftest.py\n- Mock external dependencies (subprocess, httpx)\n- Cover happy path and error cases\n- Be runnable with `pytest tests/`",
        "testStrategy": "Meta-test strategy:\n1. Run `pytest tests/ -v` to execute all tests\n2. Run `pytest tests/ --cov=metaagent --cov-report=html` for coverage\n3. Ensure >80% code coverage\n4. Verify all tests pass in CI environment (no API keys)\n5. Mark integration tests with @pytest.mark.integration\n6. Use `pytest -m \"not integration\"` for fast unit test runs",
        "priority": "medium",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create tests/conftest.py with shared fixtures",
            "description": "Implement conftest.py with fixtures for temp directories, sample PRD/code, mock YAML configs, and test repo structures.",
            "dependencies": [],
            "details": "Copy provided conftest.py code exactly. Add fixtures: temp_dir, sample_prd, sample_code_context, mock_prompts_yaml, mock_profiles_yaml, config_dir, test_repo. Ensure cleanup with shutil.rmtree.",
            "status": "pending",
            "testStrategy": "Run pytest tests/conftest.py -s to verify fixtures create/cleanup correctly without errors",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement tests/test_config.py",
            "description": "Create comprehensive tests for config.py covering env loading, defaults, validation, and path resolution.",
            "dependencies": [
              1
            ],
            "details": "Test Config loading from config_dir fixture, environment variable overrides, default values, path resolution with test_repo, validation errors for missing files/invalid YAML. Use parametrize for edge cases.",
            "status": "pending",
            "testStrategy": "pytest tests/test_config.py --cov=src/metaagent/config -v ensuring 90%+ coverage, happy path and validation errors pass",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement tests/test_prompts.py",
            "description": "Create tests for prompts.py module: loading YAML, rendering templates, error handling for missing/invalid files.",
            "dependencies": [
              1
            ],
            "details": "Test PromptLibrary loading from mock_prompts_yaml/profiles_yaml fixtures, Prompt.render() with jinja variables (prd, code_context, history, stage), get_prompts_for_profile(), list_profiles(), missing file errors.",
            "status": "pending",
            "testStrategy": "pytest tests/test_prompts.py -v verify rendering produces expected output strings, invalid YAML raises ValueError",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement tests/test_repomix.py with subprocess mocks",
            "description": "Test repomix.py with comprehensive subprocess.run mocking for success, failure, timeout, FileNotFoundError, truncation.",
            "dependencies": [
              1
            ],
            "details": "Use pytest mocker for subprocess.run. Test RepomixRunner with test_repo fixture, mock stdout/stderr/returncode, verify truncation logic, error handling paths, RepomixResult parsing.",
            "status": "pending",
            "testStrategy": "pytest tests/test_repomix.py::TestRepomix -v ensure all mock scenarios (0,1,timeout,FileNotFoundError) return expected RepomixResult objects",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement tests/test_analysis.py with httpx mocks",
            "description": "Test analysis.py: MockAnalysisEngine, create_analysis_engine(), _parse_response() with valid JSON and fallbacks.",
            "dependencies": [
              1
            ],
            "details": "Mock httpx.Client for real engine tests. Test mock mode returns deterministic AnalysisResult, parsing valid/invalid JSON responses, fallback parsing, use_mock=True/None api_key behavior.",
            "status": "pending",
            "testStrategy": "pytest tests/test_analysis.py -v verify MockAnalysisEngine.analyze() returns consistent AnalysisResult, parse_response handles malformed JSON gracefully",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement tests/test_plan_writer.py",
            "description": "Test plan_writer.py: file output generation, section formatting, priority sorting, edge case handling.",
            "dependencies": [
              1
            ],
            "details": "Use temp_dir fixture for output files. Test PlanWriter with sample AnalysisResult/tasks, verify markdown sections rendered correctly, priority sorting (high>medium>low), empty tasks edge case.",
            "status": "pending",
            "testStrategy": "pytest tests/test_plan_writer.py -v check generated markdown files match expected content via file.read_text()",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Implement orchestrator.py and CLI tests with coverage",
            "description": "Create tests/test_orchestrator.py and tests/test_cli.py. Add pytest markers, coverage config, CI-ready setup.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Test Orchestrator.refine() full workflow with mocks. Use typer.testing.CliRunner for CLI tests. Add pytest.ini with markers (slow, integration). Ensure no external deps. Mock all APIs/subprocess.",
            "status": "pending",
            "testStrategy": "pytest tests/ --cov=src/metaagent --cov-report=term-missing -m 'not slow' ensuring >80% coverage, all unit tests pass. Separate integration marker tests.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Break down Task 10 (Add Comprehensive Test Suite) into 7 subtasks: (1) create tests/conftest.py with shared fixtures for temp dirs, sample PRD/code, mock config YAMLs, and test repos; (2) implement tests for config.py (env loading, defaults, validation, path resolution); (3) implement tests for prompts.py (loading, rendering, missing files, invalid data); (4) implement tests for repomix.py with mocked subprocess.run covering success/failure/timeout/FileNotFoundError and truncation logic; (5) implement tests for analysis.py with mocked httpx, including mock engine and parse fallbacks; (6) implement tests for plan_writer.py (file output, sections, priority sorting, edge cases); (7) implement orchestrator and CLI tests using mocks/CliRunner, add coverage configuration, and mark slow/integration tests. Explicitly plan for deterministic, isolated tests suitable for CI with no external services or Node dependencies.",
        "updatedAt": "2025-12-14T02:00:39.501Z"
      },
      {
        "id": "11",
        "title": "Create Documentation and Environment Setup",
        "description": "Write README.md with installation instructions, usage examples, and update .env.example with all required environment variables.",
        "details": "Create `README.md`:\n\n```markdown\n# Meta-Agent\n\nA Python CLI tool for automated codebase refinement from v0 to MVP.\n\n## Overview\n\nMeta-Agent analyzes your codebase against a PRD and generates an improvement plan using AI-powered analysis. It integrates:\n- **Repomix** for codebase packing\n- **Perplexity API** for analysis and planning\n- Generates plans for **Claude Code** to implement\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone <repo-url>\ncd meta-agent\n\n# Install with uv (recommended)\nuv pip install -e .\n\n# Or with pip\npip install -e .\n```\n\n## Configuration\n\n1. Copy `.env.example` to `.env`\n2. Add your API keys:\n   - `PERPLEXITY_API_KEY` - Required for AI analysis\n   - `ANTHROPIC_API_KEY` - Optional, for Claude integration\n\n## Usage\n\n### Basic Usage\n\n```bash\n# Refine current directory with automation_agent profile\nmetaagent refine --profile automation_agent --repo .\n\n# Use mock mode for testing (no API calls)\nmetaagent refine --profile automation_agent --repo . --mock\n\n# List available profiles\nmetaagent list-profiles\n```\n\n### Profiles\n\n- `automation_agent` - For CLI tools and automation agents\n- `backend_service` - For API backends and services  \n- `internal_tool` - For internal developer tools\n\n### Output\n\nAfter running, find the improvement plan at `docs/mvp_improvement_plan.md`.\n\n## Development\n\n```bash\n# Install dev dependencies\nuv pip install -e \".[dev]\"\n\n# Run tests\npytest tests/ -v\n\n# Run with coverage\npytest tests/ --cov=metaagent --cov-report=html\n```\n\n## Project Structure\n\n```\nmeta-agent/\n├── src/metaagent/     # Main package\n│   ├── cli.py         # CLI entrypoint\n│   ├── orchestrator.py # Main workflow\n│   ├── repomix.py     # Repomix integration\n│   ├── prompts.py     # Prompt/profile loading\n│   ├── analysis.py    # LLM analysis engine\n│   └── plan_writer.py # Plan generation\n├── config/\n│   ├── prompts.yaml   # Prompt templates\n│   └── profiles.yaml  # Profile definitions\n└── tests/             # Test suite\n```\n\n## License\n\nMIT\n```\n\nUpdate `.env.example` for meta-agent:\n```bash\n# Meta-Agent Configuration\n\n# Required: Perplexity API key for analysis\nPERPLEXITY_API_KEY=\"pplx-...\"\n\n# Optional: Anthropic API key for Claude integration\nANTHROPIC_API_KEY=\"sk-ant-...\"\n\n# Optional: Logging and behavior\nMETAAGENT_LOG_LEVEL=INFO\nMETAAGENT_TIMEOUT=120\nMETAAGENT_MAX_TOKENS=100000\n```",
        "testStrategy": "Verification:\n1. README renders correctly in GitHub/GitLab\n2. Installation instructions work on clean environment\n3. All CLI examples in README work correctly\n4. .env.example contains all environment variables used in code\n5. No sensitive data in example files",
        "priority": "low",
        "dependencies": [
          "1",
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Write Comprehensive README.md",
            "description": "Create the full README.md file including overview, installation instructions for uv and pip, configuration steps, usage examples, profiles description, output details, development workflow, and project structure as specified.",
            "dependencies": [],
            "details": "Use the provided README template as base. Ensure all sections are complete: Overview with integrations (Repomix, Perplexity, Claude), Installation with git clone + uv/pip, Configuration with .env steps, Usage with basic/mock/list-profiles examples, Profiles list, Output location, Development with dev deps/tests/coverage, Project structure tree, and License. Make repo-url placeholder clear.",
            "status": "pending",
            "testStrategy": "Verify Markdown renders correctly in GitHub preview, check all code blocks are valid bash/python, ensure instructions match current CLI behavior from task dependencies.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create and Update .env.example File",
            "description": "Generate .env.example with all environment variables from Config.from_env including required/optional vars, safe placeholders, and explanatory comments.",
            "dependencies": [],
            "details": "Include PERPLEXITY_API_KEY (required), ANTHROPIC_API_KEY (optional), METAAGENT_LOG_LEVEL=INFO, METAAGENT_TIMEOUT=120, METAAGENT_MAX_TOKENS=100000. Match exactly Config dataclass fields: perplexity_api_key, anthropic_api_key, log_level, timeout, max_tokens. Add comments explaining purpose and defaults.",
            "status": "pending",
            "testStrategy": "Validate file is correct YAML-like format, cross-check against src/metaagent/config.py Config.from_env expected vars, ensure no real API keys, test loading with dotenv in clean env.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Validate Documentation in Clean Environment",
            "description": "Manually test README instructions and .env.example by following steps in a fresh environment, verify consistency with code and CLI behavior.",
            "dependencies": [
              1,
              2
            ],
            "details": "In clean dir: 1) Clone/follow install (uv/pip), 2) Copy .env.example to .env with dummy keys, 3) Run CLI examples (metaagent --help, list-profiles, refine --mock), 4) Check output/docs/mvp_improvement_plan.md generates, 5) Run dev commands (pytest). Note any discrepancies for future updates.",
            "status": "pending",
            "testStrategy": "Checklist: Installation succeeds without errors, all README CLI examples execute without crashes, .env vars load correctly per Config.from_env tests, docs match actual project structure/dependencies (tasks 1,2,8,9), no broken links/paths.",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down Task 11 (Create Documentation and Environment Setup) into 3 subtasks: (1) write README.md with clear overview, installation (uv and pip), configuration, usage examples, profiles description, output description, development workflow, and project structure; (2) create .env.example listing all relevant environment variables with safe placeholder values and comments; (3) manually validate docs by following the README in a clean environment and cross-checking that .env.example matches what Config.from_env expects and what the code actually uses. Note how to keep examples up to date with CLI and config behavior.",
        "updatedAt": "2025-12-14T02:00:46.066Z"
      },
      {
        "id": "12",
        "title": "End-to-End Integration Test with Mock Mode",
        "description": "Create an end-to-end test that exercises the complete workflow using mock mode, verifying the system generates a valid improvement plan.",
        "details": "Create `tests/test_e2e.py`:\n\n```python\nimport pytest\nfrom pathlib import Path\nfrom typer.testing import CliRunner\n\nfrom metaagent.cli import app\n\nrunner = CliRunner()\n\nclass TestEndToEnd:\n    \"\"\"End-to-end tests for the complete refinement workflow.\"\"\"\n    \n    @pytest.fixture\n    def sample_repo(self, tmp_path):\n        \"\"\"Create a complete sample repository for testing.\"\"\"\n        repo = tmp_path / 'sample_project'\n        repo.mkdir()\n        \n        # Create docs/prd.md\n        docs = repo / 'docs'\n        docs.mkdir()\n        (docs / 'prd.md').write_text('''\n# Sample Project PRD\n\n## Overview\nA simple calculator CLI application.\n\n## Requirements\n- FR1: Add two numbers\n- FR2: Subtract two numbers\n- FR3: Display help message\n\n## Success Criteria\n- All operations return correct results\n- Error handling for invalid input\n''')\n        \n        # Create source files\n        src = repo / 'src'\n        src.mkdir()\n        (src / 'calculator.py').write_text('''\ndef add(a, b):\n    return a + b\n\ndef subtract(a, b):\n    return a - b\n''')\n        (src / 'main.py').write_text('''\nfrom calculator import add, subtract\nimport sys\n\ndef main():\n    if len(sys.argv) < 4:\n        print(\"Usage: calc <add|sub> <a> <b>\")\n        return\n    op, a, b = sys.argv[1], int(sys.argv[2]), int(sys.argv[3])\n    if op == \"add\":\n        print(add(a, b))\n    elif op == \"sub\":\n        print(subtract(a, b))\n\nif __name__ == \"__main__\":\n    main()\n''')\n        \n        return repo\n    \n    def test_full_refinement_mock_mode(self, sample_repo):\n        \"\"\"Test complete refinement workflow in mock mode.\"\"\"\n        result = runner.invoke(app, [\n            'refine',\n            '--profile', 'automation_agent',\n            '--repo', str(sample_repo),\n            '--mock'\n        ])\n        \n        assert result.exit_code == 0\n        assert 'Refinement completed' in result.stdout\n        \n        # Verify plan was created\n        plan_path = sample_repo / 'docs' / 'mvp_improvement_plan.md'\n        assert plan_path.exists()\n        \n        plan_content = plan_path.read_text()\n        assert '# MVP Improvement Plan' in plan_content\n        assert 'automation_agent' in plan_content\n        assert 'Prioritized Task List' in plan_content\n        assert 'Instructions for Claude Code' in plan_content\n    \n    def test_refinement_with_internal_tool_profile(self, sample_repo):\n        \"\"\"Test refinement with internal_tool profile (fewer stages).\"\"\"\n        result = runner.invoke(app, [\n            'refine',\n            '--profile', 'internal_tool',\n            '--repo', str(sample_repo),\n            '--mock'\n        ])\n        \n        assert result.exit_code == 0\n        \n        plan_path = sample_repo / 'docs' / 'mvp_improvement_plan.md'\n        plan_content = plan_path.read_text()\n        \n        # internal_tool has fewer stages\n        assert 'alignment_with_prd' in plan_content\n        assert 'core_flow_hardening' in plan_content\n    \n    def test_refinement_fails_without_prd(self, tmp_path):\n        \"\"\"Test that refinement fails gracefully without PRD.\"\"\"\n        empty_repo = tmp_path / 'empty'\n        empty_repo.mkdir()\n        \n        result = runner.invoke(app, [\n            'refine',\n            '--profile', 'automation_agent',\n            '--repo', str(empty_repo),\n            '--mock'\n        ])\n        \n        # Should fail or warn about missing PRD\n        assert 'PRD' in result.stdout or result.exit_code != 0\n    \n    def test_list_profiles_command(self):\n        \"\"\"Test list-profiles command shows available profiles.\"\"\"\n        result = runner.invoke(app, ['list-profiles'])\n        \n        assert result.exit_code == 0\n        assert 'automation_agent' in result.stdout\n        assert 'backend_service' in result.stdout\n        assert 'internal_tool' in result.stdout\n```\n\nThis test validates:\n1. CLI invocation works correctly\n2. Mock mode bypasses API calls\n3. Plan file is generated with correct structure\n4. Different profiles produce appropriate output\n5. Error handling for missing PRD\n6. list-profiles command works",
        "testStrategy": "Run e2e tests:\n1. `pytest tests/test_e2e.py -v` to run all e2e tests\n2. Verify tests pass without any API keys configured\n3. Verify tests are isolated (use tmp_path fixtures)\n4. Verify generated plan files contain expected sections\n5. Run with `--tb=long` to debug any failures",
        "priority": "medium",
        "dependencies": [
          "7",
          "8",
          "9",
          "10"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and implement sample_repo fixture with minimal realistic project structure",
            "description": "Create a pytest fixture that builds a temporary sample repository with docs/prd.md and simple src code mirroring a realistic yet minimal project for end-to-end tests.",
            "dependencies": [],
            "details": "Implement the sample_repo fixture in tests/test_e2e.py using tmp_path to create an isolated directory tree (e.g., sample_project/docs/prd.md and src/*.py). Ensure prd.md contains PRD-style sections (Overview, Requirements, Success Criteria) and src includes a small but working CLI app (e.g., calculator main and helper module). Confirm paths match what metaagent refine expects (docs/prd.md and any default repo layout assumptions). Keep file contents minimal but sufficient for downstream PRD parsing and prompt rendering.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Write end-to-end tests invoking metaagent refine in mock mode for multiple profiles",
            "description": "Add pytest tests that use Typer’s CliRunner to invoke the metaagent CLI refine command in mock mode with different profiles, asserting exit codes and key output text.",
            "dependencies": [
              1
            ],
            "details": "In tests/test_e2e.py, use CliRunner (from typer.testing) to run app with commands like ['refine', '--profile', 'automation_agent', '--repo', str(sample_repo), '--mock']. Assert result.exit_code == 0 and that known success strings such as 'Refinement completed' (or equivalent success message) appear in result.stdout. Add a separate test for at least one additional profile (e.g., internal_tool) to ensure profile selection is wired correctly in the CLI and orchestrator. Ensure these tests rely only on mock mode (no network, no real API keys).",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Assert generation and structure of docs/mvp_improvement_plan.md for each profile",
            "description": "Verify that running refine in mock mode creates docs/mvp_improvement_plan.md and that the file contains required sections, headings, and profile-specific markers.",
            "dependencies": [
              2
            ],
            "details": "Extend the refine tests to compute plan_path = sample_repo / 'docs' / 'mvp_improvement_plan.md' and assert plan_path.exists(). Read the file and assert presence of required structural elements such as '# MVP Improvement Plan', a section describing a prioritized task list, and any expected headings like 'Prioritized Task List' and 'Instructions for Claude Code'. Also assert that the active profile name (e.g., 'automation_agent' or 'internal_tool') appears somewhere in the plan body so tests confirm profile-aware content. Keep expectations general enough that normal template copy changes do not cause brittle failures, but specific enough to validate correct plan template wiring.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add tests for failure scenarios and list-profiles CLI behavior",
            "description": "Create additional end-to-end tests covering graceful failure when PRD is missing and verifying that the list-profiles command reports all expected profiles.",
            "dependencies": [
              1,
              2
            ],
            "details": "Add a test that creates an empty temporary repo (e.g., empty_repo = tmp_path / 'empty') without docs/prd.md and runs refine in mock mode. Assert non-zero exit_code or that stdout contains a clear message mentioning missing PRD. Add a separate test invoking ['list-profiles'] and assert exit_code == 0 and that stdout contains all configured profile identifiers (e.g., automation_agent, backend_service, internal_tool). These tests should exercise the same CLI app object and use CliRunner, confirming both error handling paths and discovery/printing of available profiles.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Ensure e2e tests are isolated, fast, and optionally marked as integration tests",
            "description": "Harden the e2e test module by enforcing isolation with tmp_path fixtures, avoiding real network/API calls via mock mode, and optionally marking tests as integration for selective running.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Review tests/test_e2e.py to confirm all filesystem operations use tmp_path-based fixtures (no writes to the real project tree) and that refine is always invoked with --mock so the orchestrator never calls real external services. Optionally add a custom pytest marker like @pytest.mark.integration to the TestEndToEnd class or individual tests so they can be included/excluded via -m integration. Confirm that running pytest tests/test_e2e.py -v completes quickly and does not require any environment variables or API keys. Document any ordering dependencies on other tasks (e.g., requires CLI wiring, profiles/prompt config, and mock analysis path from tasks 7–10 to be implemented) in comments or task notes.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down Task 12 (End-to-End Integration Test with Mock Mode) into 5 subtasks: (1) design sample_repo fixture that mirrors a realistic but minimal project with docs/prd.md and simple src code; (2) write e2e tests using CliRunner to invoke `metaagent refine` in mock mode for different profiles, asserting exit codes and key output strings; (3) assert that docs/mvp_improvement_plan.md is created and contains required sections and profile info; (4) add tests for failure scenarios (e.g., missing PRD) and for list-profiles behavior; (5) ensure tests are isolated via tmp_path, run quickly using mock analysis (no network), and are optionally marked as integration tests. Highlight any ordering dependencies with other tasks/tests.",
        "updatedAt": "2025-12-14T02:00:52.678Z"
      },
      {
        "id": "13",
        "title": "Implement Repo-Agnostic Meta-Agent Architecture",
        "description": "Refactor the meta-agent to separate the meta-agent repository (CLI/configs) from the target repository (codebase being refined), making it fully repo-agnostic with configurable paths and restored batch mode profiling.",
        "details": "## Core Architecture Changes\n\n1. **CLI Refactoring** (`src/metaagent/cli.py`):\n   - Restore `--profile` option for batch mode (removed in prior versions)\n   - Add `--config-dir` option (default: `./config`) to specify prompt library location\n   - Add `--target-repo` option (default: current directory) for target codebase\n   - Add `--prd-path` option (default: `./docs/prd.md`) for PRD location\n   - Update usage: `metaagent refine --target-repo /path/to/project --config-dir ./config --profile batch --prd-path docs/prd.md`\n\n```python\n@app.command()\ndef refine(\n    target_repo: Path = typer.Argument(Path('.'), help=\"Target repository path\"),\n    config_dir: Optional[Path] = typer.Option(Path('./config'), '--config-dir', help=\"Meta-agent config directory\"),\n    prd_path: Optional[Path] = typer.Option(Path('docs/prd.md'), '--prd-path', help=\"PRD file path\"),\n    profile: str = typer.Option('default', '--profile', help=\"Profile name\"),\n    # ... other options\n):\n    config = Config.from_paths(config_dir=config_dir, target_repo=target_repo, prd_path=prd_path)\n```\n\n2. **Config System** (`src/metaagent/config.py`):\n   - Create `Config.from_paths(config_dir: Path, target_repo: Path, prd_path: Path)` factory\n   - Load prompts from `{config_dir}/prompts.yaml`\n   - Load profiles from `{config_dir}/profiles.yaml`\n   - Store `target_repo_path`, `config_dir_path`, `prd_path` as config attributes\n\n3. **Orchestrator Updates** (`src/metaagent/orchestrator.py`):\n   - Accept `Config` instance with repo-agnostic paths\n   - Run repomix/codebase-digest on `config.target_repo_path`\n   - Load PRD from `config.prd_path`\n   - Pass absolute paths to all tools/subprocesses\n\n4. **PromptLibrary** (`src/metaagent/prompts.py`):\n   - Constructor accepts `config_dir: Path`\n   - Load from `{config_dir}/prompts.yaml` instead of hardcoded `./config`\n\n5. **Tool Path Resolution**:\n   - All file operations use `config.target_repo_path / relative_path`\n   - Repomix: `repomix pack {config.target_repo_path} --output {temp_dir}/codebase.pack`\n   - Ensure all subprocess calls use absolute paths\n\n6. **Batch Mode Profile** (`config/profiles.yaml`):\n```yaml\nprofiles:\n  batch:\n    stages: ['alignment', 'planning', 'prioritization']\n    max_iterations: 1\n    auto_approve: true\n  interactive:\n    stages: ['alignment', 'planning', 'prioritization', 'execution']\n    max_iterations: 3\n    auto_approve: false\n```\n\n7. **Error Handling & Validation**:\n   - Validate `target_repo` exists and contains `docs/prd.md` (unless overridden)\n   - Validate `config_dir` contains required YAML files\n   - Clear error messages: \"Target repo not found: {target_repo}\"",
        "testStrategy": "## Comprehensive Test Strategy\n\n1. **Unit Tests** (`tests/test_repo_agnostic.py`):\n   - Test `Config.from_paths()` with various directory combinations\n   - Test `PromptLibrary(config_dir=tmp_config_dir)` loads correctly\n   - Test CLI argument parsing with all new options\n   \n2. **Integration Tests** (`tests/test_cli_repo_agnostic.py`):\n   ```python\n   def test_refine_separate_repos(tmp_path):\n       meta_config = tmp_path / 'meta-agent-config'\n       target_repo = tmp_path / 'sample-project'\n       # Setup both repos...\n       \n       result = runner.invoke(app, [\n           'refine',\n           f'--target-repo={target_repo}',\n           f'--config-dir={meta_config}',\n           '--profile=batch',\n           f'--prd-path={target_repo}/custom_prd.md'\n       ])\n       assert result.exit_code == 0\n   ```\n   \n3. **E2E Tests** (extend `tests/test_e2e.py`):\n   - Test complete workflow with separate meta-agent config dir\n   - Verify repomix runs on target repo only\n   - Verify PRD loaded from custom path\n   - Test `--profile batch` skips interactive stages\n   \n4. **Cross-Repo Validation**:\n   - Create isolated `tmp_path` fixtures for meta-config and target-repo\n   - Verify no file pollution between repositories\n   - Test with `--config-dir` in parent directory\n   \n5. **Edge Cases**:\n   - Missing target repo → clear error\n   - Invalid config dir → validation error\n   - Non-existent PRD path → error\n   - Relative/absolute paths\n   \n6. **Coverage & CI**:\n   - `pytest tests/ --cov=metaagent/config --cov=metaagent/cli`\n   - Ensure 90%+ coverage of new repo-agnostic code\n   - GitHub Actions matrix: different OS, Python versions",
        "status": "done",
        "dependencies": [
          "8",
          "9",
          "10",
          "12"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T03:04:39.134Z"
      },
      {
        "id": "14",
        "title": "Implement Iterative Triage Mode",
        "description": "Create an AI-driven triage system that replaces static profile-based prompt selection with dynamic prompt selection where Perplexity analyzes the codebase against the PRD and decides which 1-3 prompts to run per iteration.",
        "details": "## Overview\n\nThe iterative triage mode is partially implemented in `src/metaagent/orchestrator.py` (see `refine_iterative()` at line 256 and `_run_triage()` at line 437), but needs to be exposed via CLI and enhanced. The `meta_triage.md` prompt already exists at `config/prompt_library/meta_triage.md`.\n\n## Implementation Steps\n\n### 1. Add CLI Command for Iterative Mode\n\nUpdate `src/metaagent/cli.py` to add a new command or option:\n\n```python\n@app.command(\"refine-iterative\")\ndef refine_iterative(\n    repo: Path = typer.Option(\n        Path.cwd(),\n        \"--repo\",\n        \"-r\",\n        help=\"Path to the target repository to refine.\",\n    ),\n    prd: Optional[Path] = typer.Option(\n        None,\n        \"--prd\",\n        help=\"Path to PRD file (default: docs/prd.md in target repo).\",\n    ),\n    config_dir: Optional[Path] = typer.Option(\n        None,\n        \"--config-dir\",\n        \"-c\",\n        help=\"Path to meta-agent config directory containing prompts/profiles.\",\n    ),\n    max_iterations: int = typer.Option(\n        10,\n        \"--max-iterations\",\n        \"-n\",\n        help=\"Maximum number of triage iterations to run.\",\n    ),\n    mock: bool = typer.Option(\n        False,\n        \"--mock\",\n        \"-m\",\n        help=\"Run in mock mode (no API calls).\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\",\n        help=\"Enable verbose output.\",\n    ),\n) -> None:\n    \"\"\"Run iterative refinement with AI-driven triage.\n\n    Instead of running a fixed profile, this mode lets the AI decide which\n    prompts to run based on analyzing the codebase against the PRD.\n    \"\"\"\n    # Similar setup to refine() command\n    # Call orchestrator.refine_iterative(max_iterations)\n```\n\n### 2. Enhance TriageResult Data Class\n\nThe existing `TriageResult` at `orchestrator.py:51` is already well-structured:\n\n```python\n@dataclass\nclass TriageResult:\n    success: bool\n    done: bool = False\n    assessment: str = \"\"\n    priority_issues: list[str] = field(default_factory=list)\n    selected_prompts: list[str] = field(default_factory=list)\n    reasoning: str = \"\"\n    error: Optional[str] = None\n```\n\nConsider adding:\n- `confidence: float = 1.0` - AI's confidence in the selection\n- `iteration_context: str = \"\"` - Context about what was done in previous iterations\n\n### 3. Enhance the `_run_triage()` Method\n\nThe existing implementation at `orchestrator.py:437` parses JSON responses. Enhance it to:\n\n1. **Validate selected prompts exist** - Check each prompt_id against `self.prompt_library`\n2. **Limit to 1-3 prompts** - Enforce the prompt limit in code:\n```python\nif len(selected_prompts) > 3:\n    logger.warning(f\"Triage selected {len(selected_prompts)} prompts, limiting to 3\")\n    selected_prompts = selected_prompts[:3]\n```\n3. **Add fallback for common AI response variations** - Handle cases where AI returns prompt names slightly different from IDs\n\n### 4. Update the `meta_triage.md` Prompt\n\nThe current prompt at `config/prompt_library/meta_triage.md` is well-structured. Consider enhancing:\n\n1. Add the full list of available prompts dynamically from the prompt library\n2. Include previous iteration results in the context\n3. Add clearer termination criteria for the \"done\" flag\n\n### 5. Add Iteration Tracking to Output\n\nUpdate `RefinementResult` at `orchestrator.py:76` (already has `iterations: list[IterationResult]`) to include:\n- Which prompts were run each iteration\n- Why they were selected (from triage reasoning)\n- Whether changes were made\n\n### 6. Enhance Rich Console Output\n\nAdd iteration progress display in CLI:\n```python\nwith console.status(\"[bold blue]Running triage...\") as status:\n    for iteration in range(1, max_iterations + 1):\n        status.update(f\"[bold blue]Iteration {iteration}/{max_iterations}: Analyzing codebase...\")\n        # Display selected prompts\n        console.print(f\"[cyan]Selected prompts:[/cyan] {', '.join(triage_result.selected_prompts)}\")\n        console.print(f\"[dim]Reasoning:[/dim] {triage_result.reasoning}\")\n```\n\n### 7. Add MockAnalysisEngine Support for Triage\n\nUpdate `MockAnalysisEngine` in `src/metaagent/analysis.py` to return proper triage JSON responses when the prompt contains \"meta_triage\":\n\n```python\ndef analyze(self, prompt: str) -> AnalysisResult:\n    if \"meta_triage\" in prompt.lower() or \"triage\" in prompt.lower():\n        return AnalysisResult(\n            summary=json.dumps({\n                \"assessment\": \"Mock triage assessment\",\n                \"priority_issues\": [\"Mock issue 1\"],\n                \"selected_prompts\": [\"quality_error_analysis\"],\n                \"reasoning\": \"Mock reasoning for testing\",\n                \"done\": self.call_count >= 3  # Simulate done after 3 iterations\n            }),\n            success=True,\n        )\n    # ... rest of existing mock logic\n```\n\n## Files to Modify\n\n1. `src/metaagent/cli.py` - Add `refine-iterative` command\n2. `src/metaagent/orchestrator.py` - Enhance `_run_triage()` validation\n3. `src/metaagent/analysis.py` - Add triage-aware mock responses\n4. `config/prompt_library/meta_triage.md` - Optional enhancements to prompt\n5. `tests/test_cli.py` - Add tests for iterative mode CLI\n6. `tests/test_orchestrator.py` (new or existing) - Add tests for triage logic\n\n## Dependencies\n\nThis task depends on:\n- Task 7 (Orchestrator) - Provides the base `refine_iterative()` and `_run_triage()` methods\n- Task 5 (Analysis Engine) - Provides `AnalysisEngine` and `MockAnalysisEngine`\n- Task 3 (Prompt Loading) - Provides `PromptLibrary` for prompt validation\n- Task 8 (CLI) - Provides the CLI infrastructure to add new command",
        "testStrategy": "## Unit Tests\n\n### 1. Test CLI Command (`tests/test_cli.py`)\n```python\ndef test_refine_iterative_help():\n    result = runner.invoke(app, [\"refine-iterative\", \"--help\"])\n    assert result.exit_code == 0\n    assert \"--max-iterations\" in result.stdout\n\ndef test_refine_iterative_mock_mode(mock_config):\n    with patch(\"metaagent.cli.Orchestrator\") as mock_orch:\n        mock_instance = mock_orch.return_value\n        mock_instance.refine_iterative.return_value = RefinementResult(\n            success=True,\n            profile_name=\"iterative\",\n            stages_completed=3,\n            stages_failed=0,\n            iterations=[...],\n        )\n        result = runner.invoke(app, [\n            \"refine-iterative\",\n            \"--repo\", str(mock_config.repo_path),\n            \"--config-dir\", str(mock_config.config_dir),\n            \"--mock\",\n            \"--max-iterations\", \"5\",\n        ])\n        assert result.exit_code == 0\n        mock_instance.refine_iterative.assert_called_once_with(5)\n```\n\n### 2. Test Triage Result Parsing (`tests/test_orchestrator.py`)\n```python\ndef test_run_triage_parses_json_response():\n    # Mock analysis engine to return valid triage JSON\n    orchestrator = Orchestrator(config, analysis_engine=mock_engine)\n    result = orchestrator._run_triage(prd, code_context, history)\n    assert result.success\n    assert len(result.selected_prompts) <= 3\n    assert isinstance(result.done, bool)\n\ndef test_run_triage_validates_prompt_ids():\n    # Selected prompts should exist in prompt library\n    result = orchestrator._run_triage(...)\n    for prompt_id in result.selected_prompts:\n        assert orchestrator.prompt_library.get_prompt(prompt_id) is not None\n\ndef test_run_triage_handles_done_flag():\n    # When done=true, selected_prompts should be empty\n    mock_engine.responses[\"triage\"] = AnalysisResult(\n        summary='{\"done\": true, \"selected_prompts\": [], ...}',\n        success=True,\n    )\n    result = orchestrator._run_triage(...)\n    assert result.done is True\n    assert result.selected_prompts == []\n```\n\n### 3. Test Iterative Loop (`tests/test_orchestrator.py`)\n```python\ndef test_refine_iterative_stops_on_done():\n    # Should stop when triage returns done=true\n    orchestrator = Orchestrator(config, analysis_engine=mock_engine)\n    mock_engine.set_done_after_iterations(2)\n    result = orchestrator.refine_iterative(max_iterations=10)\n    assert result.success\n    assert len(result.iterations) == 2\n\ndef test_refine_iterative_respects_max_iterations():\n    # Should not exceed max_iterations\n    result = orchestrator.refine_iterative(max_iterations=3)\n    assert len(result.iterations) <= 3\n\ndef test_refine_iterative_tracks_prompts_per_iteration():\n    result = orchestrator.refine_iterative(max_iterations=5)\n    for iteration in result.iterations:\n        assert len(iteration.prompts_run) >= 1\n        assert len(iteration.prompts_run) <= 3\n```\n\n### 4. Test Mock Mode for Triage\n```python\ndef test_mock_engine_returns_triage_response():\n    engine = MockAnalysisEngine()\n    result = engine.analyze(\"... meta_triage ...\")\n    assert result.success\n    data = json.loads(result.summary)\n    assert \"selected_prompts\" in data\n    assert \"done\" in data\n```\n\n### 5. Integration Test\n```python\ndef test_full_iterative_workflow(mock_config, tmp_path):\n    \"\"\"End-to-end test of iterative refinement.\"\"\"\n    # Create target repo with PRD\n    target = tmp_path / \"target\"\n    target.mkdir()\n    (target / \"docs\").mkdir()\n    (target / \"docs\" / \"prd.md\").write_text(\"# Test PRD\")\n    (target / \"src\").mkdir()\n    (target / \"src\" / \"main.py\").write_text(\"def main(): pass\")\n\n    result = runner.invoke(app, [\n        \"refine-iterative\",\n        \"--repo\", str(target),\n        \"--config-dir\", str(mock_config.config_dir),\n        \"--mock\",\n        \"--max-iterations\", \"3\",\n    ])\n    assert result.exit_code == 0\n    assert \"iteration\" in result.stdout.lower()\n```\n\n## Manual Verification\n\n1. Run `metaagent refine-iterative --mock --max-iterations 3 --verbose` on meta-agent itself\n2. Verify triage output shows assessment, selected prompts, and reasoning\n3. Verify iteration progress is displayed clearly\n4. Verify loop terminates on \"done\" flag or max iterations\n5. Verify improvement plan includes all iteration results",
        "status": "done",
        "dependencies": [
          "3",
          "5",
          "7",
          "8"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-14T03:23:29.843Z"
      },
      {
        "id": "16",
        "title": "Implement Claude Code Integration",
        "description": "Replace the stub `_implement_with_claude()` method in orchestrator.py with actual Claude Code CLI integration via subprocess to automatically implement recommended changes from the analysis stage. Handle errors gracefully and report implementation status.",
        "details": "## Overview\n\nThe `_implement_with_claude()` method at `src/metaagent/orchestrator.py:510` currently only writes tasks to a file and returns True without actually invoking Claude Code. This task implements full subprocess integration with the Claude Code CLI.\n\n## Implementation Steps\n\n### 1. Create ClaudeCodeRunner Module (`src/metaagent/claude_runner.py`)\n\n```python\n\"\"\"Claude Code CLI integration for automated implementation.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport subprocess\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Optional, Any\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ClaudeCodeResult:\n    \"\"\"Result from a Claude Code execution.\"\"\"\n    \n    success: bool\n    output: str = \"\"\n    error: Optional[str] = None\n    files_modified: list[str] = field(default_factory=list)\n    exit_code: int = 0\n\n\nclass ClaudeCodeRunner:\n    \"\"\"Runs Claude Code CLI to implement changes in a repository.\"\"\"\n    \n    def __init__(\n        self,\n        timeout: int = 600,  # 10 minutes default for implementation\n        model: str = \"claude-sonnet-4-20250514\",\n        max_turns: int = 50,\n    ):\n        \"\"\"Initialize the Claude Code runner.\n        \n        Args:\n            timeout: Maximum time in seconds for Claude Code to run.\n            model: Claude model to use for implementation.\n            max_turns: Maximum conversation turns for the agentic loop.\n        \"\"\"\n        self.timeout = timeout\n        self.model = model\n        self.max_turns = max_turns\n    \n    def check_installed(self) -> bool:\n        \"\"\"Check if Claude Code CLI is installed and accessible.\n        \n        Returns:\n            True if claude command is available, False otherwise.\n        \"\"\"\n        try:\n            result = subprocess.run(\n                [\"claude\", \"--version\"],\n                capture_output=True,\n                text=True,\n                timeout=10,\n            )\n            return result.returncode == 0\n        except (FileNotFoundError, subprocess.TimeoutExpired):\n            return False\n    \n    def implement(\n        self,\n        repo_path: Path,\n        prompt: str,\n        plan_file: Optional[Path] = None,\n    ) -> ClaudeCodeResult:\n        \"\"\"Run Claude Code to implement changes.\n        \n        Args:\n            repo_path: Path to the target repository.\n            prompt: Implementation prompt to send to Claude Code.\n            plan_file: Optional path to the improvement plan file.\n        \n        Returns:\n            ClaudeCodeResult with execution outcome.\n        \"\"\"\n        if not self.check_installed():\n            return ClaudeCodeResult(\n                success=False,\n                error=\"Claude Code CLI not installed. Install with: npm install -g @anthropic-ai/claude-code\",\n                exit_code=-1,\n            )\n        \n        # Build the implementation prompt\n        full_prompt = self._build_prompt(prompt, plan_file)\n        \n        try:\n            # Run Claude Code in non-interactive mode with the prompt\n            result = subprocess.run(\n                [\n                    \"claude\",\n                    \"--print\",  # Non-interactive mode, output only\n                    \"--model\", self.model,\n                    \"--max-turns\", str(self.max_turns),\n                    \"--dangerously-skip-permissions\",  # Auto-approve for automation\n                    \"-p\", full_prompt,  # Pass prompt directly\n                ],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=self.timeout,\n            )\n            \n            if result.returncode == 0:\n                # Parse output to find modified files\n                files_modified = self._parse_modified_files(result.stdout, repo_path)\n                \n                return ClaudeCodeResult(\n                    success=True,\n                    output=result.stdout,\n                    files_modified=files_modified,\n                    exit_code=result.returncode,\n                )\n            else:\n                return ClaudeCodeResult(\n                    success=False,\n                    output=result.stdout,\n                    error=result.stderr or f\"Claude Code exited with code {result.returncode}\",\n                    exit_code=result.returncode,\n                )\n                \n        except subprocess.TimeoutExpired:\n            return ClaudeCodeResult(\n                success=False,\n                error=f\"Claude Code timed out after {self.timeout} seconds\",\n                exit_code=-1,\n            )\n        except FileNotFoundError:\n            return ClaudeCodeResult(\n                success=False,\n                error=\"Claude Code CLI not found in PATH\",\n                exit_code=-1,\n            )\n        except Exception as e:\n            return ClaudeCodeResult(\n                success=False,\n                error=f\"Unexpected error running Claude Code: {e}\",\n                exit_code=-1,\n            )\n    \n    def _build_prompt(self, prompt: str, plan_file: Optional[Path]) -> str:\n        \"\"\"Build the full implementation prompt.\n        \n        Args:\n            prompt: Base implementation prompt.\n            plan_file: Optional path to improvement plan.\n        \n        Returns:\n            Full prompt string for Claude Code.\n        \"\"\"\n        parts = []\n        \n        if plan_file and plan_file.exists():\n            parts.append(f\"Read the improvement plan at {plan_file} for context.\")\n        \n        parts.append(prompt)\n        parts.append(\n            \"\\nAfter implementing each task:\\n\"\n            \"1. Run relevant tests to verify changes work\\n\"\n            \"2. Fix any issues before moving to the next task\\n\"\n            \"3. Keep changes focused and incremental\"\n        )\n        \n        return \"\\n\\n\".join(parts)\n    \n    def _parse_modified_files(self, output: str, repo_path: Path) -> list[str]:\n        \"\"\"Parse Claude Code output to identify modified files.\n        \n        Args:\n            output: Claude Code stdout.\n            repo_path: Repository path for checking file existence.\n        \n        Returns:\n            List of modified file paths relative to repo.\n        \"\"\"\n        modified = []\n        \n        # Check git status for actual modifications\n        try:\n            result = subprocess.run(\n                [\"git\", \"diff\", \"--name-only\", \"HEAD\"],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=10,\n            )\n            if result.returncode == 0:\n                modified = [f.strip() for f in result.stdout.strip().split(\"\\n\") if f.strip()]\n        except Exception:\n            pass\n        \n        return modified\n```\n\n### 2. Update Config (`src/metaagent/config.py`)\n\nAdd new configuration options:\n\n```python\n# Add to Config dataclass\nclaude_code_timeout: int = 600  # 10 minutes\nclaude_code_model: str = \"claude-sonnet-4-20250514\"\nclaude_code_max_turns: int = 50\nauto_implement: bool = False  # Whether to auto-invoke Claude Code\n\n# Add to from_env():\nclaude_code_timeout=int(os.getenv(\"METAAGENT_CLAUDE_TIMEOUT\", \"600\")),\nclaude_code_model=os.getenv(\"METAAGENT_CLAUDE_MODEL\", \"claude-sonnet-4-20250514\"),\nclaude_code_max_turns=int(os.getenv(\"METAAGENT_CLAUDE_MAX_TURNS\", \"50\")),\nauto_implement=os.getenv(\"METAAGENT_AUTO_IMPLEMENT\", \"\").lower() in (\"true\", \"1\", \"yes\"),\n```\n\n### 3. Update Orchestrator (`src/metaagent/orchestrator.py`)\n\nReplace `_implement_with_claude()` method:\n\n```python\nfrom .claude_runner import ClaudeCodeRunner, ClaudeCodeResult\n\n# Add to __init__():\nself.claude_runner = claude_runner or ClaudeCodeRunner(\n    timeout=config.claude_code_timeout,\n    model=config.claude_code_model,\n    max_turns=config.claude_code_max_turns,\n)\n\n# Replace _implement_with_claude():\ndef _implement_with_claude(self, stage_results: list[StageResult]) -> bool:\n    \"\"\"Implement changes using Claude Code CLI.\n    \n    Args:\n        stage_results: Results from the analysis stage.\n    \n    Returns:\n        True if changes were made, False otherwise.\n    \"\"\"\n    if not stage_results:\n        return False\n    \n    # Build implementation prompt\n    tasks = []\n    for result in stage_results:\n        for task in result.tasks:\n            tasks.append(task)\n    \n    if not tasks:\n        logger.info(\"No tasks to implement\")\n        return False\n    \n    # Create implementation prompt\n    implementation_prompt = \"Implement the following improvements:\\n\\n\"\n    for i, task in enumerate(tasks, 1):\n        if isinstance(task, dict):\n            title = task.get(\"title\", \"\")\n            desc = task.get(\"description\", str(task))\n            file_ref = task.get(\"file\", \"\")\n            implementation_prompt += f\"{i}. {title}\\n\"\n            if desc:\n                implementation_prompt += f\"   Description: {desc}\\n\"\n            if file_ref:\n                implementation_prompt += f\"   File: {file_ref}\\n\"\n        else:\n            implementation_prompt += f\"{i}. {task}\\n\"\n    \n    # Write tasks to file for reference\n    prompt_file = self.config.repo_path / \".meta-agent-tasks.md\"\n    prompt_file.write_text(implementation_prompt, encoding=\"utf-8\")\n    logger.info(f\"Implementation tasks written to: {prompt_file}\")\n    \n    # Check if auto-implementation is enabled\n    if not self.config.auto_implement:\n        logger.info(\"Auto-implementation disabled. Run Claude Code manually.\")\n        logger.info(\"To enable, set METAAGENT_AUTO_IMPLEMENT=true or --auto-implement flag\")\n        return True  # Tasks written, considered success\n    \n    # Check if Claude Code is available\n    if not self.claude_runner.check_installed():\n        logger.warning(\"Claude Code CLI not installed. Tasks written to file for manual implementation.\")\n        return True\n    \n    # Run Claude Code\n    logger.info(\"Running Claude Code to implement changes...\")\n    result = self.claude_runner.implement(\n        repo_path=self.config.repo_path,\n        prompt=implementation_prompt,\n        plan_file=self.config.repo_path / \"docs\" / \"mvp_improvement_plan.md\",\n    )\n    \n    if result.success:\n        logger.info(f\"Claude Code completed successfully\")\n        if result.files_modified:\n            logger.info(f\"Files modified: {', '.join(result.files_modified)}\")\n        return True\n    else:\n        logger.error(f\"Claude Code failed: {result.error}\")\n        # Still return True because tasks were written for manual implementation\n        return True\n```\n\n### 4. Update CLI (`src/metaagent/cli.py`)\n\nAdd `--auto-implement` flag to refine command:\n\n```python\n@app.command()\ndef refine(\n    # ... existing options ...\n    auto_implement: bool = typer.Option(\n        False,\n        \"--auto-implement\",\n        \"-a\",\n        help=\"Automatically invoke Claude Code to implement changes.\",\n    ),\n) -> None:\n    # ... existing code ...\n    \n    if auto_implement:\n        config.auto_implement = True\n```\n\n### 5. Add Result Tracking to IterationResult\n\nUpdate `IterationResult` dataclass:\n\n```python\n@dataclass\nclass IterationResult:\n    \"\"\"Result from a single iteration of the refinement loop.\"\"\"\n    \n    iteration: int\n    prompts_run: list[str]\n    changes_made: bool\n    committed: bool\n    commit_hash: Optional[str] = None\n    stage_results: list[StageResult] = field(default_factory=list)\n    implementation_result: Optional[ClaudeCodeResult] = None  # NEW\n```\n\n### 6. Error Handling Strategy\n\n- **Claude Code not installed**: Log warning, fall back to file-based workflow\n- **Timeout**: Log error with duration, return partial success if tasks written\n- **Non-zero exit code**: Log stderr, consider as failure but preserve output\n- **Permission errors**: Suggest using `--dangerously-skip-permissions` flag\n- **Network errors**: Retry once, then fall back to manual mode\n\n### 7. Status Reporting\n\nAdd implementation status to the improvement plan:\n\n```python\ndef _generate_implementation_status(self, result: ClaudeCodeResult) -> str:\n    \"\"\"Generate status section for the improvement plan.\"\"\"\n    if result.success:\n        return f\"\"\"## Implementation Status\n\n**Status:** ✅ Implemented automatically\n**Files Modified:** {len(result.files_modified)}\n\nModified files:\n{chr(10).join(f'- {f}' for f in result.files_modified)}\n\"\"\"\n    else:\n        return f\"\"\"## Implementation Status\n\n**Status:** ⚠️ Manual implementation required\n**Reason:** {result.error}\n\nTasks have been written to `.meta-agent-tasks.md` for manual implementation.\n\"\"\"\n```\n\n## Key Design Decisions\n\n1. **Non-interactive mode**: Use `--print` flag for scripted execution\n2. **Permission handling**: Use `--dangerously-skip-permissions` for fully automated runs\n3. **Graceful degradation**: Always fall back to file-based workflow if Claude Code unavailable\n4. **Timeout safety**: 10-minute default timeout with configurable override\n5. **Git integration**: Track modified files via git diff for accurate reporting",
        "testStrategy": "## Unit Tests (`tests/test_claude_runner.py`)\n\n### 1. Test ClaudeCodeRunner Initialization\n```python\ndef test_claude_runner_defaults():\n    runner = ClaudeCodeRunner()\n    assert runner.timeout == 600\n    assert runner.model == \"claude-sonnet-4-20250514\"\n    assert runner.max_turns == 50\n\ndef test_claude_runner_custom_config():\n    runner = ClaudeCodeRunner(timeout=300, model=\"claude-opus-4\", max_turns=20)\n    assert runner.timeout == 300\n    assert runner.model == \"claude-opus-4\"\n    assert runner.max_turns == 20\n```\n\n### 2. Test check_installed()\n```python\ndef test_check_installed_success(mocker):\n    mocker.patch(\"subprocess.run\", return_value=Mock(returncode=0))\n    runner = ClaudeCodeRunner()\n    assert runner.check_installed() is True\n\ndef test_check_installed_not_found(mocker):\n    mocker.patch(\"subprocess.run\", side_effect=FileNotFoundError())\n    runner = ClaudeCodeRunner()\n    assert runner.check_installed() is False\n\ndef test_check_installed_timeout(mocker):\n    mocker.patch(\"subprocess.run\", side_effect=subprocess.TimeoutExpired(\"claude\", 10))\n    runner = ClaudeCodeRunner()\n    assert runner.check_installed() is False\n```\n\n### 3. Test implement() Success\n```python\ndef test_implement_success(mocker, tmp_path):\n    mock_run = mocker.patch(\"subprocess.run\")\n    mock_run.side_effect = [\n        Mock(returncode=0),  # check_installed\n        Mock(returncode=0, stdout=\"Success\", stderr=\"\"),  # implement\n        Mock(returncode=0, stdout=\"file1.py\\nfile2.py\"),  # git diff\n    ]\n    \n    runner = ClaudeCodeRunner()\n    result = runner.implement(tmp_path, \"Fix bugs\")\n    \n    assert result.success is True\n    assert result.error is None\n    assert \"file1.py\" in result.files_modified\n```\n\n### 4. Test implement() Failure Cases\n```python\ndef test_implement_not_installed(mocker, tmp_path):\n    mocker.patch.object(ClaudeCodeRunner, \"check_installed\", return_value=False)\n    runner = ClaudeCodeRunner()\n    result = runner.implement(tmp_path, \"Fix bugs\")\n    \n    assert result.success is False\n    assert \"not installed\" in result.error.lower()\n\ndef test_implement_timeout(mocker, tmp_path):\n    mocker.patch.object(ClaudeCodeRunner, \"check_installed\", return_value=True)\n    mocker.patch(\"subprocess.run\", side_effect=subprocess.TimeoutExpired(\"claude\", 600))\n    \n    runner = ClaudeCodeRunner(timeout=600)\n    result = runner.implement(tmp_path, \"Fix bugs\")\n    \n    assert result.success is False\n    assert \"timeout\" in result.error.lower()\n\ndef test_implement_nonzero_exit(mocker, tmp_path):\n    mocker.patch.object(ClaudeCodeRunner, \"check_installed\", return_value=True)\n    mocker.patch(\"subprocess.run\", return_value=Mock(\n        returncode=1, stdout=\"\", stderr=\"API error\"\n    ))\n    \n    runner = ClaudeCodeRunner()\n    result = runner.implement(tmp_path, \"Fix bugs\")\n    \n    assert result.success is False\n    assert result.exit_code == 1\n```\n\n### 5. Test _build_prompt()\n```python\ndef test_build_prompt_with_plan_file(tmp_path):\n    plan_file = tmp_path / \"plan.md\"\n    plan_file.write_text(\"# Plan\")\n    \n    runner = ClaudeCodeRunner()\n    prompt = runner._build_prompt(\"Fix bugs\", plan_file)\n    \n    assert \"plan.md\" in prompt\n    assert \"Fix bugs\" in prompt\n    assert \"Run relevant tests\" in prompt\n\ndef test_build_prompt_without_plan_file():\n    runner = ClaudeCodeRunner()\n    prompt = runner._build_prompt(\"Fix bugs\", None)\n    \n    assert \"Fix bugs\" in prompt\n    assert \"plan.md\" not in prompt\n```\n\n## Integration Tests (`tests/test_orchestrator_claude.py`)\n\n### 1. Test _implement_with_claude() with Auto-Implement Disabled\n```python\ndef test_implement_with_claude_auto_disabled(mock_orchestrator, tmp_path):\n    mock_orchestrator.config.auto_implement = False\n    stage_results = [StageResult(\n        stage_id=\"test\",\n        stage_name=\"Test\",\n        summary=\"Test\",\n        tasks=[{\"title\": \"Task 1\", \"description\": \"Do something\"}],\n    )]\n    \n    result = mock_orchestrator._implement_with_claude(stage_results)\n    \n    assert result is True\n    assert (tmp_path / \".meta-agent-tasks.md\").exists()\n```\n\n### 2. Test _implement_with_claude() with Auto-Implement Enabled\n```python\ndef test_implement_with_claude_auto_enabled(mock_orchestrator, mocker, tmp_path):\n    mock_orchestrator.config.auto_implement = True\n    mocker.patch.object(\n        mock_orchestrator.claude_runner,\n        \"implement\",\n        return_value=ClaudeCodeResult(success=True, files_modified=[\"src/test.py\"]),\n    )\n    \n    stage_results = [StageResult(\n        stage_id=\"test\",\n        stage_name=\"Test\",\n        summary=\"Test\",\n        tasks=[{\"title\": \"Task 1\"}],\n    )]\n    \n    result = mock_orchestrator._implement_with_claude(stage_results)\n    \n    assert result is True\n```\n\n### 3. Test Empty Tasks\n```python\ndef test_implement_with_claude_no_tasks(mock_orchestrator):\n    result = mock_orchestrator._implement_with_claude([])\n    assert result is False\n\ndef test_implement_with_claude_empty_stage_results(mock_orchestrator):\n    stage_results = [StageResult(\n        stage_id=\"test\",\n        stage_name=\"Test\",\n        summary=\"Test\",\n        tasks=[],\n    )]\n    result = mock_orchestrator._implement_with_claude(stage_results)\n    assert result is False\n```\n\n## CLI Tests (`tests/test_cli.py`)\n\n### 1. Test --auto-implement Flag\n```python\ndef test_refine_auto_implement_flag(runner, mock_config):\n    result = runner.invoke(app, [\n        \"refine\",\n        \"--profile\", \"automation_agent\",\n        \"--mock\",\n        \"--auto-implement\",\n    ])\n    # Should not fail even if Claude Code not installed\n    assert result.exit_code == 0\n\ndef test_refine_without_auto_implement(runner, mock_config):\n    result = runner.invoke(app, [\n        \"refine\",\n        \"--profile\", \"automation_agent\",\n        \"--mock\",\n    ])\n    assert result.exit_code == 0\n    assert \"manual\" in result.stdout.lower() or \"Claude Code\" in result.stdout\n```\n\n## End-to-End Test\n\n### 1. Full Workflow Test (requires Claude Code installed)\n```python\n@pytest.mark.skipif(not shutil.which(\"claude\"), reason=\"Claude Code not installed\")\ndef test_full_implementation_workflow(tmp_path):\n    # Create minimal test repo\n    (tmp_path / \"src\").mkdir()\n    (tmp_path / \"src\" / \"main.py\").write_text(\"# TODO: implement\")\n    (tmp_path / \"docs\").mkdir()\n    (tmp_path / \"docs\" / \"prd.md\").write_text(\"# PRD\\nAdd hello function\")\n    \n    # Initialize git\n    subprocess.run([\"git\", \"init\"], cwd=tmp_path)\n    subprocess.run([\"git\", \"add\", \"-A\"], cwd=tmp_path)\n    subprocess.run([\"git\", \"commit\", \"-m\", \"init\"], cwd=tmp_path)\n    \n    # Run implementation\n    runner = ClaudeCodeRunner(timeout=120)\n    result = runner.implement(\n        tmp_path,\n        \"Add a hello() function that returns 'Hello, World!' to src/main.py\",\n    )\n    \n    assert result.success is True\n    # Verify file was modified\n    content = (tmp_path / \"src\" / \"main.py\").read_text()\n    assert \"hello\" in content.lower()\n```\n\n## Manual Verification Steps\n\n1. Install Claude Code: `npm install -g @anthropic-ai/claude-code`\n2. Run: `metaagent refine --profile automation_agent --auto-implement`\n3. Verify Claude Code is invoked and implements at least one task\n4. Verify error handling when Claude Code is not installed\n5. Verify timeout handling with a complex task",
        "status": "done",
        "dependencies": [
          "5",
          "6",
          "7",
          "8"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-14T03:29:20.657Z"
      },
      {
        "id": "17",
        "title": "Implement Auto-Commit After Implementation",
        "description": "Enhance the orchestrator's git integration to automatically stage and commit changes after Claude Code implements improvements, with descriptive commit messages referencing the analysis prompt, and options to skip commits or provide custom messages.",
        "details": "## Overview\n\nThis task extends the existing `_commit_changes()` method in `src/metaagent/orchestrator.py:553` and integrates it with the Claude Code implementation flow from Task 16. The current implementation is basic and lacks flexibility. This task adds configurable commit behavior, detailed commit messages, and robust error handling.\n\n## Implementation Steps\n\n### 1. Create CommitConfig Dataclass (`src/metaagent/config.py`)\n\nAdd commit-related configuration options:\n\n```python\n@dataclass\nclass CommitConfig:\n    \"\"\"Configuration for auto-commit behavior.\"\"\"\n    enabled: bool = True  # Whether to auto-commit\n    push_enabled: bool = False  # Whether to push after commit (conservative default)\n    custom_message: Optional[str] = None  # Override commit message\n    include_prompt_reference: bool = True  # Include prompt ID in commit message\n    sign_commits: bool = False  # Enable GPG signing\n    \n    @classmethod\n    def from_env(cls) -> 'CommitConfig':\n        \"\"\"Load from environment variables.\"\"\"\n        return cls(\n            enabled=os.getenv(\"METAAGENT_AUTO_COMMIT\", \"true\").lower() in (\"true\", \"1\"),\n            push_enabled=os.getenv(\"METAAGENT_AUTO_PUSH\", \"false\").lower() in (\"true\", \"1\"),\n            sign_commits=os.getenv(\"METAAGENT_SIGN_COMMITS\", \"false\").lower() in (\"true\", \"1\"),\n        )\n```\n\nUpdate the main `Config` class to include `commit_config: CommitConfig`.\n\n### 2. Create CommitResult Dataclass (`src/metaagent/orchestrator.py`)\n\nAdd near other dataclasses (around line 75):\n\n```python\n@dataclass\nclass CommitResult:\n    \"\"\"Result from a git commit operation.\"\"\"\n    success: bool\n    commit_hash: Optional[str] = None\n    pushed: bool = False\n    error: Optional[str] = None\n    skipped: bool = False\n    skip_reason: Optional[str] = None\n    files_staged: int = 0\n```\n\n### 3. Enhance _commit_changes Method\n\nReplace the existing method at `src/metaagent/orchestrator.py:553` with enhanced version:\n\n```python\ndef _commit_changes(\n    self,\n    message: str,\n    prompt_ids: list[str],\n    skip_commit: bool = False,\n    custom_message: Optional[str] = None,\n) -> CommitResult:\n    \"\"\"Commit changes to git with descriptive message.\n\n    Args:\n        message: Base commit message (e.g., iteration summary).\n        prompt_ids: List of prompt IDs that generated the changes.\n        skip_commit: If True, skip the commit entirely.\n        custom_message: Override the generated commit message.\n\n    Returns:\n        CommitResult with details about the operation.\n    \"\"\"\n    if skip_commit or not self.config.commit_config.enabled:\n        return CommitResult(\n            success=True,\n            skipped=True,\n            skip_reason=\"Auto-commit disabled\" if not self.config.commit_config.enabled else \"User requested skip\"\n        )\n\n    try:\n        # Check for uncommitted changes\n        status_result = subprocess.run(\n            [\"git\", \"status\", \"--porcelain\"],\n            cwd=self.config.repo_path,\n            capture_output=True,\n            text=True,\n            timeout=30,\n        )\n\n        if not status_result.stdout.strip():\n            return CommitResult(\n                success=True,\n                skipped=True,\n                skip_reason=\"No changes to commit\"\n            )\n\n        # Count files to be staged\n        changed_files = [line for line in status_result.stdout.strip().split('\\n') if line]\n        \n        # Stage all changes\n        subprocess.run(\n            [\"git\", \"add\", \"-A\"],\n            cwd=self.config.repo_path,\n            check=True,\n            timeout=30,\n        )\n\n        # Build commit message\n        if custom_message:\n            commit_message = custom_message\n        else:\n            commit_message = self._build_commit_message(message, prompt_ids)\n\n        # Prepare commit command\n        commit_cmd = [\"git\", \"commit\", \"-m\", commit_message]\n        if self.config.commit_config.sign_commits:\n            commit_cmd.append(\"-S\")\n\n        subprocess.run(\n            commit_cmd,\n            cwd=self.config.repo_path,\n            check=True,\n            timeout=60,\n        )\n\n        # Get commit hash\n        hash_result = subprocess.run(\n            [\"git\", \"rev-parse\", \"--short\", \"HEAD\"],\n            cwd=self.config.repo_path,\n            capture_output=True,\n            text=True,\n            timeout=10,\n        )\n        commit_hash = hash_result.stdout.strip()\n\n        # Optionally push\n        pushed = False\n        if self.config.commit_config.push_enabled:\n            try:\n                subprocess.run(\n                    [\"git\", \"push\"],\n                    cwd=self.config.repo_path,\n                    check=True,\n                    timeout=120,\n                )\n                pushed = True\n            except subprocess.CalledProcessError as e:\n                logger.warning(f\"Push failed (commit still made): {e}\")\n\n        logger.info(f\"Committed changes: {commit_hash}\")\n        return CommitResult(\n            success=True,\n            commit_hash=commit_hash,\n            pushed=pushed,\n            files_staged=len(changed_files),\n        )\n\n    except subprocess.TimeoutExpired:\n        return CommitResult(success=False, error=\"Git operation timed out\")\n    except subprocess.CalledProcessError as e:\n        return CommitResult(success=False, error=f\"Git operation failed: {e.stderr or e}\")\n    except FileNotFoundError:\n        return CommitResult(success=False, error=\"Git not found in PATH\")\n```\n\n### 4. Add Commit Message Builder Method\n\nAdd helper method to orchestrator:\n\n```python\ndef _build_commit_message(self, message: str, prompt_ids: list[str]) -> str:\n    \"\"\"Build a descriptive commit message referencing analysis prompts.\n\n    Args:\n        message: Base message (e.g., iteration summary).\n        prompt_ids: List of prompt IDs that generated the changes.\n\n    Returns:\n        Formatted commit message.\n    \"\"\"\n    lines = [f\"meta-agent: {message}\"]\n    \n    if prompt_ids and self.config.commit_config.include_prompt_reference:\n        lines.append(\"\")\n        lines.append(\"Analysis prompts applied:\")\n        for prompt_id in prompt_ids:\n            prompt = self.prompt_library.get_prompt(prompt_id)\n            goal = prompt.goal if prompt else prompt_id\n            lines.append(f\"  - {prompt_id}: {goal[:60]}\")\n    \n    lines.append(\"\")\n    lines.append(\"🤖 Generated with meta-agent\")\n    \n    return \"\\n\".join(lines)\n```\n\n### 5. Update refine_iterative() Method\n\nModify the iterative refinement loop (around line 365) to use enhanced commit:\n\n```python\n# Step 5: Commit to GitHub\nlogger.info(\"Step 5: Committing changes to GitHub...\")\ncommit_result = self._commit_changes(\n    message=f\"Iteration {iteration}: improvements from AI analysis\",\n    prompt_ids=triage_result.selected_prompts,\n    skip_commit=self.config.skip_commit,  # From CLI option\n)\n\nif commit_result.success and not commit_result.skipped:\n    logger.info(f\"Committed {commit_result.files_staged} files: {commit_result.commit_hash}\")\nelif commit_result.skipped:\n    logger.info(f\"Commit skipped: {commit_result.skip_reason}\")\nelse:\n    logger.error(f\"Commit failed: {commit_result.error}\")\n\niterations.append(\n    IterationResult(\n        iteration=iteration,\n        prompts_run=triage_result.selected_prompts,\n        changes_made=changes_made,\n        committed=commit_result.success and not commit_result.skipped,\n        commit_hash=commit_result.commit_hash,\n        stage_results=iteration_stage_results,\n    )\n)\n```\n\n### 6. Add CLI Options\n\nUpdate `src/metaagent/cli.py` to add commit-related options to relevant commands:\n\n```python\n@app.command(\"refine-iterative\")\ndef refine_iterative(\n    # ... existing options ...\n    no_commit: bool = typer.Option(\n        False,\n        \"--no-commit\",\n        help=\"Skip auto-committing changes after implementation.\",\n    ),\n    commit_message: Optional[str] = typer.Option(\n        None,\n        \"--commit-message\",\n        \"-cm\",\n        help=\"Custom commit message (overrides auto-generated).\",\n    ),\n    auto_push: bool = typer.Option(\n        False,\n        \"--auto-push\",\n        help=\"Automatically push commits to remote.\",\n    ),\n) -> None:\n```\n\n### 7. Git Error Handling Module (Optional Enhancement)\n\nCreate `src/metaagent/git_utils.py` for reusable git operations:\n\n```python\n\"\"\"Git utility functions with graceful error handling.\"\"\"\n\nfrom __future__ import annotations\n\nimport subprocess\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Optional\n\n@dataclass\nclass GitStatus:\n    \"\"\"Git repository status.\"\"\"\n    is_repo: bool\n    has_changes: bool\n    current_branch: Optional[str] = None\n    error: Optional[str] = None\n\ndef check_git_status(repo_path: Path) -> GitStatus:\n    \"\"\"Check git repository status.\"\"\"\n    try:\n        # Check if directory is a git repo\n        result = subprocess.run(\n            [\"git\", \"rev-parse\", \"--git-dir\"],\n            cwd=repo_path,\n            capture_output=True,\n            text=True,\n            timeout=10,\n        )\n        if result.returncode != 0:\n            return GitStatus(is_repo=False, has_changes=False, error=\"Not a git repository\")\n        \n        # Get current branch\n        branch_result = subprocess.run(\n            [\"git\", \"branch\", \"--show-current\"],\n            cwd=repo_path,\n            capture_output=True,\n            text=True,\n            timeout=10,\n        )\n        branch = branch_result.stdout.strip() or \"HEAD (detached)\"\n        \n        # Check for changes\n        status_result = subprocess.run(\n            [\"git\", \"status\", \"--porcelain\"],\n            cwd=repo_path,\n            capture_output=True,\n            text=True,\n            timeout=30,\n        )\n        has_changes = bool(status_result.stdout.strip())\n        \n        return GitStatus(\n            is_repo=True,\n            has_changes=has_changes,\n            current_branch=branch,\n        )\n    except subprocess.TimeoutExpired:\n        return GitStatus(is_repo=False, has_changes=False, error=\"Git command timed out\")\n    except FileNotFoundError:\n        return GitStatus(is_repo=False, has_changes=False, error=\"Git not found in PATH\")\n```\n\n## Key Files to Modify\n\n1. `src/metaagent/config.py` - Add CommitConfig dataclass\n2. `src/metaagent/orchestrator.py` - Enhance _commit_changes(), add CommitResult, update refine_iterative()\n3. `src/metaagent/cli.py` - Add --no-commit, --commit-message, --auto-push options\n4. `src/metaagent/git_utils.py` - New file for reusable git utilities (optional)\n\n## Environment Variables\n\n| Variable | Default | Description |\n|----------|---------|-------------|\n| METAAGENT_AUTO_COMMIT | true | Enable/disable auto-commit |\n| METAAGENT_AUTO_PUSH | false | Enable/disable auto-push after commit |\n| METAAGENT_SIGN_COMMITS | false | Enable GPG commit signing |",
        "testStrategy": "## Unit Tests (`tests/test_git_commit.py`)\n\n### 1. Test CommitConfig Loading\n```python\ndef test_commit_config_defaults():\n    config = CommitConfig()\n    assert config.enabled is True\n    assert config.push_enabled is False\n    assert config.sign_commits is False\n\ndef test_commit_config_from_env(monkeypatch):\n    monkeypatch.setenv(\"METAAGENT_AUTO_COMMIT\", \"false\")\n    monkeypatch.setenv(\"METAAGENT_AUTO_PUSH\", \"true\")\n    config = CommitConfig.from_env()\n    assert config.enabled is False\n    assert config.push_enabled is True\n```\n\n### 2. Test CommitResult Dataclass\n```python\ndef test_commit_result_success():\n    result = CommitResult(success=True, commit_hash=\"abc1234\", files_staged=5)\n    assert result.success\n    assert result.commit_hash == \"abc1234\"\n    assert not result.skipped\n\ndef test_commit_result_skipped():\n    result = CommitResult(success=True, skipped=True, skip_reason=\"No changes\")\n    assert result.skipped\n    assert result.commit_hash is None\n```\n\n### 3. Test _commit_changes Method\n```python\ndef test_commit_changes_success(mock_subprocess, mock_config, tmp_path):\n    \"\"\"Test successful commit.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    mock_subprocess.return_value.stdout = \"M file.py\\n\"  # has changes\n    \n    result = orchestrator._commit_changes(\n        message=\"Test commit\",\n        prompt_ids=[\"quality_code_review\"]\n    )\n    \n    assert result.success\n    assert result.commit_hash is not None\n    assert not result.skipped\n\ndef test_commit_changes_no_changes(mock_subprocess, mock_config):\n    \"\"\"Test when there are no changes to commit.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    mock_subprocess.return_value.stdout = \"\"  # no changes\n    \n    result = orchestrator._commit_changes(\n        message=\"Test commit\",\n        prompt_ids=[]\n    )\n    \n    assert result.success\n    assert result.skipped\n    assert result.skip_reason == \"No changes to commit\"\n\ndef test_commit_changes_skip_flag(mock_config):\n    \"\"\"Test skip_commit flag.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    \n    result = orchestrator._commit_changes(\n        message=\"Test\",\n        prompt_ids=[],\n        skip_commit=True\n    )\n    \n    assert result.skipped\n    assert \"User requested skip\" in result.skip_reason\n\ndef test_commit_changes_disabled(mock_config):\n    \"\"\"Test when auto-commit is disabled in config.\"\"\"\n    mock_config.commit_config.enabled = False\n    orchestrator = Orchestrator(mock_config)\n    \n    result = orchestrator._commit_changes(\n        message=\"Test\",\n        prompt_ids=[]\n    )\n    \n    assert result.skipped\n    assert \"disabled\" in result.skip_reason.lower()\n\ndef test_commit_changes_custom_message(mock_subprocess, mock_config):\n    \"\"\"Test custom commit message.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    mock_subprocess.return_value.stdout = \"M file.py\\n\"\n    \n    result = orchestrator._commit_changes(\n        message=\"Auto message\",\n        prompt_ids=[],\n        custom_message=\"My custom message\"\n    )\n    \n    # Verify custom message was used in git commit call\n    commit_call = [c for c in mock_subprocess.call_args_list if \"commit\" in str(c)]\n    assert \"My custom message\" in str(commit_call)\n\ndef test_commit_changes_git_error(mock_subprocess, mock_config):\n    \"\"\"Test git command failure.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    mock_subprocess.side_effect = subprocess.CalledProcessError(1, \"git\", stderr=\"error\")\n    \n    result = orchestrator._commit_changes(\n        message=\"Test\",\n        prompt_ids=[]\n    )\n    \n    assert not result.success\n    assert result.error is not None\n\ndef test_commit_changes_timeout(mock_subprocess, mock_config):\n    \"\"\"Test git command timeout.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    mock_subprocess.side_effect = subprocess.TimeoutExpired(\"git\", 30)\n    \n    result = orchestrator._commit_changes(\n        message=\"Test\",\n        prompt_ids=[]\n    )\n    \n    assert not result.success\n    assert \"timed out\" in result.error.lower()\n\ndef test_commit_changes_git_not_found(mock_subprocess, mock_config):\n    \"\"\"Test when git is not installed.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    mock_subprocess.side_effect = FileNotFoundError()\n    \n    result = orchestrator._commit_changes(\n        message=\"Test\",\n        prompt_ids=[]\n    )\n    \n    assert not result.success\n    assert \"not found\" in result.error.lower()\n```\n\n### 4. Test Commit Message Builder\n```python\ndef test_build_commit_message_basic(mock_config):\n    \"\"\"Test basic commit message generation.\"\"\"\n    orchestrator = Orchestrator(mock_config)\n    \n    message = orchestrator._build_commit_message(\n        message=\"Iteration 1\",\n        prompt_ids=[]\n    )\n    \n    assert \"meta-agent: Iteration 1\" in message\n    assert \"🤖 Generated with meta-agent\" in message\n\ndef test_build_commit_message_with_prompts(mock_config, mock_prompt_library):\n    \"\"\"Test commit message includes prompt references.\"\"\"\n    orchestrator = Orchestrator(mock_config, prompt_library=mock_prompt_library)\n    \n    message = orchestrator._build_commit_message(\n        message=\"Iteration 1\",\n        prompt_ids=[\"quality_code_review\", \"security_analysis\"]\n    )\n    \n    assert \"Analysis prompts applied:\" in message\n    assert \"quality_code_review\" in message\n    assert \"security_analysis\" in message\n```\n\n### 5. Test CLI Options\n```python\ndef test_refine_iterative_no_commit_flag(mock_orchestrator):\n    \"\"\"Test --no-commit flag is passed to orchestrator.\"\"\"\n    result = runner.invoke(app, [\n        \"refine-iterative\",\n        \"--repo\", str(tmp_path),\n        \"--no-commit\"\n    ])\n    \n    # Verify skip_commit=True was passed\n    assert mock_orchestrator.refine_iterative.called\n    call_kwargs = mock_orchestrator.refine_iterative.call_args[1]\n    assert call_kwargs.get(\"skip_commit\") is True\n\ndef test_refine_iterative_custom_message(mock_orchestrator):\n    \"\"\"Test --commit-message flag.\"\"\"\n    result = runner.invoke(app, [\n        \"refine-iterative\",\n        \"--repo\", str(tmp_path),\n        \"--commit-message\", \"Custom: my changes\"\n    ])\n    \n    call_kwargs = mock_orchestrator.refine_iterative.call_args[1]\n    assert call_kwargs.get(\"custom_commit_message\") == \"Custom: my changes\"\n\ndef test_refine_iterative_auto_push(mock_orchestrator):\n    \"\"\"Test --auto-push flag.\"\"\"\n    result = runner.invoke(app, [\n        \"refine-iterative\",\n        \"--repo\", str(tmp_path),\n        \"--auto-push\"\n    ])\n    \n    # Verify push_enabled was set\n    assert mock_orchestrator.config.commit_config.push_enabled is True\n```\n\n### 6. Integration Tests\n```python\n@pytest.mark.integration\ndef test_commit_changes_real_git(tmp_path):\n    \"\"\"Integration test with real git repository.\"\"\"\n    # Initialize git repo\n    subprocess.run([\"git\", \"init\"], cwd=tmp_path)\n    subprocess.run([\"git\", \"config\", \"user.email\", \"test@test.com\"], cwd=tmp_path)\n    subprocess.run([\"git\", \"config\", \"user.name\", \"Test\"], cwd=tmp_path)\n    \n    # Create initial commit\n    (tmp_path / \"README.md\").write_text(\"# Test\")\n    subprocess.run([\"git\", \"add\", \"-A\"], cwd=tmp_path)\n    subprocess.run([\"git\", \"commit\", \"-m\", \"Initial\"], cwd=tmp_path)\n    \n    # Make changes\n    (tmp_path / \"new_file.py\").write_text(\"print('hello')\")\n    \n    # Test commit\n    config = Config.from_env(tmp_path)\n    orchestrator = Orchestrator(config)\n    \n    result = orchestrator._commit_changes(\n        message=\"Test iteration\",\n        prompt_ids=[\"test_prompt\"]\n    )\n    \n    assert result.success\n    assert result.commit_hash is not None\n    \n    # Verify commit exists\n    log_result = subprocess.run(\n        [\"git\", \"log\", \"--oneline\", \"-1\"],\n        cwd=tmp_path,\n        capture_output=True,\n        text=True,\n    )\n    assert \"meta-agent\" in log_result.stdout\n```\n\n## Manual Testing Checklist\n\n1. Run `metaagent refine-iterative --mock --no-commit` and verify no commits are made\n2. Run `metaagent refine-iterative --mock --commit-message \"Custom msg\"` and verify custom message\n3. Test in a repo without git initialized - verify graceful error\n4. Test with uncommitted changes - verify they are staged and committed\n5. Test with no changes to commit - verify skip message\n6. Verify commit messages include prompt references\n7. Test `--auto-push` on a repo with remote configured",
        "status": "done",
        "dependencies": [
          "7",
          "8",
          "16"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T03:33:46.159Z"
      },
      {
        "id": "18",
        "title": "Implement Full Refinement Loop",
        "description": "Create the complete iterative refinement loop that runs until the AI determines the codebase is complete, orchestrating the full cycle of packing, triage, analysis, implementation, and auto-commit with progress reporting and safety limits.",
        "details": "## Overview\n\nThis task extends the existing `refine_iterative()` method in `src/metaagent/orchestrator.py:256` to create a fully automated end-to-end refinement loop. The current implementation has the structure but relies on stub implementations for Claude Code integration (`_implement_with_claude()` at line 510) and basic git commit (`_commit_changes()` at line 553). This task integrates everything into a complete, production-ready loop.\n\n## Implementation Steps\n\n### 1. Add CLI Command for Full Loop (`src/metaagent/cli.py`)\n\nAdd a new `refine-loop` command that exposes the full iterative refinement:\n\n```python\n@app.command(\"refine-loop\")\ndef refine_loop(\n    repo: Path = typer.Option(\n        Path.cwd(),\n        \"--repo\",\n        \"-r\",\n        help=\"Path to the target repository to refine.\",\n    ),\n    prd: Optional[Path] = typer.Option(\n        None,\n        \"--prd\",\n        help=\"Path to PRD file (default: docs/prd.md in target repo).\",\n    ),\n    config_dir: Optional[Path] = typer.Option(\n        None,\n        \"--config-dir\",\n        \"-c\",\n        help=\"Path to meta-agent config directory.\",\n    ),\n    max_iterations: int = typer.Option(\n        10,\n        \"--max-iterations\",\n        \"-n\",\n        help=\"Maximum number of refinement iterations (safety limit).\",\n    ),\n    auto_commit: bool = typer.Option(\n        True,\n        \"--auto-commit/--no-auto-commit\",\n        help=\"Automatically commit changes after each iteration.\",\n    ),\n    push: bool = typer.Option(\n        False,\n        \"--push/--no-push\",\n        help=\"Push commits to remote after each iteration.\",\n    ),\n    dry_run: bool = typer.Option(\n        False,\n        \"--dry-run\",\n        help=\"Run analysis without implementing changes.\",\n    ),\n    mock: bool = typer.Option(\n        False,\n        \"--mock\",\n        \"-m\",\n        help=\"Run in mock mode (no API calls).\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\",\n        help=\"Enable verbose output.\",\n    ),\n) -> None:\n    \"\"\"Run the full iterative refinement loop until completion.\n    \n    The loop performs these steps each iteration:\n    1. Pack codebase with Repomix\n    2. Run triage to select analysis prompts\n    3. Run selected analysis prompts\n    4. Implement changes with Claude Code\n    5. Auto-commit changes (if enabled)\n    6. Re-pack and repeat from step 2\n    \n    Stops when triage returns done=true or max iterations reached.\n    \"\"\"\n```\n\n### 2. Create Progress Reporter (`src/metaagent/progress.py`)\n\nCreate a new module for rich progress reporting:\n\n```python\n\"\"\"Progress reporting for the refinement loop.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional\n\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.panel import Panel\nfrom rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\nfrom rich.table import Table\n\n\n@dataclass\nclass IterationProgress:\n    \"\"\"Progress tracking for a single iteration.\"\"\"\n    iteration: int\n    max_iterations: int\n    current_step: str\n    step_number: int\n    total_steps: int = 6\n    prompts_selected: list[str] = field(default_factory=list)\n    changes_made: int = 0\n    commit_hash: Optional[str] = None\n    started_at: datetime = field(default_factory=datetime.now)\n\n\nclass ProgressReporter:\n    \"\"\"Rich progress reporter for the refinement loop.\"\"\"\n\n    STEPS = [\n        \"Packing codebase\",\n        \"Running triage\",\n        \"Analyzing codebase\",\n        \"Implementing changes\",\n        \"Committing changes\",\n        \"Checking completion\",\n    ]\n\n    def __init__(self, console: Optional[Console] = None):\n        self.console = console or Console()\n        self.current: Optional[IterationProgress] = None\n        self._live: Optional[Live] = None\n\n    def start_iteration(self, iteration: int, max_iterations: int) -> None:\n        \"\"\"Start tracking a new iteration.\"\"\"\n        self.current = IterationProgress(\n            iteration=iteration,\n            max_iterations=max_iterations,\n            current_step=self.STEPS[0],\n            step_number=1,\n        )\n        self._render()\n\n    def update_step(self, step: int, details: str = \"\") -> None:\n        \"\"\"Update the current step.\"\"\"\n        if self.current:\n            self.current.step_number = step\n            self.current.current_step = f\"{self.STEPS[step - 1]}: {details}\" if details else self.STEPS[step - 1]\n            self._render()\n\n    def set_prompts(self, prompts: list[str]) -> None:\n        \"\"\"Set the prompts selected by triage.\"\"\"\n        if self.current:\n            self.current.prompts_selected = prompts\n            self._render()\n\n    def set_changes(self, count: int) -> None:\n        \"\"\"Set the number of changes made.\"\"\"\n        if self.current:\n            self.current.changes_made = count\n            self._render()\n\n    def set_commit(self, hash: str) -> None:\n        \"\"\"Set the commit hash.\"\"\"\n        if self.current:\n            self.current.commit_hash = hash\n            self._render()\n\n    def complete_iteration(self, success: bool = True) -> None:\n        \"\"\"Mark the iteration as complete.\"\"\"\n        status = \"[green]✓[/green]\" if success else \"[red]✗[/red]\"\n        self.console.print(f\"\\nIteration {self.current.iteration} {status}\")\n\n    def finish(self, done: bool, total_iterations: int) -> None:\n        \"\"\"Finish progress reporting.\"\"\"\n        if done:\n            self.console.print(\"\\n[green bold]✓ Refinement complete![/green bold]\")\n            self.console.print(\"Triage determined the codebase meets PRD requirements.\")\n        else:\n            self.console.print(f\"\\n[yellow]⚠ Maximum iterations ({total_iterations}) reached[/yellow]\")\n\n    def _render(self) -> None:\n        \"\"\"Render current progress.\"\"\"\n        if not self.current:\n            return\n\n        table = Table(show_header=False, box=None)\n        table.add_column(\"Key\", style=\"dim\")\n        table.add_column(\"Value\")\n\n        table.add_row(\"Iteration\", f\"{self.current.iteration}/{self.current.max_iterations}\")\n        table.add_row(\"Step\", f\"{self.current.step_number}/{self.current.total_steps}\")\n        table.add_row(\"Current\", self.current.current_step)\n\n        if self.current.prompts_selected:\n            table.add_row(\"Prompts\", \", \".join(self.current.prompts_selected))\n\n        if self.current.changes_made:\n            table.add_row(\"Changes\", str(self.current.changes_made))\n\n        if self.current.commit_hash:\n            table.add_row(\"Commit\", self.current.commit_hash)\n\n        panel = Panel(table, title=\"[bold blue]Refinement Progress[/bold blue]\")\n        self.console.print(panel, end=\"\\r\")\n```\n\n### 3. Enhance Orchestrator Loop (`src/metaagent/orchestrator.py`)\n\nUpdate `refine_iterative()` to integrate with progress reporting and Task 16/17 implementations:\n\n```python\ndef refine_iterative(\n    self,\n    max_iterations: int = 10,\n    auto_commit: bool = True,\n    push: bool = False,\n    dry_run: bool = False,\n    progress_reporter: Optional[\"ProgressReporter\"] = None,\n) -> RefinementResult:\n    \"\"\"Run the iterative refinement loop with AI-driven triage.\n\n    This is the main entry point for the full refinement loop:\n    1. Pack codebase with Repomix\n    2. Triage (AI decides which prompts to run)\n    3. Run selected prompts\n    4. Implement changes with Claude Code\n    5. Commit to git (if auto_commit enabled)\n    6. Repeat until triage says \"done\" or max iterations reached\n\n    Args:\n        max_iterations: Maximum number of iterations to run (safety limit).\n        auto_commit: Whether to automatically commit after each iteration.\n        push: Whether to push commits to remote.\n        dry_run: If True, skip implementation step.\n        progress_reporter: Optional progress reporter for UI feedback.\n\n    Returns:\n        RefinementResult with the outcome.\n    \"\"\"\n```\n\n### 4. Add Loop Dataclasses (`src/metaagent/orchestrator.py`)\n\nAdd new dataclasses for tracking loop state:\n\n```python\n@dataclass\nclass LoopConfig:\n    \"\"\"Configuration for the refinement loop.\"\"\"\n    max_iterations: int = 10\n    auto_commit: bool = True\n    push: bool = False\n    dry_run: bool = False\n    min_changes_per_iteration: int = 1\n\n\n@dataclass\nclass LoopState:\n    \"\"\"State tracking for the refinement loop.\"\"\"\n    iteration: int = 0\n    total_changes: int = 0\n    total_commits: int = 0\n    total_prompts_run: int = 0\n    consecutive_no_change_iterations: int = 0\n    stopped_reason: Optional[str] = None\n```\n\n### 5. Integrate Claude Code Runner (depends on Task 16)\n\nAfter Task 16 is complete, update `_implement_with_claude()` to use `ClaudeCodeRunner`:\n\n```python\ndef _implement_with_claude(\n    self,\n    stage_results: list[StageResult],\n    progress_reporter: Optional[\"ProgressReporter\"] = None,\n) -> tuple[bool, int]:\n    \"\"\"Implement changes using Claude Code.\n\n    Args:\n        stage_results: Results from the analysis stage.\n        progress_reporter: Optional progress reporter.\n\n    Returns:\n        Tuple of (changes_made, change_count).\n    \"\"\"\n    if not stage_results:\n        return False, 0\n\n    # Import ClaudeCodeRunner from Task 16\n    from .claude_runner import ClaudeCodeRunner\n\n    runner = ClaudeCodeRunner(\n        timeout=self.config.claude_timeout,\n        model=self.config.claude_model,\n        max_turns=self.config.claude_max_turns,\n    )\n\n    # Build implementation prompt from tasks\n    tasks = []\n    for result in stage_results:\n        tasks.extend(result.tasks)\n\n    if not tasks:\n        logger.info(\"No tasks to implement\")\n        return False, 0\n\n    implementation_prompt = self._build_implementation_prompt(tasks)\n\n    if progress_reporter:\n        progress_reporter.update_step(4, f\"{len(tasks)} tasks\")\n\n    result = runner.run(\n        prompt=implementation_prompt,\n        working_dir=self.config.repo_path,\n    )\n\n    return result.success, len(result.files_modified)\n```\n\n### 6. Integrate Git Commit (depends on Task 17)\n\nAfter Task 17 is complete, update `_commit_changes()` to use enhanced git integration:\n\n```python\ndef _commit_changes(\n    self,\n    message: str,\n    push: bool = False,\n    progress_reporter: Optional[\"ProgressReporter\"] = None,\n) -> Optional[str]:\n    \"\"\"Commit changes to git with enhanced options.\n\n    Args:\n        message: Commit message.\n        push: Whether to push to remote.\n        progress_reporter: Optional progress reporter.\n\n    Returns:\n        Commit hash if successful, None otherwise.\n    \"\"\"\n    # Use CommitConfig from Task 17\n    from .git_utils import commit_changes, CommitConfig\n\n    config = CommitConfig(\n        enabled=True,\n        push_enabled=push,\n        message_prefix=\"meta-agent: \",\n        message_suffix=\"\\n\\n🤖 Generated with meta-agent\",\n    )\n\n    result = commit_changes(\n        repo_path=self.config.repo_path,\n        message=message,\n        config=config,\n    )\n\n    if result.success and progress_reporter:\n        progress_reporter.set_commit(result.commit_hash[:8])\n\n    return result.commit_hash if result.success else None\n```\n\n### 7. Add Safety Mechanisms\n\nAdd safety checks to prevent runaway loops:\n\n```python\ndef _check_loop_safety(self, state: LoopState, config: LoopConfig) -> Optional[str]:\n    \"\"\"Check if the loop should be stopped for safety reasons.\n\n    Args:\n        state: Current loop state.\n        config: Loop configuration.\n\n    Returns:\n        Stop reason if loop should stop, None otherwise.\n    \"\"\"\n    # Max iterations reached\n    if state.iteration >= config.max_iterations:\n        return f\"Maximum iterations ({config.max_iterations}) reached\"\n\n    # Too many consecutive no-change iterations\n    if state.consecutive_no_change_iterations >= 3:\n        return \"No changes made in 3 consecutive iterations\"\n\n    # Total runtime check could be added here\n\n    return None\n```\n\n## File Changes Summary\n\n1. **`src/metaagent/cli.py`**: Add `refine-loop` command with `--max-iterations` flag\n2. **`src/metaagent/progress.py`**: New file for rich progress reporting\n3. **`src/metaagent/orchestrator.py`**: Enhance `refine_iterative()` with full loop, add `LoopConfig` and `LoopState` dataclasses\n4. **`src/metaagent/__init__.py`**: Export new classes\n\n## Dependencies on Other Tasks\n\n- **Task 16 (Claude Code Integration)**: Required for `_implement_with_claude()` to actually invoke Claude Code\n- **Task 17 (Auto-Commit)**: Required for `_commit_changes()` to use enhanced git features\n- **Task 14/15 (Iterative Triage)**: The `_run_triage()` method at line 437 is already implemented and will be used as-is",
        "testStrategy": "## Unit Tests (`tests/test_refinement_loop.py`)\n\n### 1. Test CLI Command\n```python\ndef test_refine_loop_help():\n    result = runner.invoke(app, [\"refine-loop\", \"--help\"])\n    assert result.exit_code == 0\n    assert \"--max-iterations\" in result.stdout\n    assert \"--auto-commit\" in result.stdout\n    assert \"--push\" in result.stdout\n    assert \"--dry-run\" in result.stdout\n\ndef test_refine_loop_default_max_iterations(mock_config):\n    \"\"\"Test default max-iterations is 10.\"\"\"\n    with patch(\"metaagent.cli.Orchestrator\") as mock_orch:\n        mock_instance = mock_orch.return_value\n        mock_instance.refine_iterative.return_value = RefinementResult(\n            success=True,\n            profile_name=\"iterative\",\n            stages_completed=1,\n            stages_failed=0,\n        )\n\n        result = runner.invoke(app, [\n            \"refine-loop\",\n            \"--repo\", str(mock_config.repo_path),\n            \"--config-dir\", str(mock_config.config_dir),\n            \"--mock\",\n        ])\n\n        # Verify max_iterations=10 was passed\n        call_args = mock_instance.refine_iterative.call_args\n        assert call_args.kwargs.get(\"max_iterations\", 10) == 10\n```\n\n### 2. Test Progress Reporter\n```python\ndef test_progress_reporter_initialization():\n    reporter = ProgressReporter()\n    assert reporter.current is None\n\ndef test_progress_reporter_start_iteration():\n    reporter = ProgressReporter(Console(quiet=True))\n    reporter.start_iteration(1, 10)\n    assert reporter.current is not None\n    assert reporter.current.iteration == 1\n    assert reporter.current.max_iterations == 10\n\ndef test_progress_reporter_update_step():\n    reporter = ProgressReporter(Console(quiet=True))\n    reporter.start_iteration(1, 10)\n    reporter.update_step(2, \"Running analysis\")\n    assert reporter.current.step_number == 2\n    assert \"Running analysis\" in reporter.current.current_step\n\ndef test_progress_reporter_set_prompts():\n    reporter = ProgressReporter(Console(quiet=True))\n    reporter.start_iteration(1, 10)\n    reporter.set_prompts([\"quality_error_analysis\", \"security_vulnerability_analysis\"])\n    assert len(reporter.current.prompts_selected) == 2\n```\n\n### 3. Test Loop State and Config\n```python\ndef test_loop_config_defaults():\n    config = LoopConfig()\n    assert config.max_iterations == 10\n    assert config.auto_commit is True\n    assert config.push is False\n    assert config.dry_run is False\n\ndef test_loop_state_initialization():\n    state = LoopState()\n    assert state.iteration == 0\n    assert state.total_changes == 0\n    assert state.stopped_reason is None\n```\n\n### 4. Test Safety Mechanisms\n```python\ndef test_check_loop_safety_max_iterations():\n    orchestrator = create_test_orchestrator()\n    state = LoopState(iteration=10)\n    config = LoopConfig(max_iterations=10)\n    reason = orchestrator._check_loop_safety(state, config)\n    assert \"Maximum iterations\" in reason\n\ndef test_check_loop_safety_no_changes():\n    orchestrator = create_test_orchestrator()\n    state = LoopState(consecutive_no_change_iterations=3)\n    config = LoopConfig()\n    reason = orchestrator._check_loop_safety(state, config)\n    assert \"No changes made\" in reason\n\ndef test_check_loop_safety_continue():\n    orchestrator = create_test_orchestrator()\n    state = LoopState(iteration=5, consecutive_no_change_iterations=1)\n    config = LoopConfig(max_iterations=10)\n    reason = orchestrator._check_loop_safety(state, config)\n    assert reason is None\n```\n\n### 5. Test Full Loop Integration\n```python\ndef test_refine_iterative_stops_on_done(mock_orchestrator):\n    \"\"\"Test loop stops when triage returns done=true.\"\"\"\n    # Mock triage to return done=True on second iteration\n    mock_orchestrator._run_triage = MagicMock(side_effect=[\n        TriageResult(success=True, done=False, selected_prompts=[\"quality_error_analysis\"]),\n        TriageResult(success=True, done=True, assessment=\"Complete\"),\n    ])\n\n    result = mock_orchestrator.refine_iterative(max_iterations=10)\n\n    assert result.success\n    assert len(result.iterations) == 1  # Only one iteration before done\n\ndef test_refine_iterative_stops_at_max_iterations(mock_orchestrator):\n    \"\"\"Test loop stops at max iterations.\"\"\"\n    # Mock triage to never return done\n    mock_orchestrator._run_triage = MagicMock(return_value=TriageResult(\n        success=True,\n        done=False,\n        selected_prompts=[\"quality_error_analysis\"],\n    ))\n\n    result = mock_orchestrator.refine_iterative(max_iterations=3)\n\n    assert len(result.iterations) == 3\n\ndef test_refine_iterative_dry_run(mock_orchestrator):\n    \"\"\"Test dry run skips implementation.\"\"\"\n    result = mock_orchestrator.refine_iterative(max_iterations=1, dry_run=True)\n\n    # Implementation should not have been called\n    mock_orchestrator._implement_with_claude.assert_not_called()\n```\n\n### 6. Test Error Handling\n```python\ndef test_refine_iterative_triage_failure(mock_orchestrator):\n    \"\"\"Test graceful handling of triage failure.\"\"\"\n    mock_orchestrator._run_triage = MagicMock(return_value=TriageResult(\n        success=False,\n        error=\"API error\",\n    ))\n\n    result = mock_orchestrator.refine_iterative(max_iterations=10)\n\n    assert not result.success\n    assert \"Triage failed\" in result.error\n\ndef test_refine_iterative_commit_failure(mock_orchestrator):\n    \"\"\"Test loop continues despite commit failure.\"\"\"\n    mock_orchestrator._commit_changes = MagicMock(return_value=None)\n\n    result = mock_orchestrator.refine_iterative(max_iterations=1, auto_commit=True)\n\n    # Should still succeed despite commit failure\n    assert result.iterations[0].committed is False\n```\n\n## Integration Tests\n\n### 1. End-to-End Mock Test\n```python\ndef test_full_loop_end_to_end(mock_config, tmp_path):\n    \"\"\"Test complete loop with all components mocked.\"\"\"\n    # Setup mock responses for triage, analysis, implementation\n    with patch.multiple(\n        \"metaagent.orchestrator.Orchestrator\",\n        _pack_codebase=MagicMock(return_value=\"mock code context\"),\n        _run_triage=MagicMock(side_effect=[\n            TriageResult(success=True, done=False, selected_prompts=[\"quality_error_analysis\"]),\n            TriageResult(success=True, done=True),\n        ]),\n        _implement_with_claude=MagicMock(return_value=(True, 3)),\n        _commit_changes=MagicMock(return_value=\"abc123\"),\n    ):\n        result = runner.invoke(app, [\n            \"refine-loop\",\n            \"--repo\", str(mock_config.repo_path),\n            \"--config-dir\", str(mock_config.config_dir),\n            \"--max-iterations\", \"5\",\n            \"--mock\",\n        ])\n\n        assert result.exit_code == 0\n        assert \"complete\" in result.stdout.lower()\n```\n\n## Manual Verification Steps\n\n1. Run `metaagent refine-loop --help` and verify all options are shown\n2. Run `metaagent refine-loop --mock --max-iterations 2` and verify progress output\n3. Run `metaagent refine-loop --dry-run` and verify no changes are made\n4. Verify loop stops correctly when max iterations reached\n5. Verify `--no-auto-commit` flag prevents git commits\n6. Test with real API keys on a test repository",
        "status": "done",
        "dependencies": [
          "7",
          "8",
          "14",
          "16",
          "17"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-14T03:35:30.227Z"
      },
      {
        "id": "19",
        "title": "Create JSON Response Schema Constant in prompts.py",
        "description": "Add JSON_RESPONSE_SCHEMA constant to prompts.py defining the structured response format with summary, recommendations, and tasks structure for AI analysis outputs.",
        "details": "Update `src/metaagent/prompts.py` to include a JSON schema constant following JSON Schema draft-07 specification and Pydantic best practices for structured LLM outputs[1][2].\n\n**Implementation Steps:**\n\n1. **Define the JSON_RESPONSE_SCHEMA constant** at module level in `prompts.py`:\n\n```python\nfrom typing import List, Dict, Any\nimport json\n\nJSON_RESPONSE_SCHEMA = {\n    '$schema': 'http://json-schema.org/draft-07/schema#',\n    'type': 'object',\n    'properties': {\n        'summary': {\n            'type': 'object',\n            'properties': {\n                'current_state': {'type': 'string', 'description': 'Current codebase state summary'},\n                'gaps_identified': {\n                    'type': 'array',\n                    'items': {'type': 'string'},\n                    'description': 'List of identified gaps vs PRD'\n                },\n                'readiness_score': {'type': 'number', 'minimum': 0, 'maximum': 10}\n            },\n            'required': ['current_state', 'gaps_identified', 'readiness_score']\n        },\n        'recommendations': {\n            'type': 'array',\n            'items': {\n                'type': 'object',\n                'properties': {\n                    'id': {'type': 'string'},\n                    'priority': {'type': 'string', 'enum': ['high', 'medium', 'low']},\n                    'description': {'type': 'string'},\n                    'category': {'type': 'string', 'enum': ['bug', 'feature', 'refactor', 'docs']},\n                    'estimated_effort': {'type': 'string'}\n                },\n                'required': ['id', 'priority', 'description']\n            }\n        },\n        'tasks': {\n            'type': 'array',\n            'items': {\n                'type': 'object',\n                'properties': {\n                    'title': {'type': 'string'},\n                    'description': {'type': 'string'},\n                    'priority': {'type': 'string', 'enum': ['high', 'medium', 'low']},\n                    'dependencies': {\n                        'type': 'array',\n                        'items': {'type': 'number'}\n                    },\n                    'details': {'type': 'string'},\n                    'testStrategy': {'type': 'string'}\n                },\n                'required': ['title', 'description', 'priority']\n            }\n        }\n    },\n    'required': ['summary', 'recommendations', 'tasks'],\n    'additionalProperties': False\n}\n```\n\n2. **Add helper method** to validate JSON responses:\n\n```python\ndef validate_json_response(response_json: str) -> Dict[str, Any]:\n    \"\"\"Validate AI response against JSON_RESPONSE_SCHEMA.\"\"\"\n    try:\n        data = json.loads(response_json)\n        # Use jsonschema.validate if available, or simple structural check\n        return data\n    except json.JSONDecodeError as e:\n        raise ValueError(f'Invalid JSON response: {e}')\n```\n\n3. **Update Prompt.render()** to include schema in system prompt context:\n\n```python\n    def render(self, prd: str, code_context: str, history: str, current_stage: str) -> str:\n        tmpl = Template(self.template)\n        schema_str = json.dumps(JSON_RESPONSE_SCHEMA, indent=2)\n        return tmpl.render(\n            prd=prd,\n            code_context=code_context,\n            history=history,\n            current_stage=current_stage,\n            json_response_schema=schema_str  # Add schema to context\n        )\n```\n\n4. **Update prompt templates** in `config/prompts.yaml` to reference `{{json_response_schema}}` in instructions:\n\n```yaml\ntemplate: |\n  [...]\n  \n  IMPORTANT: Respond EXACTLY in this JSON format:\n  ```json\n  {{json_response_schema}}\n  ```\n  [...]\n```\n\n**Best Practices Applied:**\n- JSON Schema draft-07 compliance[2][7]\n- Pydantic-style nested object definitions[1][2]\n- Strict `additionalProperties: false` to prevent hallucinated fields\n- Clear descriptions for each property\n- Enum constraints for priority/category fields\n- Matches existing task structure from codebase\n\n**File Location:** `src/metaagent/prompts.py` (extends Task 3 implementation)",
        "testStrategy": "Comprehensive validation tests in `tests/test_prompts.py`:\n\n1. **Schema Validation:**\n   ```bash\n   python -c \"from metaagent.prompts import JSON_RESPONSE_SCHEMA; import jsonschema; print('Schema valid')\"\n   ```\n   Verify schema parses as valid JSON Schema[3]\n\n2. **Unit Tests:**\n   - Test `JSON_RESPONSE_SCHEMA` is dict with correct structure\n   - Test `validate_json_response()` accepts valid schema-compliant JSON\n   - Test `validate_json_response()` raises ValueError for invalid JSON\n   - Test `Prompt.render()` includes schema in rendered template\n   - Test schema string is properly escaped in Jinja context\n\n3. **Integration Tests:**\n   - Mock Perplexity response with valid JSON → verify parsing succeeds\n   - Mock Perplexity response with missing `summary` → verify validation fails\n   - Mock Perplexity response with extra properties → verify rejected\n   - Test updated prompt templates render without Jinja errors\n\n4. **Schema Compliance:**\n   - Use `jsonschema` library to validate sample valid/invalid responses\n   - Verify `tasks` array matches task structure from existing codebase\n   - Test `recommendations.priority` only accepts enum values\n\n5. **Coverage:**\n   ```bash\n   pytest tests/test_prompts.py::TestJSONSchema -v --cov=metaagent/prompts\n   ```\n   Ensure 100% coverage of new schema-related code\n\n6. **E2E Verification:**\n   Run `metaagent refine --mock` and verify generated plan contains structured JSON sections\n\n**Expected pytest markers:** `@pytest.mark.schema @pytest.mark.unit @pytest.mark.integration`",
        "status": "done",
        "dependencies": [
          "3"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:32:40.069Z"
      },
      {
        "id": "20",
        "title": "Add source and has_json_schema Fields to Prompt Dataclass",
        "description": "Extend the Prompt dataclass in prompts.py with source (yaml/markdown) and has_json_schema fields to track prompt origin and built-in JSON schema presence, enabling dynamic prompt handling and validation.",
        "details": "## Implementation Steps\n\n### 1. Update Prompt Dataclass (`src/metaagent/prompts.py`)\n\nAdd the two new fields using `dataclass.field()` for proper default handling and metadata support[1][3]:\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n@dataclass\nclass Prompt:\n    id: str\n    goal: str\n    stage: str\n    template: str\n    \n    # NEW FIELDS\n    source: str = field(default='yaml')  # 'yaml' or 'markdown'\n    has_json_schema: bool = field(default=False)\n    \n    def render(self, prd: str, code_context: str, history: str, current_stage: str) -> str:\n        \"\"\"Render template with provided variables.\"\"\"\n        tmpl = Template(self.template)\n        return tmpl.render(\n            prd=prd,\n            code_context=code_context,\n            history=history,\n            current_stage=current_stage\n        )\n\n    @classmethod\n    def from_yaml(cls, data: dict) -> 'Prompt':\n        \"\"\"Factory method for YAML-loaded prompts.\"\"\"\n        return cls(\n            id=data['id'],\n            goal=data['goal'],\n            stage=data['stage'],\n            template=data['template'],\n            source='yaml',\n            has_json_schema=False  # YAML prompts don't have built-in schema\n        )\n\n    @classmethod\n    def from_markdown(cls, path: Path, prompt_id: str) -> 'Prompt':\n        \"\"\"Factory method for Markdown-loaded prompts.\"\"\"\n        with open(path, 'r') as f:\n            template = f.read()\n        return cls(\n            id=prompt_id,\n            goal=f\"Prompt from {path.name}\",\n            stage='dynamic',\n            template=template,\n            source='markdown',\n            has_json_schema='JSON_SCHEMA' in template  # Detect schema marker\n        )\n```\n\n### 2. Update Prompt Loading Logic\n\nIn the existing `PromptLibrary` or loading functions, use the new factory methods:\n\n```python\ndef load_prompts_from_yaml(yaml_path: Path) -> list[Prompt]:\n    with open(yaml_path) as f:\n        data = yaml.safe_load(f)\n    return [Prompt.from_yaml(p) for p in data['prompts'].values()]\n\n# Example for dynamic markdown loading\ndef load_prompt_from_markdown(md_path: Path, prompt_id: str) -> Prompt:\n    return Prompt.from_markdown(md_path, prompt_id)\n```\n\n### 3. Add Field Validation with __post_init__\n\n```python\n    def __post_init__(self):\n        \"\"\"Validate source field.\"\"\"\n        if self.source not in ('yaml', 'markdown'):\n            raise ValueError(f\"Invalid source: {self.source}. Must be 'yaml' or 'markdown'\")\n        \n        # Auto-detect JSON schema for markdown prompts\n        if self.source == 'markdown' and not hasattr(self, '_has_json_schema_detected'):\n            self.has_json_schema = 'JSON_SCHEMA' in self.template or 'json-schema.org' in self.template\n            self._has_json_schema_detected = True[1]\n```\n\n### 4. Update JSON_RESPONSE_SCHEMA Integration (Task 19)\n\nEnsure prompts with `has_json_schema=True` reference the `JSON_RESPONSE_SCHEMA` constant:\n\n```python\n    def get_response_schema(self) -> Optional[dict]:\n        \"\"\"Return JSON schema if prompt has built-in schema.\"\"\"\n        if self.has_json_schema:\n            from .prompts import JSON_RESPONSE_SCHEMA  # Task 19\n            return JSON_RESPONSE_SCHEMA\n        return None\n```\n\n## Best Practices Applied\n- Use `field(default=...)` for mutable defaults[1]\n- Factory methods for different loading sources\n- `__post_init__` for validation[1][4]\n- Type hints throughout\n- Backward compatible with existing YAML prompts",
        "testStrategy": "## Comprehensive Test Strategy (`tests/test_prompts.py`)\n\n### 1. Dataclass Field Tests\n```python\n def test_prompt_new_fields():\n    p = Prompt('test', 'goal', 'stage', 'template')\n    assert p.source == 'yaml'\n    assert p.has_json_schema is False\n    \n    p2 = Prompt('test', 'goal', 'stage', 'template', source='markdown', has_json_schema=True)\n    assert p2.source == 'markdown'\n    assert p2.has_json_schema is True\n```\n\n### 2. Factory Method Tests\n```python\n def test_from_yaml_factory():\n    data = {'id': 'test', 'goal': 'test', 'stage': 'test', 'template': 'test'}\n    p = Prompt.from_yaml(data)\n    assert p.source == 'yaml'\n    assert p.has_json_schema is False\n    \n def test_from_markdown_factory(tmp_path):\n    md_file = tmp_path / 'test.md'\n    md_file.write_text('# Test\\nJSON_SCHEMA')\n    p = Prompt.from_markdown(md_file, 'test')\n    assert p.source == 'markdown'\n    assert p.has_json_schema is True\n```\n\n### 3. Validation Tests\n```python\n def test_invalid_source_raises():\n    with pytest.raises(ValueError):\n        Prompt('id', 'goal', 'stage', 'template', source='invalid')\n        \n def test_post_init_detection():\n    p = Prompt('test', 'goal', 'stage', 'template with JSON_SCHEMA')\n    p.__post_init__()  # Triggers detection\n    assert p.has_json_schema is True\n```\n\n### 4. Integration Tests\n```python\n def test_loading_preserves_new_fields():\n    # Test existing YAML loading still works\n    prompts = load_prompts_from_yaml(Path('config/prompts.yaml'))\n    for p in prompts:\n        assert hasattr(p, 'source')\n        assert hasattr(p, 'has_json_schema')\n        \n def test_schema_integration():\n    p = Prompt.from_markdown(Path('config/prompt_library/meta_triage.md'), 'meta_triage')\n    assert p.has_json_schema is True  # Assuming it contains schema\n    schema = p.get_response_schema()\n    assert schema is not None\n    assert schema['$schema'] == 'http://json-schema.org/draft-07/schema#'\n```\n\n### 5. Roundtrip Serialization Test\n```python\n import json\n from dataclasses import asdict\n \n def test_dataclass_serialization():\n    p = Prompt('test', 'goal', 'stage', 'template', source='markdown', has_json_schema=True)\n    data = asdict(p)\n    assert data['source'] == 'markdown'\n    assert data['has_json_schema'] is True\n    # Reconstruct\n    p2 = Prompt(**data)\n    assert p2 == p\n```\n\nRun with: `pytest tests/test_prompts.py -v -k 'Prompt.*(source|schema)'`",
        "status": "done",
        "dependencies": [
          "3",
          "19"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:32:46.739Z"
      },
      {
        "id": "21",
        "title": "Refactor Prompt.render() to Enforce Context–Instructions–Schema Ordering",
        "description": "Refactor the Prompt.render() method so that markdown-based prompts are constructed in a consistent order—context first, then instructions, then any embedded JSON schema—while maintaining backward compatibility with existing YAML prompts.",
        "details": "## Goal\nEnsure `Prompt.render()` builds prompts in a well-defined, LLM-friendly order: **context → instructions → JSON schema**, especially for markdown-origin prompts that may include structured response schemas.\n\n## Background & Rationale\nModern prompt engineering guidance recommends:\n- Supplying **all relevant context first**, then posing the specific question or task.[4]\n- Using a **consistent structure** and clear delimiters for different prompt sections.[2][4]\n- Clearly separating **task instructions** from **output format / schema constraints** (e.g., JSON schema) to reduce ambiguity and improve adherence.[3][5]\n\nTask 19 introduces `JSON_RESPONSE_SCHEMA`, and Task 20 adds `source` and `has_json_schema` fields to `Prompt`, enabling differentiated handling for markdown prompts that embed schemas. This task leverages those fields to standardize how prompts are assembled.\n\n## Implementation Plan\n\n### 1. Analyze Current Prompt.render() Behavior\n- Open `src/metaagent/prompts.py` and review the existing `Prompt.render()` implementation from Task 3.\n- Identify:\n  - Current template variables (`prd`, `code_context`, `history`, `current_stage`, etc.).\n  - Whether templates themselves currently encode order (likely via YAML templates) versus any post-processing in `render()`.\n  - Any existing handling of JSON schema or structured output hints (may be none yet).\n\n### 2. Define Canonical Prompt Sections\nDesign the logical sections that `render()` should guarantee, independent of the raw template:\n- **Context section** (input data / background):\n  - PRD, code context, previous analysis, current stage, and any other environmental info.\n- **Instruction section**:\n  - The main analytic or action instructions, including role, goals, constraints, and style.\n- **JSON schema section** (for markdown prompts with schemas):\n  - A clearly delimited section that explains the expected JSON response structure (e.g., via `JSON_RESPONSE_SCHEMA` from Task 19 or a markdown-embedded variant).\n\nAdopt clear, consistent markdown heading conventions aligned with best practices for prompt structure:[4]\n- Example (adjust to fit existing templates):\n  - `## Context`\n  - `## Task`\n  - `## Response JSON Schema` (only when `has_json_schema` is true)\n\n### 3. Introduce an Internal Assembly Pipeline\nRefactor `Prompt.render()` to:\n\n1. **Render the base template** with Jinja2, as today, into an intermediate string (to preserve compatibility with existing YAML prompts).\n2. **If `self.source == \"markdown\"`** (from Task 20), further process the rendered content into the canonical order:\n   - Parse or heuristically split the rendered template into provisional sections:\n     - Use markdown headings or custom delimiters where available (e.g., look for `## PRD`, `## Current Codebase`, `## Instructions`, etc.).\n   - Map content into three buckets:\n     - `context_parts`\n     - `instruction_parts`\n     - `schema_parts` (for `has_json_schema` prompts or schema markers)\n   - Reassemble the final prompt as:\n     1. All `context_parts` (in original order)\n     2. All `instruction_parts`\n     3. All `schema_parts`\n   - Use clear separators (double newlines, headings) to avoid conflation.\n\n3. **If `self.source != \"markdown\"`** (default YAML prompts):\n   - Return the Jinja2-rendered string unchanged to preserve current behavior.\n\nImplementation notes:\n- Keep the schema section **at the end of the prompt** so the model reads all context and instructions before learning about the strict output format, aligning with modern guidance.[3][4]\n- Avoid brittle parsing by starting with simple, well-documented conventions; if headings are absent, treat the entire rendered template as instructions + context and append schema separately when `has_json_schema` is true.\n\n### 4. Integrate JSON_RESPONSE_SCHEMA for Markdown Prompts\n- Import `JSON_RESPONSE_SCHEMA` from the same module (Task 19) and provide it to markdown prompts that set `has_json_schema=True`.\n- For these prompts:\n  - Ensure `render()` includes a **machine-readable schema block** at the end, for example:\n    - A fenced code block with `json` and a compact version of `JSON_RESPONSE_SCHEMA`, or\n    - A textual explanation followed by the schema.\n  - Consider pre-serializing `JSON_RESPONSE_SCHEMA` with `json.dumps(..., indent=2, sort_keys=True)` for readability.\n- Keep this behavior gated by `has_json_schema` to avoid affecting prompts that do not need structured JSON output.\n\n### 5. Backward Compatibility & Configuration\n- Guarantee that existing YAML prompts (from Task 9) continue to work identically:\n  - Do **not** reorder or modify templates when `source == \"yaml\"`.\n- Document behavior in the module docstring and/or `Prompt` docstring:\n  - Explain the new `source` semantics and the markdown assembly pipeline.\n  - Describe how `has_json_schema` changes the rendered output.\n\n### 6. Implementation Best Practices\n- Follow current Python best practices:\n  - Keep `Prompt.render()` focused; if logic grows complex, factor new private helpers like `_assemble_markdown_prompt(...)` and `_append_json_schema(...)`.\n  - Ensure deterministic behavior (no random ordering or environment-dependent behavior).\n- Maintain template-agnostic design:\n  - Avoid tying logic to specific prompt IDs; key on `source` and `has_json_schema` instead.\n\n### 7. Documentation\n- Update internal developer docs or code comments to describe the new canonical order and how to author markdown prompts that take advantage of it.\n- Add examples in comments or tests demonstrating the final assembled prompt shape for markdown prompts with and without schemas.\n",
        "testStrategy": "1. **Unit Tests for Prompt Assembly (tests/test_prompts.py)**\n- Add tests that create `Prompt` instances directly (without file I/O) and call `render()` with dummy values for `prd`, `code_context`, `history`, and `current_stage`.\n\n- `test_yaml_prompt_rendering_unchanged`:\n  - Create a `Prompt` with `source='yaml'` and a simple template that includes context and instructions.\n  - Assert that `render()` returns the Jinja2-rendered string exactly as expected (no reordering or extra sections).\n\n- `test_markdown_prompt_orders_context_before_instructions`:\n  - Create a `Prompt` with `source='markdown'`, `has_json_schema=False`, and a template that intermixes context and instruction markers.\n  - After `render()`, verify that:\n    - The context heading (e.g., `## Context`) appears before the instructions heading (e.g., `## Task`).\n    - All context-related substrings appear before instruction-related substrings.\n\n- `test_markdown_prompt_places_schema_last_when_has_json_schema`:\n  - Create a `Prompt` with `source='markdown'`, `has_json_schema=True`.\n  - Call `render()` and assert that:\n    - A schema marker section (e.g., `## Response JSON Schema`) or fenced `json` block is present.\n    - This schema section appears **after** both context and instruction sections.\n\n- `test_markdown_prompt_without_schema_has_no_schema_section`:\n  - Same as above but with `has_json_schema=False`.\n  - Assert that no schema heading or JSON schema block is appended.\n\n- `test_render_includes_json_response_schema_content` (integration with Task 19):\n  - Patch or import `JSON_RESPONSE_SCHEMA` and call `render()` for a `Prompt` with `has_json_schema=True`.\n  - Parse the schema block from the rendered prompt and `json.loads` it.\n  - Assert that the resulting dict equals `JSON_RESPONSE_SCHEMA` (or is a subset with expected keys such as `summary`, `recommendations`, `tasks`).\n\n2. **Ordering and Idempotency Tests**\n- `test_markdown_prompt_reordering_is_deterministic`:\n  - Call `render()` twice on the same `Prompt` and inputs.\n  - Assert both outputs are identical.\n\n- `test_markdown_prompt_keeps_internal_section_order`:\n  - Ensure that within the context, instruction, and schema groups, the relative order of lines/sections is preserved by `render()`.\n\n3. **Regression Tests for Existing Configs (Task 9)**\n- `test_existing_yaml_prompts_render_successfully`:\n  - Load prompts via the existing `PromptLibrary`.\n  - For each YAML-backed prompt, call `render()` with minimal dummy data.\n  - Assert no exceptions are raised and that previously captured snapshots (or key substrings like `## PRD:`) are still present in the correct positions.\n\n4. **Documentation & Behavior Tests**\n- `test_prompt_source_and_has_json_schema_defaults` (integration with Task 20):\n  - Ensure `Prompt` defaults (`source='yaml'`, `has_json_schema=False`) behave as expected with `render()`.\n\n- If doctest-style examples are added to docstrings, run doctests as part of the CI pipeline to confirm sample outputs reflect the new order.\n",
        "status": "done",
        "dependencies": [
          "3",
          "9",
          "19",
          "20"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:32:53.400Z"
      },
      {
        "id": "22",
        "title": "Improve JSON Parsing Robustness in analysis.py",
        "description": "Enhance the JSON parsing logic in analysis.py with multiple extraction strategies, task normalization using the JSON_RESPONSE_SCHEMA, and fallback unstructured text processing to handle malformed LLM responses reliably.",
        "details": "Update `src/metaagent/analysis.py` to implement robust JSON parsing following best practices for LLM output extraction[1][2][6]:\n\n**1. Add Multi-Strategy JSON Extractor Class:**\n```python\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nimport json\nimport re\nfrom jsonschema import validate, ValidationError\nimport logging\n\n@dataclass\nclass ParsedAnalysisResult:\n    success: bool\n    summary: str = ''\n    recommendations: List[str] = None\n    tasks: List[Dict[str, Any]] = None\n    raw_text: str = ''\n    extraction_method: str = ''\n\nclass JSONExtractor:\n    '''Multi-strategy JSON extractor for LLM responses.'''\n    \n    def __init__(self, schema: Dict[str, Any]):\n        self.schema = schema\n        self.logger = logging.getLogger(__name__)\n    \n    def extract(self, response_text: str) -> ParsedAnalysisResult:\n        '''Try multiple extraction strategies in order of preference.'''\n        strategies = [\n            self._extract_json_block,\n            self._extract_regex_json,\n            self._extract_line_json,\n            self._fallback_unstructured\n        ]\n        \n        for strategy in strategies:\n            try:\n                result = strategy(response_text)\n                if result.success:\n                    self.logger.info(f'Successful extraction with {result.extraction_method}')\n                    return result\n            except Exception as e:\n                self.logger.debug(f'{strategy.__name__} failed: {e}')\n                continue\n        \n        return ParsedAnalysisResult(success=False, raw_text=response_text)\n```\n\n**2. Implement Extraction Strategies:**\n- `_extract_json_block`: Find JSON between ```json ... ``` markers\n- `_extract_regex_json`: Regex `r'\\{[\\s\\S]*?\\}'` for standalone objects[1]\n- `_extract_line_json`: Line-by-line parsing for NDJSON format[1]\n- `_fallback_unstructured`: Extract key fields using regex/heuristics\n\n**3. Add Task Normalization:**\n```python\ndef normalize_tasks(tasks: List[Dict], schema: Dict) -> List[Dict]:\n    '''Normalize extracted tasks against JSON_RESPONSE_SCHEMA from prompts.py[19]'''\n    normalized = []\n    for task in tasks:\n        norm_task = {\n            'title': task.get('title', 'Untitled'),\n            'description': task.get('description', ''),\n            'priority': task.get('priority', 'medium'),\n            'details': task.get('details', ''),\n            'testStrategy': task.get('testStrategy', ''),\n            'dependencies': task.get('dependencies', []),\n        }\n        # Validate against schema subset\n        try:\n            validate(instance=norm_task, schema=schema['properties']['tasks']['items'])\n        except ValidationError:\n            # Apply defaults for missing required fields\n            norm_task['description'] = norm_task['description'] or 'No description provided'\n        normalized.append(norm_task)\n    return normalized\n```\n\n**4. Update AnalysisEngine:**\n- Replace `_parse_response()` with `JSONExtractor.extract()`\n- Import `JSON_RESPONSE_SCHEMA` from `prompts.py`\n- Convert `ParsedAnalysisResult` to `AnalysisResult` with validation\n\n**5. Error Handling & Logging:**\n- Log extraction method used and any parsing errors\n- Return partial results when possible (e.g., summary only)\n- Graceful degradation to mock data in mock mode\n\n**6. Configuration:**\nAdd `json_parsing_strategy: 'strict' | 'lenient' | 'fallback'` to Config.",
        "testStrategy": "Comprehensive unit tests in `tests/test_analysis.py` using pytest and Hypothesis:\n\n1. **Strategy Tests:**\n   ```python\n   @pytest.mark.parametrize('strategy,expected', [\n       ('json_block', True),\n       ('regex_json', True),\n       ('line_json', True),\n       ('fallback', True)\n   ])\n   def test_extraction_strategies(self, strategy, expected):\n       # Test each strategy with malformed LLM responses\n   ```\n\n2. **Malformed Response Tests:**\n   - Test ```json malformed content ``` blocks\n   - Test JSON embedded in prose\n   - Test NDJSON line format\n   - Test completely unstructured text\n\n3. **Schema Normalization Tests:**\n   - Test task normalization adds missing fields\n   - Test jsonschema validation passes for normalized tasks\n   - Test malformed tasks get default values\n\n4. **Integration Tests:**\n   - Test full `analyze()` method with various mock LLM responses\n   - Verify `AnalysisResult` always populated (no None values)\n   - Test logging contains extraction method used\n\n5. **Edge Cases:**\n   - Empty response\n   - Extremely large responses (>1MB)\n   - Unicode characters\n   - Nested JSON arrays/objects[2][3]\n\n6. **Coverage & Performance:**\n   ```bash\n   pytest tests/test_analysis.py --cov=metaagent.analysis --cov-report=html\n   # Ensure >95% coverage of new parsing code\n   # Benchmark extraction time <500ms for 10k token responses\n   ```",
        "status": "done",
        "dependencies": [
          "5",
          "19"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:34:05.168Z"
      },
      {
        "id": "23",
        "title": "Add Integration Test for Codebase Digest Prompt Execution Flow",
        "description": "Create an integration test that exercises the full Codebase Digest prompt execution pipeline: loading a markdown-based prompt, rendering it with context, and verifying JSON schema appending and context–instructions–schema ordering.",
        "details": "Implementation steps:\n\n1. **Choose test location and structure**\n- Add a new integration-style test module, e.g. `tests/test_codebase_digest_integration.py`, keeping it separate from low-level unit tests to reflect its broader scope and slightly higher runtime cost.[1][3][5]\n- Use `pytest` fixtures for any reusable test data (e.g., sample markdown prompt file, sample PRD, code context, history) to keep the test clear and maintainable.[5]\n\n2. **Prepare a realistic markdown prompt fixture**\n- Under `tests/fixtures/prompts/` (or similar), add a markdown file that mimics the real **Codebase Digest** prompt used in production, including:\n  - A **context section** placeholder (e.g., headings for PRD, code, history) that will be filled by `Prompt.render()`.\n  - An **instructions section** describing the analysis or task.\n  - A **JSON schema block** that should be appended or embedded according to `JSON_RESPONSE_SCHEMA`, making sure it follows the agreed structure from Task 19.\n- Ensure the prompt metadata (id, goal, stage, source=\"markdown\", has_json_schema=True) is either:\n  - Loaded through the same mechanism as other prompts (if markdown prompts are already wired into `prompts.py`), or\n  - Constructed explicitly in the test, while still using the real markdown template contents.\n\n3. **Load prompt via existing prompt/profile loading logic**\n- Use the public API from `src/metaagent/prompts.py` (from Task 3 and Task 9) to load prompts/config where possible instead of hard-coding paths, so the integration test validates the **same configuration and loading pipeline** used by the CLI.\n- If Codebase Digest has a dedicated profile or prompt id, load it by id; otherwise, configure a minimal profile/prompt entry in a test-local YAML file under `tests/fixtures/config/` and point the prompt library at that file in the test via environment variable or injection.\n\n4. **Render the prompt with realistic context data**\n- In the test, construct representative dummy values for:\n  - `prd`: small markdown string representing a PRD.\n  - `code_context`: content that would plausibly come from the Repomix/Codebase Digest output (a few file summaries or code blocks).\n  - `history`: previous analysis or empty string.\n  - `current_stage`: the appropriate stage string for Codebase Digest.\n- Call `Prompt.render(prd, code_context, history, current_stage)` and capture the resulting string.\n\n5. **Verify context–instructions–schema ordering contract**\n- Using the ordering rules implemented in Task 21, assert that the rendered output respects **context → instructions → JSON schema**:\n  - Locate key markers or headings (e.g., `## PRD`, `## Current Codebase`, `## Instructions`, `## JSON Response Schema` or similar) using string search or regex.\n  - Assert that all context sections (PRD, code context, history, current stage) appear **before** the instructions segment.\n  - Assert that the JSON schema block appears **after** the instructions section and at the end of the prompt (aside from trailing whitespace).\n- Confirm that the context segments appear in the expected **relative order** (e.g., PRD before code context, code context before history) to maintain a predictable prompt structure.\n\n6. **Verify JSON schema content and integration**\n- Import `JSON_RESPONSE_SCHEMA` from `metaagent.prompts` and serialize it in the same way the production code does when building markdown prompts (e.g., `json.dumps(JSON_RESPONSE_SCHEMA, indent=2, sort_keys=False)` if that matches Task 19’s implementation).\n- Assert that the rendered prompt contains:\n  - A recognizable delimiter or heading introducing the schema section.\n  - The serialized schema (possibly after normalizing whitespace) so the integration test guarantees that **the actual constant** is attached, not a stale or hard-coded schema.\n- Optionally, parse the schema substring back into a dict (using `json.loads`) to validate that what is embedded is syntactically valid JSON.\n\n7. **Validate compatibility with markdown-based prompts**\n- Confirm that the integration test explicitly exercises a prompt instance with `source=\"markdown\"` and `has_json_schema=True` (from Task 20), and that these fields influence the render behavior as expected (e.g., conditional schema appending for markdown prompts only).\n- If the render logic branches based on `source` or `has_json_schema`, assert those branches are indeed taken (e.g., by checking for markdown-specific delimiters or schema markers unique to that path).\n\n8. **Keep the test maintainable and fast**\n- Avoid external dependencies (network calls, CLI invocations); focus on in-process interactions among `prompts.py`, configuration, and the template rendering engine to maintain integration-test speed and determinism.[1][3][5]\n- Use clear test names like `test_codebase_digest_markdown_prompt_full_flow()` and add short docstrings to describe intent.\n- Document in comments that this test is a **contract test** for prompt assembly rules, so future changes to the prompt format are made intentionally and with test updates.\n\n9. **CI and documentation**\n- Ensure the new integration test runs as part of the standard `pytest` suite so regressions in prompt ordering or schema embedding are caught early.[1][5]\n- Add a brief note to the project’s testing or contributing documentation describing this test as the canonical reference for Codebase Digest prompt structure, helping contributors avoid unintended prompt format changes.\n",
        "testStrategy": "1. **Run the integration test module**\n- Execute `pytest tests/test_codebase_digest_integration.py -v` and confirm the new test(s) pass locally.\n\n2. **Verify ordering invariants**\n- Temporarily introduce a deliberate violation (e.g., move the JSON schema before the instructions in `Prompt.render()`) and confirm the integration test fails with a clear assertion error pointing to ordering/context issues.\n\n3. **Verify JSON schema assertions**\n- Change a property name in `JSON_RESPONSE_SCHEMA` in `prompts.py` and ensure the test fails because the expected schema content is no longer present in the rendered prompt, proving the test is bound to the real schema constant.\n- Corrupt the schema section in the prompt template (e.g., remove closing braces) and verify the test fails when attempting to `json.loads` the embedded schema substring, confirming JSON validity checks are effective.\n\n4. **Check markdown-specific behavior**\n- Adjust the prompt instance to have `source=\"yaml\"` and `has_json_schema=False` and confirm that the test (or an additional variant) either:\n  - Fails because the schema is no longer appended, or\n  - Passes a separate assertion verifying that non-markdown prompts do not receive a schema, depending on the expected behavior.\n\n5. **Regression protection in CI**\n- Run the full test suite (`pytest -v`) and confirm overall runtime remains acceptable and that this integration test consistently passes.\n- Enable/verify CI configuration executes this test file so any future change to `Prompt.render()`, `JSON_RESPONSE_SCHEMA`, or the Codebase Digest markdown template that breaks the ordering or schema embedding will be caught automatically.[1][5]\n",
        "status": "done",
        "dependencies": [
          "3",
          "9",
          "19",
          "20",
          "21"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-14T06:35:47.015Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-14T06:35:47.017Z",
      "taskCount": 22,
      "completedCount": 22,
      "tags": [
        "master"
      ]
    }
  }
}