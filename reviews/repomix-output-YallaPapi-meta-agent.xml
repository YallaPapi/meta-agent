This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.taskmaster/
  docs/
    prd.md
  reports/
    task-complexity-report.json
  tasks/
    tasks.json
  templates/
    example_prd_rpg.txt
    example_prd.txt
  CLAUDE.md
  config.json
  state.json
config/
  prompt_library/
    ansoff_matrix_analysis.md
    architecture_api_client_code_generation.md
    architecture_api_conformance_check.md
    architecture_coupling_cohesion_analysis.md
    architecture_database_schema_documentation.md
    architecture_database_schema_review.md
    architecture_design_pattern_identification.md
    architecture_diagram_generation.md
    architecture_layer_identification.md
    architecture_refactoring_for_design_patterns.md
    bcg_matrix_analysis.md
    blue_ocean_strategy_analysis.md
    business_impact_analysis.md
    business_model_canvas_analysis.md
    competitive_positioning_map.md
    customer_journey_map_analysis.md
    evolution_code_churn_hotspot_analysis.md
    evolution_code_evolution_report_generation.md
    evolution_codebase_evolution_visualization.md
    evolution_impact_analysis_of_code_changes.md
    evolution_refactoring_recommendation_generation.md
    evolution_technical_debt_estimation.md
    improvement_best_practice_analysis.md
    improvement_language_translation.md
    improvement_refactoring.md
    jobs_to_be_done_analysis.md
    kano_model_analysis.md
    lean_canvas_analysis.md
    learning_algorithmic_storytelling.md
    learning_backend_api_documentation.md
    learning_backend_code_analysis.md
    learning_code_analogies_metaphors.md
    learning_code_evolution_visualization.md
    learning_code_pattern_recognition.md
    learning_code_refactoring_exercises.md
    learning_code_review_checklist.md
    learning_code_style_readability_analysis.md
    learning_codebase_trivia_game.md
    learning_frontend_code_analysis.md
    learning_frontend_component_documentation.md
    learning_mini_lesson_generation.md
    learning_personal_development_recommendations.md
    learning_socratic_dialogue_code_review.md
    learning_user_story_reconstruction.md
    mckinsey_7s_analysis.md
    meta_triage.md
    okr_analysis.md
    performance_bottleneck_identification.md
    performance_code_optimization_suggestions.md
    performance_concurrency_synchronization_analysis.md
    performance_configuration_tuning.md
    performance_resource_usage_profiling.md
    performance_scalability_analysis.md
    performance_test_scenario_generation.md
    pestel_analysis.md
    porters_five_forces_analysis.md
    product_market_fit_analysis.md
    quality_code_complexity_analysis.md
    quality_code_documentation_coverage_analysis.md
    quality_code_duplication_analysis.md
    quality_code_style_consistency_analysis.md
    quality_documentation_generation.md
    quality_error_analysis.md
    quality_risk_assessment.md
    security_vulnerability_analysis.md
    stakeholder_persona_generation.md
    swot_analysis.md
    tech_adoption_lifecycle_analysis.md
    testing_unit_test_generation.md
    value_chain_analysis.md
    value_proposition_canvas_analysis.md
  profiles.yaml
  prompts.yaml
docs/
  mvp_improvement_plan.md
  prd.md
src/
  metaagent/
    __init__.py
    analysis.py
    cli.py
    codebase_digest.py
    config.py
    orchestrator.py
    plan_writer.py
    prompts.py
    repomix.py
tests/
  __init__.py
  conftest.py
  test_analysis.py
  test_cli.py
  test_codebase_digest.py
  test_config.py
  test_prompts.py
.env.example
.gitignore
.mcp.json
CLAUDE.md
prd.txt
pyproject.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".taskmaster/docs/prd.md">
# Product Requirements Document (PRD)

**Project:** Meta-Agent for Automated Codebase Refinement
**Owner:** Developer
**Date:** 2025-12-14

---

## Executive Summary

This document describes a Python CLI "meta-agent" that refines an existing codebase from v0 to MVP. The system integrates with:
- A codebase packer (Repomix) to generate a single-file representation of the repo
- An analysis/planning LLM (like Perplexity) to run prompt-library driven analyses
- A coding assistant (like Claude Code) to apply code changes and run tests

The system uses a prompt library and profile system that define stages such as:
- `alignment_with_prd`
- `architecture_sanity`
- `core_flow_hardening`
- `test_suite_mvp`

---

## 1. Problem Statement

Building an initial v0 from a PRD with Claude Code is now relatively fast, but turning that v0 into a robust, production-ready MVP still requires a lot of manual review, planning, and refactoring. Human time is spent on repetitive tasks: scanning the codebase, deciding which Codebase-Digest-style prompts to use, interpreting results, and translating them into concrete implementation work.

The goal is to create a **meta-agent system** that can automatically analyze a codebase, choose appropriate prompts from a prompt library (inspired by Codebase Digest), and orchestrate Perplexity + Claude/Claude Code to iteratively refine a project from "v0 that runs" into "viable MVP," with minimal human intervention.

---

## 2. Goals & Non-Goals

### 2.1 Goals

- **G1 – Automate post-v0 refinement:**
  Given a repository and its PRD, the system should automatically run analysis cycles, generate improvement plans, and invoke Claude Code to apply changes.

- **G2 – Prompt-driven analysis engine:**
  Maintain a configurable prompt library (similar to Codebase Digest's) and allow the system to select and apply prompts in stages (alignment, architecture, hardening, tests).

- **G3 – Tool integration:**
  Integrate at least these tools into a coherent pipeline:
  - Repomix (codebase packing)
  - Perplexity (analysis/planning)
  - Claude (review/summary)
  - Claude Code (implementation)

- **G4 – Project profiles:**
  Support multiple "profiles" (e.g., backend service, automation/agent, internal tool) that define which stages and prompts to run.

- **G5 – Transparent plans and diffs:**
  Every cycle should produce:
  - A human-readable analysis summary.
  - A prioritized task list / implementation plan.
  - Links or references to actual code changes (diffs) created by Claude Code.

### 2.2 Non-Goals

- Not trying to fully replace human review for production-critical code; human sign-off is still expected.
- Not building a generic LLM platform; this is a **developer-centric pipeline** optimized for your own workflows.
- Not designing a UI beyond a basic CLI or minimal web dashboard in v1.

---

## 3. System Architecture

### 3.1 Project Structure

```
meta-agent/
├── pyproject.toml                 # Project configuration (uv/pip compatible)
├── README.md                      # Setup and usage documentation
├── .env.example                   # Environment variable template
├── src/
│   └── metaagent/
│       ├── __init__.py
│       ├── cli.py                 # CLI entrypoint (Click/Typer)
│       ├── orchestrator.py        # Main refinement orchestration logic
│       ├── repomix.py             # Repomix subprocess integration
│       ├── prompts.py             # Prompt/profile loading and rendering
│       ├── analysis.py            # Analysis LLM integration (Perplexity)
│       ├── plan_writer.py         # Plan file generation
│       └── config.py              # Configuration management
├── config/
│   ├── prompts.yaml               # Prompt templates library
│   └── profiles.yaml              # Profile definitions
├── docs/
│   └── prd.md                     # Project PRD (this document)
└── tests/
    ├── __init__.py
    ├── test_cli.py
    ├── test_orchestrator.py
    ├── test_repomix.py
    └── test_prompts.py
```

### 3.2 Key Components

#### CLI Entrypoint (`cli.py`)
- Provides `metaagent refine --profile <profile> --repo <path>` command
- Handles argument parsing and validation
- Initializes and invokes the orchestrator

#### Orchestrator (`orchestrator.py`)
- Main refinement loop coordinator
- Loads PRD, prompts, and profile configuration
- Executes stages in order defined by profile
- Aggregates results and produces improvement plan

#### Repomix Integration (`repomix.py`)
- Runs Repomix CLI via subprocess
- Reads and returns packed codebase content
- Handles errors and timeouts

#### Prompt/Profile Loading (`prompts.py`)
- Loads `config/prompts.yaml` and `config/profiles.yaml`
- Renders prompt templates with variables:
  - `{{prd}}` - PRD content
  - `{{code_context}}` - Packed codebase
  - `{{history}}` - Previous analysis summaries
  - `{{current_stage}}` - Current stage name

#### Analysis Engine (`analysis.py`)
- Wraps Perplexity API calls
- Sends rendered prompts
- Parses structured responses (summary, recommendations, tasks)
- Provides mock mode for testing

#### Plan Writer (`plan_writer.py`)
- Generates `docs/mvp_improvement_plan.md`
- Includes PRD recap, stage summaries, and prioritized task list
- Formats tasks with checkboxes for Claude Code consumption

---

## 4. High-Level User Flows

### 4.1 Flow A: PRD to v0 (baseline - outside this system)

1. User writes a PRD for a new project and saves it as `docs/prd.md` in an empty repo.
2. User opens the repo in Claude Code.
3. User instructs Claude Code to implement v0.
4. Claude Code builds the initial implementation and passes tests.
5. User commits v0 to version control.

*(This phase is manual and outside this system, but assumed as a prerequisite.)*

### 4.2 Flow B: Meta-Agent MVP Refinement

1. User runs the meta-agent CLI:
   ```bash
   metaagent refine --profile automation_agent --repo /path/to/repo
   ```

2. Meta-agent:
   - Reads `docs/prd.md`
   - Runs Repomix on the repo to produce a packed codebase file
   - Loads the configured profile and its stages

3. Stage 1 – **PRD Alignment Analysis**:
   - Selects the `alignment_with_prd` prompt
   - Calls Perplexity with PRD + Repomix output + prompt template
   - Receives: summary of gaps vs PRD, task list to close those gaps

4. Stage 2 – **Architecture / Best Practices**:
   - Selects `architecture_sanity` and/or `best_practices_analysis` prompts
   - Calls Perplexity again
   - Receives: architecture issues, refactor suggestions, and tasks

5. Stage 3 – **Feature-specific Hardening**:
   - Runs `core_flow_hardening` (retry logic, error handling)
   - Calls Perplexity with the packed code + PRD + current history
   - Receives: detailed implementation plan for robustness

6. Stage 4 – **Test Suite MVP**:
   - Runs a testing prompt (e.g., `test_suite_mvp`)
   - Receives: list of missing tests (file names, test cases)

7. Meta-agent merges all tasks into a single `mvp_improvement_plan.md`

8. User opens Claude Code on the repo and feeds in `mvp_improvement_plan.md` with instructions to execute tasks in order

9. After implementation:
   - Meta-agent can re-run to confirm improvements and suggest final tweaks

---

## 5. Functional Requirements

### 5.1 CLI / Orchestrator

- **FR1:** Provide a CLI command `metaagent refine --profile <profile> --repo <path>`
- **FR2:** Detect and load:
  - PRD file (default `docs/prd.md`)
  - Prompt library configuration (YAML)
  - Profile configuration (mapping stages to prompts)
- **FR3:** Run Repomix on the repo to produce a packed code representation
- **FR4:** Maintain a simple "history log" for each run

### 5.2 Prompt Library & Profiles

- **FR5:** Store prompt templates in `config/prompts.yaml` including:
  - `id`
  - `goal`
  - `template`
  - `stage` or `category`
  - Optional `dependencies` or `when_to_use` hints
- **FR6:** Store profiles in `config/profiles.yaml` mapping:
  - Profile name → ordered list of stages
- **FR7:** Allow selection of prompts per stage based on profile

### 5.3 Analysis Engine (Perplexity)

- **FR8:** Construct Perplexity prompts including:
  - PRD text
  - Truncated Repomix output (within context budget)
  - Run history summary
  - Stage's prompt template
- **FR9:** Expect structured responses:
  - `summary` (what was found)
  - `recommendations`
  - `tasks` (actionable items with file references)
- **FR10:** Aggregate tasks from all stages into ordered improvement plan

### 5.4 Plan & Handoff to Claude Code

- **FR11:** Write aggregated plan to `docs/mvp_improvement_plan.md`:
  - Short recap of PRD
  - Stage summaries
  - Prioritized task list with checkboxes
- **FR12:** Provide standard instruction block for Claude Code
- **FR13:** Optionally provide separate prompt for Claude to create polished review docs

### 5.5 Iteration / Re-analysis

- **FR14:** Support re-running refinement after code changes
- **FR15:** Track whether "Must-fix" tasks are resolved

---

## 6. Non-Functional Requirements

- **NFR1:** Implementation language: Python 3.10+
- **NFR2:** All orchestration via CLI; no GUI required for v1
- **NFR3:** Configurable timeouts and max token sizes for LLM calls
- **NFR4:** Keep secrets (API keys) in environment variables
- **NFR5:** Easy to extend prompt library and profiles without changing Python code

---

## 7. Configuration Layer Design

### 7.1 config/prompts.yaml Format

```yaml
prompts:
  alignment_with_prd:
    id: alignment_with_prd
    goal: "Identify gaps between current implementation and PRD requirements"
    stage: alignment
    template: |
      You are analyzing a codebase against its PRD.

      ## PRD:
      {{prd}}

      ## Current Codebase:
      {{code_context}}

      ## Previous Analysis (if any):
      {{history}}

      Current Stage: {{current_stage}}

      Please analyze and provide:
      1. Summary of alignment gaps
      2. Missing features or incomplete implementations
      3. Prioritized task list to close gaps

      Format your response as JSON with keys: summary, recommendations, tasks

  architecture_sanity:
    id: architecture_sanity
    goal: "Review architecture for best practices and maintainability"
    stage: architecture
    template: |
      Review this codebase for architectural quality.

      ## PRD Context:
      {{prd}}

      ## Codebase:
      {{code_context}}

      Analyze:
      1. Code organization and modularity
      2. Separation of concerns
      3. Error handling patterns
      4. Dependency management

      Format your response as JSON with keys: summary, recommendations, tasks

  core_flow_hardening:
    id: core_flow_hardening
    goal: "Identify robustness improvements for core flows"
    stage: hardening
    template: |
      Analyze core flows for robustness.

      ## PRD:
      {{prd}}

      ## Codebase:
      {{code_context}}

      ## Analysis History:
      {{history}}

      Focus on:
      1. Error handling and recovery
      2. Retry logic for external calls
      3. Input validation
      4. Edge cases

      Format your response as JSON with keys: summary, recommendations, tasks

  test_suite_mvp:
    id: test_suite_mvp
    goal: "Identify critical tests needed for MVP quality"
    stage: testing
    template: |
      Review test coverage for this codebase.

      ## PRD:
      {{prd}}

      ## Codebase:
      {{code_context}}

      Identify:
      1. Missing unit tests for core functions
      2. Missing integration tests for main flows
      3. Edge cases without test coverage

      Format your response as JSON with keys: summary, recommendations, tasks
```

### 7.2 config/profiles.yaml Format

```yaml
profiles:
  automation_agent:
    name: "Automation Agent"
    description: "Profile for CLI tools and automation agents"
    stages:
      - alignment_with_prd
      - architecture_sanity
      - core_flow_hardening
      - test_suite_mvp

  backend_service:
    name: "Backend Service"
    description: "Profile for API backends and services"
    stages:
      - alignment_with_prd
      - architecture_sanity
      - core_flow_hardening
      - test_suite_mvp

  internal_tool:
    name: "Internal Tool"
    description: "Profile for internal developer tools"
    stages:
      - alignment_with_prd
      - core_flow_hardening
```

---

## 8. Milestones

### M1 – Minimal Orchestrator (MVP)
- CLI command implementation
- Repomix integration
- Single profile with 2 stages: `alignment_with_prd`, `core_flow_hardening`
- Generates basic `mvp_improvement_plan.md`
- Mock analysis function with clear interface for future LLM integration

### M2 – Full Profile + Prompt Library
- Add `architecture_sanity` and `test_suite_mvp` stages
- Config-driven prompts and profiles
- Basic run history logging
- Full Perplexity API integration

### M3 – Iteration Support
- Re-run refinement after changes
- Detect remaining gaps
- Simple rule-based logic for skipping/repeating stages

### M4 – Optional Review Generation
- Claude integration for polished review documents
- Diff tracking and reporting

---

## 9. Integrations & Dependencies

| Tool | Purpose | Integration Method |
|------|---------|-------------------|
| Repomix | Codebase packing | CLI subprocess |
| Perplexity API | Analysis/planning | HTTP API |
| Claude API | Review generation (optional) | HTTP API |
| Claude Code | Implementation execution | Plan file handoff |

---

## 10. Extension Points

The following functions should be clearly marked as extension points:

```python
def run_analysis(prompt: str) -> dict:
    """
    Extension point for LLM analysis calls.

    Args:
        prompt: Rendered prompt template

    Returns:
        dict with keys: summary, recommendations, tasks
    """
    pass

def generate_review_document(analysis_results: list, prd: str) -> str:
    """
    Extension point for generating polished review documents.

    Args:
        analysis_results: List of analysis results from all stages
        prd: Original PRD content

    Returns:
        Markdown formatted review document
    """
    pass
```

---

## 11. Environment Variables

```
PERPLEXITY_API_KEY=<your-perplexity-api-key>
ANTHROPIC_API_KEY=<your-anthropic-api-key>  # Optional, for Claude integration
METAAGENT_LOG_LEVEL=INFO
METAAGENT_TIMEOUT=120
METAAGENT_MAX_TOKENS=100000
```

---

## 12. Success Criteria

1. Running `metaagent refine --profile automation_agent --repo .` produces a valid `mvp_improvement_plan.md`
2. The plan contains actionable tasks derived from PRD alignment analysis
3. The system can be extended with new prompts/profiles via YAML configuration only
4. All core functionality works with mock analysis (no API keys required for testing)
</file>

<file path=".taskmaster/reports/task-complexity-report.json">
{
	"meta": {
		"generatedAt": "2025-12-14T01:45:45.101Z",
		"tasksAnalyzed": 12,
		"totalTasks": 12,
		"analysisCount": 12,
		"thresholdScore": 5,
		"projectName": "Taskmaster",
		"usedResearch": true
	},
	"complexityAnalysis": [
		{
			"taskId": 1,
			"taskTitle": "Initialize Python Project Structure",
			"complexityScore": 3,
			"recommendedSubtasks": 4,
			"expansionPrompt": "Break down Task 1 (Initialize Python Project Structure) into 4 concrete subtasks covering: (1) authoring a modern pyproject.toml with src-layout, dependency groups for dev, and console_script entry point; (2) creating the specified src/, config/, tests/, and docs/ directory/files skeletons; (3) setting up .gitignore following Python and common tooling patterns; (4) creating a minimal but correct README.md with install, usage, and development notes. For each subtask, specify clear acceptance criteria and any tooling conventions (e.g., uv, pytest).",
			"reasoning": "This is mostly mechanical project scaffolding using standard src-layout packaging best practices and a console_script entry point, with minimal logic and straightforward verification via install and `metaagent --help`. Complexity comes from getting pyproject metadata and paths correct, but there are no tricky algorithms or integrations."
		},
		{
			"taskId": 2,
			"taskTitle": "Implement Configuration Management",
			"complexityScore": 4,
			"recommendedSubtasks": 4,
			"expansionPrompt": "Break down Task 2 (Implement Configuration Management) into 4 subtasks: (1) implement Config dataclass and from_env factory using python-dotenv and robust type conversion with defaults; (2) implement validate() including repo/config/output/PRD path checks and behavior in dev vs installed modes; (3) wire Config into a minimal caller (e.g., stub CLI) to verify usage patterns; (4) create focused pytest unit tests using monkeypatch/tmp_path for environment and filesystem scenarios. For each subtask, call out edge cases (missing env vars, non-existent paths) and test expectations.",
			"reasoning": "The module is small but sits at the core of the app; it needs careful handling of environment variables, path resolution, and validation semantics, plus testability across dev and installed modes. Still, patterns are standard and well-understood, so complexity is moderate but not high."
		},
		{
			"taskId": 3,
			"taskTitle": "Implement Prompt and Profile Loading",
			"complexityScore": 5,
			"recommendedSubtasks": 5,
			"expansionPrompt": "Break down Task 3 (Implement Prompt and Profile Loading) into 5 subtasks: (1) define Prompt and Profile dataclasses plus PromptLibrary interface; (2) implement YAML loading with safe_load, error handling, and missing-file behavior; (3) implement Prompt.render() using Jinja2 with a fixed variable contract and guardrails for template errors; (4) implement profile/query helpers (get_prompt, get_profile, list_profiles, get_prompts_for_profile) with ordering guarantees; (5) write unit tests covering YAML fixtures, rendering, missing files, and invalid data cases. Explicitly note how to keep loading side effects contained for testability.",
			"reasoning": "This introduces configuration-driven behavior, YAML parsing, templating, and lookup utilities. The implementation itself is straightforward, but correctness depends on handling malformed configs, missing keys, and template rendering errors, and on designing a stable contract for templates, which adds moderate complexity."
		},
		{
			"taskId": 4,
			"taskTitle": "Implement Repomix Integration",
			"complexityScore": 6,
			"recommendedSubtasks": 5,
			"expansionPrompt": "Break down Task 4 (Implement Repomix Integration) into 5 subtasks: (1) design RepomixResult dataclass and RepomixRunner API; (2) implement pack() using subprocess.run with cwd, temp file handling, and robust cleanup in finally; (3) implement error handling branches for non-zero exit, TimeoutExpired, and FileNotFoundError with clear messages; (4) implement truncate_content() with a well-documented token-to-char heuristic and tests around boundaries; (5) write unit tests using monkeypatch to mock subprocess.run and filesystem interactions, covering success, failure, timeout, and missing binary cases. Include notes on making this stable in CI where Node/repomix may not be installed.",
			"reasoning": "Although the code is not large, integrating with an external CLI via subprocess, timeouts, temp files, and cleanup adds meaningful edge cases. Achieving deterministic, platform-agnostic tests with mocks increases complexity beyond simple I/O code."
		},
		{
			"taskId": 5,
			"taskTitle": "Implement Analysis Engine with Mock Mode",
			"complexityScore": 7,
			"recommendedSubtasks": 6,
			"expansionPrompt": "Break down Task 5 (Implement Analysis Engine with Mock Mode) into 6 subtasks: (1) define AnalysisResult dataclass and AnalysisEngine protocol; (2) implement MockAnalysisEngine with deterministic outputs for tests; (3) implement PerplexityAnalysisEngine HTTP client using httpx, including headers, payload shape, timeout, and error handling; (4) implement _parse_response() to robustly extract JSON from raw content or ```json``` blocks and degrade gracefully on invalid JSON; (5) implement create_analysis_engine() factory with clear rules for mock vs real engine; (6) write unit tests that fully mock httpx.Client to cover success, HTTP errors, malformed model responses, and JSON parse fallbacks, ensuring no real network access. Highlight security considerations around API keys and logging.",
			"reasoning": "This module integrates with an external API, handles network errors, timeouts, and JSON parsing of sometimes messy LLM responses. It also defines a key extension point and must be robust and testable without real API calls, which raises both design and testing complexity."
		},
		{
			"taskId": 6,
			"taskTitle": "Implement Plan Writer",
			"complexityScore": 5,
			"recommendedSubtasks": 5,
			"expansionPrompt": "Break down Task 6 (Implement Plan Writer) into 5 subtasks: (1) define StageResult dataclass and PlanWriter constructor creating output_dir; (2) implement write_plan() to assemble sections (metadata, PRD summary, stage summaries, prioritized tasks, instructions) into Markdown; (3) implement _extract_prd_summary() with length limits and line-aware truncation; (4) implement _aggregate_tasks() and _priority_badge() with clear priority ordering and emoji mapping; (5) write unit tests using tmp_path to verify file creation, content sections, priority sorting, and behavior with empty tasks/stages. Note any i18n/encoding considerations for emojis and UTF-8 writes.",
			"reasoning": "The logic is mostly deterministic string and list processing, but it coordinates multiple inputs (PRD, multi-stage results, prioritization) into a user-facing artifact. Edge cases around empty inputs, ordering, and Markdown formatting need attention, yet overall complexity remains moderate."
		},
		{
			"taskId": 7,
			"taskTitle": "Implement Orchestrator",
			"complexityScore": 8,
			"recommendedSubtasks": 7,
			"expansionPrompt": "Break down Task 7 (Implement Orchestrator) into 7 subtasks: (1) define RunHistory and RefinementResult data structures; (2) implement Orchestrator.__init__ wiring Config, PromptLibrary, RepomixRunner, AnalysisEngine factory, and PlanWriter; (3) implement _load_prd() with clear failure behavior; (4) implement Repomix integration inside refine(), including warning handling and context truncation; (5) implement stage loop: fetching prompts for a profile, rendering with history, invoking analysis, accumulating StageResult, and updating RunHistory; (6) implement plan generation and final RefinementResult assembly with success/error semantics; (7) write unit tests that heavily mock PromptLibrary, RepomixRunner, and AnalysisEngine to cover success path, missing profile, missing PRD, Repomix failures, partial stage failures, history accumulation, and no-stages cases. Emphasize separation of orchestration logic from I/O for testability.",
			"reasoning": "This is the central workflow coordinator that ties together configuration, prompts, external tooling, analysis engine, and plan writing. It must manage control flow, error propagation, partial failures, and history across multiple stages. The number of dependencies and branching paths makes this one of the most complex pieces in the system."
		},
		{
			"taskId": 8,
			"taskTitle": "Implement CLI Entrypoint",
			"complexityScore": 6,
			"recommendedSubtasks": 6,
			"expansionPrompt": "Break down Task 8 (Implement CLI Entrypoint) into 6 subtasks: (1) set up Typer app structure and main() entrypoint compatible with pyproject console_script; (2) implement logging setup with RichHandler and configurable levels; (3) implement refine command: argument parsing, repo path resolution, Config.from_env usage, optional PRD override, config validation, profile existence check, orchestrator invocation, and result reporting; (4) implement list_profiles command: config_dir override, PromptLibrary usage, and formatted output; (5) ensure CLI error handling exits with proper codes and user-friendly Rich messages; (6) write tests using typer.testing.CliRunner that exercise success and failure paths, mock mode, verbose flag, invalid repo/profile, and help text. Note cross-platform path considerations and how to avoid hitting real APIs/subprocesses in tests.",
			"reasoning": "While Typer simplifies argument parsing, the CLI still has to coordinate configuration, validation, and orchestrator invocation, and present clear UX and exit codes. It’s at the boundary with the user and must handle many error conditions, increasing complexity, especially for robust tests."
		},
		{
			"taskId": 9,
			"taskTitle": "Create Complete Configuration Files",
			"complexityScore": 3,
			"recommendedSubtasks": 3,
			"expansionPrompt": "Break down Task 9 (Create Complete Configuration Files) into 3 subtasks: (1) author prompts.yaml exactly per PRD, adding required Jinja placeholders (prd, code_context, history, current_stage) and validate formatting; (2) author profiles.yaml with all profiles and stage sequences, ensuring all referenced stages exist in prompts.yaml; (3) write small validation tests or scripts (e.g., in tests/test_prompts.py) to load both YAML files via PromptLibrary and assert prompt/profile counts, renderability, and referential integrity. Note how to keep these configs environment-agnostic for CI.",
			"reasoning": "This is primarily static configuration authoring with YAML. The main risk is mismatched IDs, bad indentation, or missing template variables, all of which can be caught with simple validation tests. Implementation effort and technical complexity are relatively low."
		},
		{
			"taskId": 10,
			"taskTitle": "Add Comprehensive Test Suite",
			"complexityScore": 8,
			"recommendedSubtasks": 7,
			"expansionPrompt": "Break down Task 10 (Add Comprehensive Test Suite) into 7 subtasks: (1) create tests/conftest.py with shared fixtures for temp dirs, sample PRD/code, mock config YAMLs, and test repos; (2) implement tests for config.py (env loading, defaults, validation, path resolution); (3) implement tests for prompts.py (loading, rendering, missing files, invalid data); (4) implement tests for repomix.py with mocked subprocess.run covering success/failure/timeout/FileNotFoundError and truncation logic; (5) implement tests for analysis.py with mocked httpx, including mock engine and parse fallbacks; (6) implement tests for plan_writer.py (file output, sections, priority sorting, edge cases); (7) implement orchestrator and CLI tests using mocks/CliRunner, add coverage configuration, and mark slow/integration tests. Explicitly plan for deterministic, isolated tests suitable for CI with no external services or Node dependencies.",
			"reasoning": "Designing and implementing a cohesive, high-coverage test suite across multiple modules and external boundaries is substantial work. It requires thoughtful fixture design, extensive mocking, and ensuring tests remain fast and reliable in CI. While each individual test is not complex, the breadth and integration make overall complexity high."
		},
		{
			"taskId": 11,
			"taskTitle": "Create Documentation and Environment Setup",
			"complexityScore": 4,
			"recommendedSubtasks": 3,
			"expansionPrompt": "Break down Task 11 (Create Documentation and Environment Setup) into 3 subtasks: (1) write README.md with clear overview, installation (uv and pip), configuration, usage examples, profiles description, output description, development workflow, and project structure; (2) create .env.example listing all relevant environment variables with safe placeholder values and comments; (3) manually validate docs by following the README in a clean environment and cross-checking that .env.example matches what Config.from_env expects and what the code actually uses. Note how to keep examples up to date with CLI and config behavior.",
			"reasoning": "This is mostly documentation and example configuration work. It must accurately reflect the implemented behavior and be tested manually, but it does not involve intricate code or algorithms. Complexity is modest, driven mainly by the need for consistency with the evolving codebase."
		},
		{
			"taskId": 12,
			"taskTitle": "End-to-End Integration Test with Mock Mode",
			"complexityScore": 7,
			"recommendedSubtasks": 5,
			"expansionPrompt": "Break down Task 12 (End-to-End Integration Test with Mock Mode) into 5 subtasks: (1) design sample_repo fixture that mirrors a realistic but minimal project with docs/prd.md and simple src code; (2) write e2e tests using CliRunner to invoke `metaagent refine` in mock mode for different profiles, asserting exit codes and key output strings; (3) assert that docs/mvp_improvement_plan.md is created and contains required sections and profile info; (4) add tests for failure scenarios (e.g., missing PRD) and for list-profiles behavior; (5) ensure tests are isolated via tmp_path, run quickly using mock analysis (no network), and are optionally marked as integration tests. Highlight any ordering dependencies with other tasks/tests.",
			"reasoning": "These tests exercise the full stack—CLI, orchestrator, config, prompts, plan writer—within an isolated environment. While logic is not complex, coordinating filesystem setup, environment, and assertions across modules, and keeping tests deterministic and fast, makes this moderately high in complexity."
		}
	]
}
</file>

<file path=".taskmaster/tasks/tasks.json">
{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Initialize Python Project Structure",
        "description": "Set up the Python project with pyproject.toml, create the directory structure as specified in the PRD, and configure development dependencies.",
        "details": "Create the following structure:\n\n1. Create `pyproject.toml` with:\n   - Project metadata (name='metaagent', version='0.1.0')\n   - Python 3.10+ requirement\n   - Dependencies: click or typer, pyyaml, httpx, python-dotenv, jinja2\n   - Dev dependencies: pytest, pytest-cov, pytest-asyncio\n   - Entry point: metaagent = 'metaagent.cli:main'\n\n2. Create directory structure:\n   ```\n   src/metaagent/__init__.py\n   src/metaagent/cli.py (stub)\n   src/metaagent/orchestrator.py (stub)\n   src/metaagent/repomix.py (stub)\n   src/metaagent/prompts.py (stub)\n   src/metaagent/analysis.py (stub)\n   src/metaagent/plan_writer.py (stub)\n   src/metaagent/config.py (stub)\n   config/prompts.yaml (empty structure)\n   config/profiles.yaml (empty structure)\n   tests/__init__.py\n   tests/test_cli.py (stub)\n   tests/test_orchestrator.py (stub)\n   docs/\n   ```\n\n3. Update .gitignore for Python:\n   - __pycache__/, *.pyc, *.pyo\n   - .venv/, venv/, .env\n   - dist/, build/, *.egg-info/\n   - .pytest_cache/, .coverage\n\n4. Create README.md with basic project description and setup instructions.",
        "testStrategy": "Verify project structure exists with `ls -R`. Verify `uv pip install -e .` or `pip install -e .` succeeds. Verify `metaagent --help` runs without errors (will show help from stub CLI).",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Author modern pyproject.toml with src-layout and dependency groups",
            "description": "Create pyproject.toml file with project metadata, Python 3.10+ requirement, runtime and dev dependencies, and console_script entry point using modern standards.",
            "dependencies": [],
            "details": "Use [tool.uv] or [build-system] with hatchling; define [project] table with name='metaagent', version='0.1.0', requires-python='>=3.10'; dependencies=['click', 'pyyaml', 'httpx', 'python-dotenv', 'jinja2']; optional-dependencies.dev=['pytest', 'pytest-cov', 'pytest-asyncio']; [project.scripts] metaagent='metaagent.cli:main'; enable src-layout with packages=['src/metaagent'].",
            "status": "pending",
            "testStrategy": "Verify with `uv pip install -e .` or `pip install -e .` succeeds without errors; check `metaagent --help` displays CLI help from stub; validate TOML syntax and metadata with `uv project info`.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create src/, config/, tests/, and docs/ directory skeletons",
            "description": "Set up complete directory structure with all specified empty stub files as per PRD requirements.",
            "dependencies": [
              1
            ],
            "details": "Create directories: src/metaagent/, config/, tests/, docs/; add __init__.py files to src/metaagent/ and tests/; create stub Python files: cli.py, orchestrator.py, repomix.py, prompts.py, analysis.py, plan_writer.py, config.py; create empty YAML files: config/prompts.yaml, config/profiles.yaml.",
            "status": "pending",
            "testStrategy": "Run `ls -R src config tests docs` to verify exact structure matches spec; ensure all stub files exist and contain pass or basic if __name__ == '__main__' guards.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Set up .gitignore for Python projects and common tooling",
            "description": "Create comprehensive .gitignore covering Python caches, virtualenvs, build artifacts, and testing caches.",
            "dependencies": [
              1
            ],
            "details": "Include patterns: __pycache__/, *.pyc, *.pyo, *.pyd; .venv/, venv/, ENV/, env/; .env, .env.*; dist/, build/, *.egg-info/, *.whl; .pytest_cache/, .coverage, .coverage.*; recommend adding .uv/, uv.lock if using uv tooling.",
            "status": "pending",
            "testStrategy": "Verify git ignores files by creating test files matching patterns and running `git status --ignored`; ensure common ignore files like .env are properly excluded.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create minimal README.md with install, usage, and dev notes",
            "description": "Write basic project README including project description, installation instructions, basic usage, and development setup.",
            "dependencies": [
              1,
              2
            ],
            "details": "Include sections: # MetaAgent (AI-powered repo refinement); ## Installation (`uv venv; uv pip install -e .` or pip); ## Usage (`metaagent --help`); ## Development (pytest, pre-commit if added); ## Structure overview; link to PRD/docs.",
            "status": "pending",
            "testStrategy": "Verify README renders correctly in GitHub/Markdown viewer; check all commands in install/usage sections execute successfully after project setup.",
            "parentId": "undefined"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down Task 1 (Initialize Python Project Structure) into 4 concrete subtasks covering: (1) authoring a modern pyproject.toml with src-layout, dependency groups for dev, and console_script entry point; (2) creating the specified src/, config/, tests/, and docs/ directory/files skeletons; (3) setting up .gitignore following Python and common tooling patterns; (4) creating a minimal but correct README.md with install, usage, and development notes. For each subtask, specify clear acceptance criteria and any tooling conventions (e.g., uv, pytest).",
        "updatedAt": "2025-12-14T01:59:42.237Z"
      },
      {
        "id": "2",
        "title": "Implement Configuration Management",
        "description": "Create the config.py module to handle environment variables, application settings, and provide a centralized configuration interface.",
        "details": "Implement `src/metaagent/config.py`:\n\n```python\nimport os\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\n\n@dataclass\nclass Config:\n    perplexity_api_key: str | None\n    anthropic_api_key: str | None\n    log_level: str\n    timeout: int\n    max_tokens: int\n    repo_path: Path\n    prd_path: Path\n    config_dir: Path\n    output_dir: Path\n\n    @classmethod\n    def from_env(cls, repo_path: Path | None = None) -> 'Config':\n        load_dotenv()\n        repo = repo_path or Path.cwd()\n        return cls(\n            perplexity_api_key=os.getenv('PERPLEXITY_API_KEY'),\n            anthropic_api_key=os.getenv('ANTHROPIC_API_KEY'),\n            log_level=os.getenv('METAAGENT_LOG_LEVEL', 'INFO'),\n            timeout=int(os.getenv('METAAGENT_TIMEOUT', '120')),\n            max_tokens=int(os.getenv('METAAGENT_MAX_TOKENS', '100000')),\n            repo_path=repo,\n            prd_path=repo / 'docs' / 'prd.md',\n            config_dir=Path(__file__).parent.parent.parent / 'config',\n            output_dir=repo / 'docs'\n        )\n\n    def validate(self) -> list[str]:\n        \"\"\"Return list of validation errors, empty if valid.\"\"\"\n        errors = []\n        if not self.repo_path.exists():\n            errors.append(f'Repository path does not exist: {self.repo_path}')\n        if not self.config_dir.exists():\n            errors.append(f'Config directory does not exist: {self.config_dir}')\n        return errors\n```\n\nKey features:\n- Load from environment variables with defaults\n- Dataclass for type safety and immutability\n- Path resolution for repo, PRD, config, and output directories\n- Validation method for required paths\n- Support for both installed package and development mode paths",
        "testStrategy": "Unit tests in `tests/test_config.py`:\n1. Test Config.from_env() with mocked environment variables\n2. Test default values when env vars not set\n3. Test validate() returns errors for missing paths\n4. Test validate() returns empty list for valid paths\n5. Test path resolution works correctly",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Config dataclass and from_env() factory with dotenv loading and robust defaults",
            "description": "Create the Config dataclass and the from_env() classmethod to load configuration from environment variables using python-dotenv, including sensible defaults and safe type conversion.",
            "dependencies": [],
            "details": "- Define the Config dataclass exactly as specified (API keys, log_level, timeout, max_tokens, repo_path, prd_path, config_dir, output_dir) with appropriate type hints.\n- Implement from_env() to call load_dotenv() first so that .env files are respected.\n- Accept an optional repo_path argument; if not provided, default to Path.cwd().\n- Read PERPLEXITY_API_KEY and ANTHROPIC_API_KEY from the environment; allow them to be missing (set to None) without raising.\n- Read METAAGENT_LOG_LEVEL, METAAGENT_TIMEOUT, METAAGENT_MAX_TOKENS with robust defaulting and type conversion (e.g., fall back to safe defaults if env values are malformed integers).\n- Compute prd_path as repo_path / 'docs' / 'prd.md'.\n- Compute config_dir relative to the installed package layout (e.g., Path(__file__).parent.parent.parent / 'config') so it works in both editable and installed modes.\n- Compute output_dir as repo_path / 'docs', creating only paths in later code, not here.\n- Edge cases to consider and later test: missing env vars (ensure defaults/None used), invalid integer values for timeout/max_tokens (decide on fallback behavior, e.g., catch ValueError and revert to defaults rather than crash), unusual CWD when repo_path is not passed.\n- Do not perform filesystem existence checks here; leave that to validate().",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement validate() with repo/config/output/PRD path checks and dev vs installed behavior",
            "description": "Implement the validate() method to check existence and correctness of key paths (repo_path, config_dir, output_dir, prd_path), handling differences between development and installed package modes.",
            "dependencies": [
              1
            ],
            "details": "- Extend validate() to return a list of human-readable error strings describing configuration problems; keep empty list on success.\n- Check that repo_path exists and is a directory; if not, add an error.\n- Check that config_dir exists and is a directory; if not, add an error describing that configuration assets are missing.\n- Check that output_dir exists or, if your design requires it, that its parent exists; decide whether absence of output_dir is an error (e.g., might be created later) or only a warning (still represented as an error string if you choose to enforce creation upfront).\n- Check that prd_path exists and is a file; add an error if the PRD is missing so downstream components (orchestrator, plan writer) can fail early.\n- For dev vs installed modes, base behavior only on the resolved config_dir path: for example, treat a config_dir located inside the source tree (e.g., under src/metaagent/../config) as dev mode, and a site-packages-like path as installed mode; ensure validate() behaves consistently in both, without hard-coding environment-specific assumptions.\n- Ensure validate() never raises; all issues must be represented as error messages so callers can decide how to handle them.\n- Edge cases and expectations: non-existent repo_path passed explicitly; repo_path that exists but lacks docs/ or prd.md; config_dir missing because package assets not installed; output_dir pointing to a file instead of a directory; paths that are symlinks (still treated as existing).",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Wire Config into a minimal caller (stub CLI or script) to exercise usage patterns",
            "description": "Create a small entrypoint (e.g., a stub CLI command or script) that constructs Config.from_env(), runs validate(), and reports configuration status, to verify real-world usage patterns.",
            "dependencies": [
              1,
              2
            ],
            "details": "- Implement a minimal callable entrypoint, such as a function main() or a tiny Typer/Rich-based CLI stub under src/metaagent, that imports and uses Config.\n- In the entrypoint, call Config.from_env() with an optional repo_path argument (e.g., from CLI flag or default Path.cwd()).\n- Immediately call config.validate() and, if errors are returned, print them in a user-friendly way and exit with a non-zero status code.\n- On success (no validation errors), print or log key configuration values (e.g., repo_path, prd_path, config_dir, output_dir, timeout, log_level) without exposing sensitive API keys.\n- Ensure the entrypoint demonstrates how higher-level components (orchestrator, CLI) will interact with Config, including typical error-handling flow.\n- Edge cases and expectations: behavior when repo_path points to a non-existent directory (entrypoint should show validation errors, not crash); behavior with missing PRD or config_dir (clear error output); running from different working directories to confirm repo_path resolution is intuitive.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create focused pytest unit tests for Config using monkeypatch and tmp_path",
            "description": "Add pytest unit tests covering environment-variable handling, default values, path resolution, and validate() behavior across various filesystem scenarios using monkeypatch and tmp_path.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "- Implement tests in tests/test_config.py structured around the provided test strategy.\n- Use monkeypatch to control os.environ for Config.from_env() tests, including setting PERPLEXITY_API_KEY, ANTHROPIC_API_KEY, METAAGENT_LOG_LEVEL, METAAGENT_TIMEOUT, and METAAGENT_MAX_TOKENS, and clearing them to test defaults.\n- Test edge cases of env parsing: missing API keys (must yield None), missing optional settings (must use defaults), and invalid integer strings for timeout/max_tokens (must fall back safely instead of raising).\n- Use tmp_path to create ephemeral directory structures representing a fake repo with docs/, prd.md, and optional config/ directory, passing these paths into Config instances for validation tests.\n- Write tests where repo_path does not exist, config_dir does not exist, output_dir points to a file instead of a directory, and prd.md is missing; assert that validate() returns clear error messages for each condition.\n- Add a test to confirm that path resolution from from_env() (repo_path, prd_path, config_dir, output_dir) matches expectations when run from different working directories or with an explicit repo_path.\n- Optionally add an integration-style test that uses the minimal caller entrypoint to confirm end-to-end behavior (construct, validate, and report), asserting correct exit codes and output using pytest’s capsys or CliRunner if using Typer.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down Task 2 (Implement Configuration Management) into 4 subtasks: (1) implement Config dataclass and from_env factory using python-dotenv and robust type conversion with defaults; (2) implement validate() including repo/config/output/PRD path checks and behavior in dev vs installed modes; (3) wire Config into a minimal caller (e.g., stub CLI) to verify usage patterns; (4) create focused pytest unit tests using monkeypatch/tmp_path for environment and filesystem scenarios. For each subtask, call out edge cases (missing env vars, non-existent paths) and test expectations.",
        "updatedAt": "2025-12-14T01:59:48.825Z"
      },
      {
        "id": "3",
        "title": "Implement Prompt and Profile Loading",
        "description": "Create the prompts.py module to load and render prompt templates from YAML configuration, and load profile definitions that map stages to prompts.",
        "details": "Implement `src/metaagent/prompts.py`:\n\n```python\nfrom pathlib import Path\nfrom dataclasses import dataclass\nimport yaml\nfrom jinja2 import Template\n\n@dataclass\nclass Prompt:\n    id: str\n    goal: str\n    stage: str\n    template: str\n\n    def render(self, prd: str, code_context: str, history: str, current_stage: str) -> str:\n        \"\"\"Render template with provided variables.\"\"\"\n        tmpl = Template(self.template)\n        return tmpl.render(\n            prd=prd,\n            code_context=code_context,\n            history=history,\n            current_stage=current_stage\n        )\n\n@dataclass\nclass Profile:\n    name: str\n    description: str\n    stages: list[str]\n\nclass PromptLibrary:\n    def __init__(self, config_dir: Path):\n        self.config_dir = config_dir\n        self._prompts: dict[str, Prompt] = {}\n        self._profiles: dict[str, Profile] = {}\n        self._load()\n\n    def _load(self):\n        # Load prompts.yaml\n        prompts_file = self.config_dir / 'prompts.yaml'\n        if prompts_file.exists():\n            with open(prompts_file) as f:\n                data = yaml.safe_load(f)\n            for pid, pdata in data.get('prompts', {}).items():\n                self._prompts[pid] = Prompt(\n                    id=pdata['id'],\n                    goal=pdata['goal'],\n                    stage=pdata['stage'],\n                    template=pdata['template']\n                )\n\n        # Load profiles.yaml\n        profiles_file = self.config_dir / 'profiles.yaml'\n        if profiles_file.exists():\n            with open(profiles_file) as f:\n                data = yaml.safe_load(f)\n            for pname, pdata in data.get('profiles', {}).items():\n                self._profiles[pname] = Profile(\n                    name=pdata['name'],\n                    description=pdata['description'],\n                    stages=pdata['stages']\n                )\n\n    def get_prompt(self, prompt_id: str) -> Prompt | None:\n        return self._prompts.get(prompt_id)\n\n    def get_profile(self, profile_name: str) -> Profile | None:\n        return self._profiles.get(profile_name)\n\n    def list_profiles(self) -> list[str]:\n        return list(self._profiles.keys())\n\n    def get_prompts_for_profile(self, profile_name: str) -> list[Prompt]:\n        profile = self.get_profile(profile_name)\n        if not profile:\n            return []\n        return [self._prompts[s] for s in profile.stages if s in self._prompts]\n```\n\nPopulate `config/prompts.yaml` and `config/profiles.yaml` with the exact content from PRD Section 7.1 and 7.2.",
        "testStrategy": "Unit tests in `tests/test_prompts.py`:\n1. Test loading prompts from YAML file\n2. Test loading profiles from YAML file\n3. Test Prompt.render() with template variables\n4. Test get_prompts_for_profile() returns correct ordered list\n5. Test handling of missing files gracefully\n6. Test list_profiles() returns all profile names",
        "priority": "high",
        "dependencies": [
          "1",
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Prompt and Profile dataclasses and PromptLibrary public interface",
            "description": "Create the Prompt and Profile dataclasses and sketch the PromptLibrary API surface that will manage loading and accessing prompts and profiles.",
            "dependencies": [
              3
            ],
            "details": "Implement Prompt and Profile as @dataclass structures with the fields specified in the PRD (id, goal, stage, template for Prompt; name, description, stages for Profile). Define the PromptLibrary __init__(config_dir: Path) signature and internal dictionaries for prompts and profiles. Stub out _load(), get_prompt(), get_profile(), list_profiles(), and get_prompts_for_profile() with type hints and docstrings but no logic yet, ensuring the interface is stable for later use by orchestrator and CLI.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement YAML loading with safe_load, validation, and missing-file behavior",
            "description": "Add YAML loading logic to PromptLibrary that reads prompts.yaml and profiles.yaml, using yaml.safe_load with graceful handling of missing files and malformed content.",
            "dependencies": [
              1
            ],
            "details": "In PromptLibrary._load(), implement reading config_dir / 'prompts.yaml' and config_dir / 'profiles.yaml' using context managers and yaml.safe_load. If a file does not exist, skip loading without raising, so construction of PromptLibrary has minimal side effects and is deterministic. Add minimal schema validation (e.g., ensure top-level keys 'prompts' and 'profiles' are dicts, required fields exist) and either log or raise clear exceptions for invalid structures while keeping side effects confined to object state. Avoid any global state; all loaded data should be stored only in self._prompts and self._profiles so tests can inject a temporary config_dir with fixture YAML files.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Prompt.render() using Jinja2 with fixed variable contract and error guardrails",
            "description": "Complete the Prompt.render() method so it renders templates with a fixed set of variables via Jinja2, handling template errors safely.",
            "dependencies": [
              1,
              2
            ],
            "details": "Use jinja2.Template to compile self.template and render it with a fixed context including prd, code_context, history, and current_stage. Decide on behavior for template syntax or rendering errors (e.g., catch TemplateError and either re-raise a custom exception or return a fallback string) to prevent hard crashes in callers like the orchestrator. Keep rendering pure and side-effect free: no file I/O or logging inside render, so unit tests can call it deterministically. Document the variable contract in the render docstring so YAML templates in prompts.yaml can rely on a stable set of fields.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement profile and prompt lookup helpers with ordering guarantees",
            "description": "Fill in PromptLibrary helper methods for querying prompts and profiles, ensuring get_prompts_for_profile preserves the profile-defined stage order and handles missing mappings robustly.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement get_prompt(prompt_id) and get_profile(profile_name) as simple dictionary lookups returning None when not found. Implement list_profiles() to return profile names in deterministic order (e.g., sorted or insertion order, documented explicitly). Implement get_prompts_for_profile(profile_name) to resolve the profile, then map profile.stages entries to Prompt instances, skipping unknown prompt IDs safely while preserving the original stage order. Keep logic purely in-memory so tests can construct PromptLibrary against temporary YAML directories without global side effects.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write unit tests for YAML loading, rendering, helpers, and error cases",
            "description": "Create tests in tests/test_prompts.py that cover YAML fixtures, prompt rendering, helper behaviors, missing files, and invalid data, while keeping loading side effects isolated.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Add pytest-based tests that use temporary directories to host prompts.yaml and profiles.yaml fixtures derived from PRD sections 7.1 and 7.2. Test that PromptLibrary loads valid YAML into correct Prompt and Profile objects, that Prompt.render() correctly interpolates all supported variables, and that get_prompts_for_profile() returns prompts in the expected stage order. Add tests for behavior when prompts.yaml and/or profiles.yaml are missing (PromptLibrary still constructs and methods return empty/None appropriately) and when YAML contains invalid structures or missing required fields. Ensure each test constructs its own PromptLibrary instance pointing at an isolated temp config_dir so there are no cross-test side effects or reliance on global filesystem state.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down Task 3 (Implement Prompt and Profile Loading) into 5 subtasks: (1) define Prompt and Profile dataclasses plus PromptLibrary interface; (2) implement YAML loading with safe_load, error handling, and missing-file behavior; (3) implement Prompt.render() using Jinja2 with a fixed variable contract and guardrails for template errors; (4) implement profile/query helpers (get_prompt, get_profile, list_profiles, get_prompts_for_profile) with ordering guarantees; (5) write unit tests covering YAML fixtures, rendering, missing files, and invalid data cases. Explicitly note how to keep loading side effects contained for testability.",
        "updatedAt": "2025-12-14T01:59:55.377Z"
      },
      {
        "id": "4",
        "title": "Implement Repomix Integration",
        "description": "Create the repomix.py module to run Repomix CLI as a subprocess and return the packed codebase content.",
        "details": "Implement `src/metaagent/repomix.py`:\n\n```python\nimport subprocess\nimport tempfile\nfrom pathlib import Path\nfrom dataclasses import dataclass\n\n@dataclass\nclass RepomixResult:\n    success: bool\n    content: str\n    error: str | None = None\n\nclass RepomixRunner:\n    def __init__(self, timeout: int = 120):\n        self.timeout = timeout\n\n    def pack(self, repo_path: Path) -> RepomixResult:\n        \"\"\"\n        Run Repomix on the given repository and return packed content.\n        \n        Args:\n            repo_path: Path to the repository to pack\n            \n        Returns:\n            RepomixResult with success status and content or error\n        \"\"\"\n        with tempfile.NamedTemporaryFile(suffix='.txt', delete=False) as tmp:\n            output_file = Path(tmp.name)\n        \n        try:\n            result = subprocess.run(\n                ['npx', '-y', 'repomix', '--output', str(output_file)],\n                cwd=str(repo_path),\n                capture_output=True,\n                text=True,\n                timeout=self.timeout\n            )\n            \n            if result.returncode != 0:\n                return RepomixResult(\n                    success=False,\n                    content='',\n                    error=f'Repomix failed: {result.stderr}'\n                )\n            \n            content = output_file.read_text(encoding='utf-8')\n            return RepomixResult(success=True, content=content)\n            \n        except subprocess.TimeoutExpired:\n            return RepomixResult(\n                success=False,\n                content='',\n                error=f'Repomix timed out after {self.timeout}s'\n            )\n        except FileNotFoundError:\n            return RepomixResult(\n                success=False,\n                content='',\n                error='npx/repomix not found. Ensure Node.js is installed.'\n            )\n        finally:\n            output_file.unlink(missing_ok=True)\n\n    def truncate_content(self, content: str, max_tokens: int) -> str:\n        \"\"\"Truncate content to fit within token budget (rough char estimate).\"\"\"\n        # Rough estimate: 1 token ≈ 4 characters\n        max_chars = max_tokens * 4\n        if len(content) <= max_chars:\n            return content\n        return content[:max_chars] + '\\n\\n[Content truncated to fit token limit]'\n```\n\nKey considerations:\n- Use npx to run repomix without global installation\n- Capture output to temp file and read content\n- Handle timeout errors gracefully\n- Handle missing npx/node gracefully\n- Provide truncation utility for context budget",
        "testStrategy": "Unit tests in `tests/test_repomix.py`:\n1. Test successful pack with mock subprocess (mock subprocess.run)\n2. Test timeout handling\n3. Test error handling for failed subprocess\n4. Test FileNotFoundError handling\n5. Test truncate_content() with content under limit\n6. Test truncate_content() with content over limit\n7. Integration test with real Repomix on small test repo (optional, mark as slow)",
        "priority": "high",
        "dependencies": [
          "1",
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design RepomixResult dataclass and RepomixRunner API",
            "description": "Define the data class for results and the main class structure with init and method signatures.",
            "dependencies": [],
            "details": "Create RepomixResult dataclass with success, content, error fields. Define RepomixRunner __init__ with timeout param and pack() method signature accepting Path returning RepomixResult. Add truncate_content signature.",
            "status": "pending",
            "testStrategy": "Verify dataclass field types and defaults. Test __init__ sets timeout correctly. Test method signatures via inspect.signature.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement pack() method core logic with subprocess and temp file",
            "description": "Implement the main subprocess.run call using npx repomix with temp file output and cwd set to repo_path.",
            "dependencies": [
              1
            ],
            "details": "Use tempfile.NamedTemporaryFile for output. Run subprocess.run(['npx', '-y', 'repomix', '--output', str(output_file)], cwd=str(repo_path), capture_output=True, text=True, timeout=self.timeout). Read content on success. Ensure cleanup in finally block with unlink(missing_ok=True).",
            "status": "pending",
            "testStrategy": "Mock subprocess.run returning success (returncode=0). Verify temp file path passed correctly to --output. Verify cwd set to repo_path. Verify content read from temp file after success.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add comprehensive error handling branches",
            "description": "Handle subprocess failure, timeout, and missing npx/Node.js cases with descriptive error messages in RepomixResult.",
            "dependencies": [
              2
            ],
            "details": "Check result.returncode != 0 and return error with result.stderr. Catch subprocess.TimeoutExpired with timeout message. Catch FileNotFoundError with 'npx/repomix not found' message ensuring Node.js check.",
            "status": "pending",
            "testStrategy": "Mock subprocess.run with returncode=1 and stderr. Mock TimeoutExpired exception. Mock FileNotFoundError. Verify each returns RepomixResult(success=False, error=expected_message).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement truncate_content() with token estimation",
            "description": "Add utility method to truncate content based on max_tokens using 4 chars per token heuristic.",
            "dependencies": [
              1
            ],
            "details": "Calculate max_chars = max_tokens * 4. Return content unchanged if under limit. Otherwise truncate and append '[Content truncated to fit token limit]' suffix. Document the 1 token ≈ 4 chars approximation.",
            "status": "pending",
            "testStrategy": "Test content under limit returns unchanged. Test exact boundary (max_chars). Test over limit truncates correctly with suffix. Test empty string and max_tokens=0 edge cases.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write comprehensive unit tests with mocks for CI stability",
            "description": "Create test_repomix.py with pytest monkeypatch covering all success/failure paths without requiring Node.js.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Use monkeypatch to mock subprocess.run for success/failure/timeout cases. Mock Path.read_text() and tempfile.NamedTemporaryFile. Test truncate_content boundaries. Ensure tests pass in CI without Node/repomix installed by mocking all external calls.",
            "status": "pending",
            "testStrategy": "Coverage: success pack, failed subprocess, timeout, FileNotFoundError, truncate under/over limit. Verify temp file cleanup called. Use pytest tmp_path for filesystem isolation. Mock Path.unlink().",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down Task 4 (Implement Repomix Integration) into 5 subtasks: (1) design RepomixResult dataclass and RepomixRunner API; (2) implement pack() using subprocess.run with cwd, temp file handling, and robust cleanup in finally; (3) implement error handling branches for non-zero exit, TimeoutExpired, and FileNotFoundError with clear messages; (4) implement truncate_content() with a well-documented token-to-char heuristic and tests around boundaries; (5) write unit tests using monkeypatch to mock subprocess.run and filesystem interactions, covering success, failure, timeout, and missing binary cases. Include notes on making this stable in CI where Node/repomix may not be installed.",
        "updatedAt": "2025-12-14T02:33:18.775Z"
      },
      {
        "id": "5",
        "title": "Implement Analysis Engine with Mock Mode",
        "description": "Create the analysis.py module to wrap LLM API calls for analysis. Include a mock mode for testing and a clear extension point for future API integration.",
        "details": "Implement `src/metaagent/analysis.py`:\n\n```python\nimport json\nimport httpx\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Protocol\n\n@dataclass\nclass AnalysisResult:\n    summary: str\n    recommendations: list[str]\n    tasks: list[dict]  # Each task: {title, description, priority, files}\n\nclass AnalysisEngine(Protocol):\n    def analyze(self, prompt: str) -> AnalysisResult:\n        \"\"\"Run analysis with rendered prompt and return structured result.\"\"\"\n        ...\n\nclass MockAnalysisEngine:\n    \"\"\"Mock engine for testing without API calls.\"\"\"\n    \n    def analyze(self, prompt: str) -> AnalysisResult:\n        return AnalysisResult(\n            summary='Mock analysis completed. This is a placeholder result.',\n            recommendations=[\n                'Implement core functionality first',\n                'Add comprehensive error handling',\n                'Write unit tests for critical paths'\n            ],\n            tasks=[\n                {'title': 'Sample Task 1', 'description': 'Placeholder task', 'priority': 'high', 'files': []},\n                {'title': 'Sample Task 2', 'description': 'Another placeholder', 'priority': 'medium', 'files': []}\n            ]\n        )\n\nclass PerplexityAnalysisEngine:\n    \"\"\"Perplexity API integration for analysis.\"\"\"\n    \n    def __init__(self, api_key: str, timeout: int = 120):\n        self.api_key = api_key\n        self.timeout = timeout\n        self.base_url = 'https://api.perplexity.ai'\n    \n    def analyze(self, prompt: str) -> AnalysisResult:\n        \"\"\"Extension point for LLM analysis calls.\"\"\"\n        headers = {\n            'Authorization': f'Bearer {self.api_key}',\n            'Content-Type': 'application/json'\n        }\n        \n        payload = {\n            'model': 'llama-3.1-sonar-large-128k-online',\n            'messages': [\n                {'role': 'system', 'content': 'You are a code analysis expert. Always respond with valid JSON containing keys: summary, recommendations, tasks.'},\n                {'role': 'user', 'content': prompt}\n            ]\n        }\n        \n        with httpx.Client(timeout=self.timeout) as client:\n            response = client.post(\n                f'{self.base_url}/chat/completions',\n                headers=headers,\n                json=payload\n            )\n            response.raise_for_status()\n            data = response.json()\n        \n        content = data['choices'][0]['message']['content']\n        return self._parse_response(content)\n    \n    def _parse_response(self, content: str) -> AnalysisResult:\n        \"\"\"Parse JSON response from LLM.\"\"\"\n        try:\n            # Try to extract JSON from response\n            parsed = json.loads(content)\n        except json.JSONDecodeError:\n            # Fallback: try to find JSON block\n            import re\n            match = re.search(r'```json\\s*(.+?)\\s*```', content, re.DOTALL)\n            if match:\n                parsed = json.loads(match.group(1))\n            else:\n                return AnalysisResult(\n                    summary=content[:500],\n                    recommendations=[],\n                    tasks=[]\n                )\n        \n        return AnalysisResult(\n            summary=parsed.get('summary', ''),\n            recommendations=parsed.get('recommendations', []),\n            tasks=parsed.get('tasks', [])\n        )\n\ndef create_analysis_engine(api_key: str | None, use_mock: bool = False) -> AnalysisEngine:\n    \"\"\"Factory function to create appropriate analysis engine.\"\"\"\n    if use_mock or not api_key:\n        return MockAnalysisEngine()\n    return PerplexityAnalysisEngine(api_key)\n```",
        "testStrategy": "Unit tests in `tests/test_analysis.py`:\n1. Test MockAnalysisEngine.analyze() returns valid AnalysisResult\n2. Test create_analysis_engine() returns mock when use_mock=True\n3. Test create_analysis_engine() returns mock when api_key is None\n4. Test _parse_response() with valid JSON\n5. Test _parse_response() with JSON in code block\n6. Test _parse_response() with invalid JSON (fallback)\n7. Integration test with real Perplexity API (optional, mark as integration, skip in CI)",
        "priority": "high",
        "dependencies": [
          "1",
          "2",
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define AnalysisResult dataclass and AnalysisEngine protocol in analysis.py",
            "description": "Create the AnalysisResult dataclass and AnalysisEngine protocol to formalize structured analysis outputs and the engine interface.",
            "dependencies": [],
            "details": "Implement AnalysisResult with fields: summary: str, recommendations: list[str], tasks: list[dict] where each task dict includes at least title, description, priority, files. Define an AnalysisEngine Protocol with a single method analyze(self, prompt: str) -> AnalysisResult and a clear docstring describing its contract as the extension point for different LLM providers.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement MockAnalysisEngine with deterministic analyze() output",
            "description": "Create MockAnalysisEngine that implements AnalysisEngine and returns fixed, deterministic results for tests.",
            "dependencies": [
              1
            ],
            "details": "Add MockAnalysisEngine class with analyze(self, prompt: str) -> AnalysisResult returning a constant AnalysisResult instance matching the provided example values so tests can rely on stable outputs. Ensure it does not perform any network calls or depend on external state. Keep implementation simple and side-effect free so unit tests can assert exact summaries, recommendations, and tasks.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement PerplexityAnalysisEngine HTTP client with secure configuration",
            "description": "Create PerplexityAnalysisEngine that calls the Perplexity API using httpx with proper headers, payload, timeout, and error handling while avoiding API key leaks.",
            "dependencies": [
              1
            ],
            "details": "Implement __init__(self, api_key: str, timeout: int = 120) storing api_key, timeout, and base_url. Implement analyze(self, prompt: str) -> AnalysisResult using httpx.Client with configured timeout to POST to /chat/completions. Build headers including Authorization: Bearer <api_key> and Content-Type: application/json, and construct the payload with the specified model and system/user messages. Call response.raise_for_status() to surface HTTP errors, then parse JSON and extract the LLM message content. Never log or print the raw api_key, and avoid logging full request/response bodies that may contain sensitive data; if logging is needed, redact keys and truncate content.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement robust _parse_response() JSON extraction and graceful fallback",
            "description": "Add private _parse_response() helper on PerplexityAnalysisEngine to convert raw LLM content into AnalysisResult, handling invalid or wrapped JSON safely.",
            "dependencies": [
              3
            ],
            "details": "Implement _parse_response(self, content: str) -> AnalysisResult that first attempts json.loads(content). On json.JSONDecodeError, search for a ```json ... ``` fenced block via regex, attempt to json.loads on the captured block, and if that also fails, return an AnalysisResult with summary set to a truncated slice of the raw content (e.g., first 500 chars) and empty recommendations and tasks. Ensure missing keys in parsed JSON are handled via .get with sensible defaults. Do not execute or eval any content, and avoid logging full untrusted content; if logging parse failures, log only short snippets or generic error messages to reduce risk of leaking sensitive data.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement create_analysis_engine() factory for mock vs real engines",
            "description": "Create the create_analysis_engine() factory function to choose between MockAnalysisEngine and PerplexityAnalysisEngine based on api_key and use_mock flag.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement create_analysis_engine(api_key: str | None, use_mock: bool = False) -> AnalysisEngine such that it returns MockAnalysisEngine when use_mock is True or when api_key is falsy/None, and returns PerplexityAnalysisEngine(api_key) otherwise. Document this behavior clearly so callers (e.g., configuration/orchestrator) understand how to enable mock mode. Ensure the function does not log API keys and only logs high-level selection decisions if logging is added.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Write unit tests for analysis engines and JSON parsing with mocked httpx",
            "description": "Add comprehensive unit tests in tests/test_analysis.py covering mock engine, factory behavior, PerplexityAnalysisEngine HTTP interactions, and JSON parsing fallbacks without real network calls.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "Create tests verifying: (1) MockAnalysisEngine.analyze() returns an AnalysisResult with expected deterministic fields; (2) create_analysis_engine() returns MockAnalysisEngine when use_mock=True; (3) create_analysis_engine() returns MockAnalysisEngine when api_key is None or empty; (4) _parse_response() correctly parses valid JSON content; (5) _parse_response() extracts JSON from ```json fenced blocks; (6) _parse_response() falls back to truncated summary with empty lists on invalid JSON. Fully mock httpx.Client using monkeypatch or unittest.mock to simulate successful responses, HTTP error responses (ensuring raise_for_status propagates), and malformed model responses where choices/message/content is missing or non-JSON. Assert that no real HTTP requests are made and that error paths do not expose API keys or full sensitive payloads in exception messages or logs if any logging is present.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down Task 5 (Implement Analysis Engine with Mock Mode) into 6 subtasks: (1) define AnalysisResult dataclass and AnalysisEngine protocol; (2) implement MockAnalysisEngine with deterministic outputs for tests; (3) implement PerplexityAnalysisEngine HTTP client using httpx, including headers, payload shape, timeout, and error handling; (4) implement _parse_response() to robustly extract JSON from raw content or ```json``` blocks and degrade gracefully on invalid JSON; (5) implement create_analysis_engine() factory with clear rules for mock vs real engine; (6) write unit tests that fully mock httpx.Client to cover success, HTTP errors, malformed model responses, and JSON parse fallbacks, ensuring no real network access. Highlight security considerations around API keys and logging.",
        "updatedAt": "2025-12-14T02:00:08.782Z"
      },
      {
        "id": "6",
        "title": "Implement Plan Writer",
        "description": "Create the plan_writer.py module to generate the mvp_improvement_plan.md file from aggregated analysis results.",
        "details": "Implement `src/metaagent/plan_writer.py`:\n\n```python\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom .analysis import AnalysisResult\n\n@dataclass\nclass StageResult:\n    stage_name: str\n    prompt_id: str\n    result: AnalysisResult\n\nclass PlanWriter:\n    def __init__(self, output_dir: Path):\n        self.output_dir = output_dir\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n    \n    def write_plan(self, prd_content: str, stage_results: list[StageResult], profile_name: str) -> Path:\n        \"\"\"\n        Generate mvp_improvement_plan.md from analysis results.\n        \n        Args:\n            prd_content: Original PRD content\n            stage_results: Results from each analysis stage\n            profile_name: Name of the profile used\n            \n        Returns:\n            Path to the generated plan file\n        \"\"\"\n        output_path = self.output_dir / 'mvp_improvement_plan.md'\n        \n        lines = [\n            '# MVP Improvement Plan',\n            '',\n            f'**Generated:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}',\n            f'**Profile:** {profile_name}',\n            '',\n            '---',\n            '',\n            '## PRD Summary',\n            '',\n            self._extract_prd_summary(prd_content),\n            '',\n            '---',\n            ''\n        ]\n        \n        # Add stage summaries\n        lines.append('## Analysis Summaries')\n        lines.append('')\n        for sr in stage_results:\n            lines.append(f'### {sr.stage_name}')\n            lines.append('')\n            lines.append(sr.result.summary)\n            lines.append('')\n            if sr.result.recommendations:\n                lines.append('**Recommendations:**')\n                for rec in sr.result.recommendations:\n                    lines.append(f'- {rec}')\n                lines.append('')\n        \n        # Aggregate and prioritize tasks\n        lines.append('---')\n        lines.append('')\n        lines.append('## Prioritized Task List')\n        lines.append('')\n        lines.append('Complete tasks in order. Check off as completed.')\n        lines.append('')\n        \n        all_tasks = self._aggregate_tasks(stage_results)\n        for i, task in enumerate(all_tasks, 1):\n            priority_badge = self._priority_badge(task.get('priority', 'medium'))\n            lines.append(f'- [ ] **{i}. {task[\"title\"]}** {priority_badge}')\n            lines.append(f'  - {task[\"description\"]}')\n            if task.get('files'):\n                lines.append(f'  - Files: {\", \".join(task[\"files\"])}')\n            lines.append('')\n        \n        # Claude Code instruction block\n        lines.extend(self._claude_code_instructions())\n        \n        output_path.write_text('\\n'.join(lines), encoding='utf-8')\n        return output_path\n    \n    def _extract_prd_summary(self, prd_content: str) -> str:\n        \"\"\"Extract first 500 chars as summary.\"\"\"\n        lines = prd_content.strip().split('\\n')\n        summary_lines = []\n        char_count = 0\n        for line in lines:\n            if char_count + len(line) > 500:\n                break\n            summary_lines.append(line)\n            char_count += len(line)\n        return '\\n'.join(summary_lines) + '...'\n    \n    def _aggregate_tasks(self, stage_results: list[StageResult]) -> list[dict]:\n        \"\"\"Aggregate tasks from all stages, sorted by priority.\"\"\"\n        all_tasks = []\n        for sr in stage_results:\n            all_tasks.extend(sr.result.tasks)\n        \n        priority_order = {'high': 0, 'medium': 1, 'low': 2}\n        return sorted(all_tasks, key=lambda t: priority_order.get(t.get('priority', 'medium'), 1))\n    \n    def _priority_badge(self, priority: str) -> str:\n        badges = {'high': '🔴', 'medium': '🟡', 'low': '🟢'}\n        return badges.get(priority, '🟡')\n    \n    def _claude_code_instructions(self) -> list[str]:\n        return [\n            '---',\n            '',\n            '## Instructions for Claude Code',\n            '',\n            '1. Read this entire document before starting',\n            '2. Work through tasks in order, checking off each as completed',\n            '3. Run tests after each significant change',\n            '4. Commit changes incrementally with descriptive messages',\n            '5. If blocked, document the blocker and move to the next task',\n            ''\n        ]\n```",
        "testStrategy": "Unit tests in `tests/test_plan_writer.py`:\n1. Test write_plan() creates file at expected path\n2. Test output contains PRD summary section\n3. Test output contains stage summaries for all stages\n4. Test tasks are aggregated and sorted by priority\n5. Test Claude Code instruction block is included\n6. Test _priority_badge() returns correct emojis\n7. Test with empty stage_results list",
        "priority": "medium",
        "dependencies": [
          "1",
          "2",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define StageResult dataclass and PlanWriter constructor",
            "description": "Implement the basic class structure with StageResult dataclass and PlanWriter __init__ that creates output directory.",
            "dependencies": [],
            "details": "Create src/metaagent/plan_writer.py with imports (Path, dataclass, datetime, AnalysisResult), define StageResult with stage_name, prompt_id, result fields, and PlanWriter __init__ that sets self.output_dir and calls mkdir(parents=True, exist_ok=True).",
            "status": "pending",
            "testStrategy": "Test StageResult instantiation with sample data and PlanWriter __init__ creates directory using tmp_path fixture.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement write_plan() method with Markdown sections",
            "description": "Create the main write_plan method that assembles metadata, PRD summary, stage summaries, prioritized tasks, and Claude instructions into mvp_improvement_plan.md.",
            "dependencies": [
              1
            ],
            "details": "Implement write_plan(prd_content, stage_results, profile_name) that builds lines list with header, timestamp/profile metadata, PRD summary call, stage summaries loop, prioritized tasks section with _aggregate_tasks call, and _claude_code_instructions, then writes UTF-8 file.",
            "status": "pending",
            "testStrategy": "Test file creation at expected path, verify all sections present in output (header, metadata, PRD summary, stage summaries, tasks, instructions) using file content assertions.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement _extract_prd_summary with line-aware truncation",
            "description": "Create helper method to extract first ~500 characters of PRD content while preserving complete lines and adding ellipsis.",
            "dependencies": [
              1
            ],
            "details": "Implement _extract_prd_summary(prd_content) that splits by lines, accumulates until 500 char limit, joins lines, appends '...' for truncation. Handle empty/whitespace PRD gracefully.",
            "status": "pending",
            "testStrategy": "Test truncation at ~500 chars preserves lines, test short content returns full, test empty PRD returns empty string or minimal output.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement task aggregation and priority utilities",
            "description": "Add _aggregate_tasks to collect/sort tasks by priority and _priority_badge for emoji mapping.",
            "dependencies": [
              1
            ],
            "details": "Implement _aggregate_tasks(stage_results) that flattens all tasks and sorts by priority_order={'high':0,'medium':1,'low':2}, _priority_badge(priority) mapping to emojis (🔴🟡🟢). Ensure UTF-8 compatibility for emojis.",
            "status": "pending",
            "testStrategy": "Test task aggregation collects from multiple stages, verify priority sorting (high>medium>low), test badge emojis render correctly in UTF-8 output.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write comprehensive unit tests for PlanWriter",
            "description": "Create tests/test_plan_writer.py with pytest tests covering file creation, content sections, priority sorting, edge cases.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Use tmp_path fixture to test: file creation/path, all Markdown sections present, stage summaries rendered, tasks sorted by priority, empty tasks/stages handled, UTF-8 emoji encoding, PRD truncation. Mock AnalysisResult and StageResult.",
            "status": "pending",
            "testStrategy": "Run pytest with coverage: verify 100% pass rate, test empty inputs (no stages/no tasks), mixed priorities, long PRD truncation, Unicode emoji preservation in output file.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down Task 6 (Implement Plan Writer) into 5 subtasks: (1) define StageResult dataclass and PlanWriter constructor creating output_dir; (2) implement write_plan() to assemble sections (metadata, PRD summary, stage summaries, prioritized tasks, instructions) into Markdown; (3) implement _extract_prd_summary() with length limits and line-aware truncation; (4) implement _aggregate_tasks() and _priority_badge() with clear priority ordering and emoji mapping; (5) write unit tests using tmp_path to verify file creation, content sections, priority sorting, and behavior with empty tasks/stages. Note any i18n/encoding considerations for emojis and UTF-8 writes.",
        "updatedAt": "2025-12-14T02:00:13.300Z"
      },
      {
        "id": "7",
        "title": "Implement Orchestrator",
        "description": "Create the orchestrator.py module that coordinates the entire refinement workflow: loading config, running stages, calling analysis engine, and generating the plan.",
        "details": "Implement `src/metaagent/orchestrator.py`:\n\n```python\nimport logging\nfrom pathlib import Path\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\n\nfrom .config import Config\nfrom .prompts import PromptLibrary, Prompt\nfrom .repomix import RepomixRunner, RepomixResult\nfrom .analysis import create_analysis_engine, AnalysisResult\nfrom .plan_writer import PlanWriter, StageResult\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RunHistory:\n    \"\"\"Tracks analysis history for context in subsequent stages.\"\"\"\n    entries: list[dict] = field(default_factory=list)\n    \n    def add(self, stage: str, summary: str):\n        self.entries.append({\n            'stage': stage,\n            'timestamp': datetime.now().isoformat(),\n            'summary': summary\n        })\n    \n    def to_string(self) -> str:\n        if not self.entries:\n            return 'No previous analysis.'\n        lines = ['Previous analysis results:']\n        for entry in self.entries:\n            lines.append(f\"\\n## {entry['stage']} ({entry['timestamp']})\")\n            lines.append(entry['summary'])\n        return '\\n'.join(lines)\n\n@dataclass  \nclass RefinementResult:\n    success: bool\n    plan_path: Path | None\n    stage_results: list[StageResult]\n    errors: list[str]\n\nclass Orchestrator:\n    def __init__(self, config: Config, use_mock: bool = False):\n        self.config = config\n        self.use_mock = use_mock\n        self.prompt_library = PromptLibrary(config.config_dir)\n        self.repomix = RepomixRunner(timeout=config.timeout)\n        self.analysis_engine = create_analysis_engine(\n            config.perplexity_api_key,\n            use_mock=use_mock\n        )\n        self.plan_writer = PlanWriter(config.output_dir)\n        self.history = RunHistory()\n    \n    def refine(self, profile_name: str) -> RefinementResult:\n        \"\"\"\n        Run the full refinement workflow for the given profile.\n        \n        Args:\n            profile_name: Name of the profile to use\n            \n        Returns:\n            RefinementResult with success status and plan path\n        \"\"\"\n        errors = []\n        stage_results = []\n        \n        # Validate profile exists\n        profile = self.prompt_library.get_profile(profile_name)\n        if not profile:\n            available = self.prompt_library.list_profiles()\n            return RefinementResult(\n                success=False,\n                plan_path=None,\n                stage_results=[],\n                errors=[f'Profile \"{profile_name}\" not found. Available: {available}']\n            )\n        \n        logger.info(f'Starting refinement with profile: {profile_name}')\n        \n        # Load PRD\n        prd_content = self._load_prd()\n        if not prd_content:\n            return RefinementResult(\n                success=False,\n                plan_path=None,\n                stage_results=[],\n                errors=[f'PRD not found at {self.config.prd_path}']\n            )\n        \n        # Run Repomix\n        logger.info('Packing codebase with Repomix...')\n        repomix_result = self.repomix.pack(self.config.repo_path)\n        if not repomix_result.success:\n            errors.append(f'Repomix warning: {repomix_result.error}')\n            code_context = '[Repomix failed - limited code context available]'\n        else:\n            code_context = self.repomix.truncate_content(\n                repomix_result.content,\n                self.config.max_tokens\n            )\n        \n        # Run each stage\n        prompts = self.prompt_library.get_prompts_for_profile(profile_name)\n        for prompt in prompts:\n            logger.info(f'Running stage: {prompt.id}')\n            try:\n                result = self._run_stage(prompt, prd_content, code_context)\n                stage_results.append(StageResult(\n                    stage_name=prompt.id,\n                    prompt_id=prompt.id,\n                    result=result\n                ))\n                self.history.add(prompt.id, result.summary)\n            except Exception as e:\n                logger.error(f'Stage {prompt.id} failed: {e}')\n                errors.append(f'Stage {prompt.id} failed: {str(e)}')\n        \n        # Generate plan\n        if stage_results:\n            logger.info('Generating improvement plan...')\n            plan_path = self.plan_writer.write_plan(\n                prd_content,\n                stage_results,\n                profile_name\n            )\n            logger.info(f'Plan written to {plan_path}')\n        else:\n            plan_path = None\n            errors.append('No stages completed successfully')\n        \n        return RefinementResult(\n            success=len(errors) == 0,\n            plan_path=plan_path,\n            stage_results=stage_results,\n            errors=errors\n        )\n    \n    def _load_prd(self) -> str | None:\n        if self.config.prd_path.exists():\n            return self.config.prd_path.read_text(encoding='utf-8')\n        return None\n    \n    def _run_stage(self, prompt: Prompt, prd: str, code_context: str) -> AnalysisResult:\n        rendered = prompt.render(\n            prd=prd,\n            code_context=code_context,\n            history=self.history.to_string(),\n            current_stage=prompt.id\n        )\n        return self.analysis_engine.analyze(rendered)\n```",
        "testStrategy": "Unit tests in `tests/test_orchestrator.py`:\n1. Test refine() with valid profile returns success\n2. Test refine() with invalid profile returns error\n3. Test refine() with missing PRD returns error\n4. Test refine() continues after Repomix failure with warning\n5. Test stages are run in profile order\n6. Test history is accumulated across stages\n7. Test plan is generated after successful stages\n8. Use MockAnalysisEngine and mock Repomix for isolation",
        "priority": "high",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define RunHistory and RefinementResult dataclasses",
            "description": "Implement RunHistory class with add() and to_string() methods, and RefinementResult dataclass for workflow results.",
            "dependencies": [],
            "details": "Use @dataclass with field(default_factory=list) for RunHistory.entries. Implement timestamped history tracking and string formatting for prompt context. RefinementResult holds success flag, plan_path, stage_results list, and errors list.",
            "status": "pending",
            "testStrategy": "Test RunHistory.add() appends correctly, to_string() formats multi-entry history, empty history returns 'No previous analysis.' Test RefinementResult instantiation and field access.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Orchestrator.__init__ dependency wiring",
            "description": "Wire Config, PromptLibrary, RepomixRunner, analysis_engine factory, PlanWriter, and RunHistory in constructor.",
            "dependencies": [
              1
            ],
            "details": "Initialize self.config, self.prompt_library(config.config_dir), self.repomix(config.timeout), self.analysis_engine=create_analysis_engine(config.perplexity_api_key, use_mock), self.plan_writer(config.output_dir), self.history=RunHistory(). Support use_mock flag.",
            "status": "pending",
            "testStrategy": "Mock all dependencies, verify __init__ passes correct params to each component, test use_mock=True creates mock analysis engine.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement _load_prd() with file existence check",
            "description": "Load PRD content from config.prd_path with clear None return on missing file.",
            "dependencies": [
              2
            ],
            "details": "Use self.config.prd_path.exists() check, read_text(encoding='utf-8') on success, return None on failure. No exceptions, clean failure path for orchestrator.",
            "status": "pending",
            "testStrategy": "Mock Path.exists()=False returns None, mock Path.read_text() returns content, verify encoding handling.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Repomix integration in refine()",
            "description": "Run repomix.pack(), handle failure with warning, truncate content if successful.",
            "dependencies": [
              2,
              3
            ],
            "details": "Call self.repomix.pack(self.config.repo_path), on failure set code_context='[Repomix failed...]', on success truncate with self.repomix.truncate_content(result.content, self.config.max_tokens). Log progress.",
            "status": "pending",
            "testStrategy": "Mock repomix.pack() success with long content (verify truncation), mock failure (verify warning context), verify logging calls.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement stage execution loop with history",
            "description": "Fetch prompts for profile, render each with history/PRD/code_context, run analysis, accumulate StageResult, update history.",
            "dependencies": [
              1,
              2,
              4
            ],
            "details": "Get prompts = self.prompt_library.get_prompts_for_profile(profile_name), loop: render=prompt.render(prd, code_context, history=self.history.to_string(), current_stage=prompt.id), result=self.analysis_engine.analyze(rendered), create StageResult, self.history.add(). Catch exceptions per stage.",
            "status": "pending",
            "testStrategy": "Mock prompt_library.get_prompts_for_profile() returns 2 prompts, verify sequential execution, history updates after each stage, StageResult list accumulates correctly, partial failure continues.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement plan generation and RefinementResult",
            "description": "Generate plan if stages succeeded, assemble final RefinementResult with success/errors semantics.",
            "dependencies": [
              1,
              2,
              5
            ],
            "details": "After stage loop, if stage_results: plan_path=self.plan_writer.write_plan(prd_content, stage_results, profile_name), else plan_path=None. Return RefinementResult(success=len(errors)==0, plan_path, stage_results, errors). Early returns for profile/PRD validation.",
            "status": "pending",
            "testStrategy": "Mock plan_writer.write_plan() returns Path, verify success=True when errors=[], success=False with errors, test no-stages-empty-results case.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Write comprehensive unit tests for Orchestrator",
            "description": "Create tests/test_orchestrator.py with heavy mocking covering all paths: success, missing profile/PRD, Repomix/stage failures.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Use pytest, unittest.mock: Mock PromptLibrary.get_profile()/get_prompts_for_profile(), RepomixRunner.pack(), create_analysis_engine(), PlanWriter.write_plan(). Test 10+ scenarios: valid profile success path, invalid profile early return, missing PRD, Repomix fail continues, stage exceptions accumulate, history chaining, empty profile prompts.",
            "status": "pending",
            "testStrategy": "Aim for 90%+ coverage. Verify orchestration logic isolation from I/O. Test history.to_string() called sequentially, StageResult.prompt_id==stage_name consistency, error accumulation without crashing.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Break down Task 7 (Implement Orchestrator) into 7 subtasks: (1) define RunHistory and RefinementResult data structures; (2) implement Orchestrator.__init__ wiring Config, PromptLibrary, RepomixRunner, AnalysisEngine factory, and PlanWriter; (3) implement _load_prd() with clear failure behavior; (4) implement Repomix integration inside refine(), including warning handling and context truncation; (5) implement stage loop: fetching prompts for a profile, rendering with history, invoking analysis, accumulating StageResult, and updating RunHistory; (6) implement plan generation and final RefinementResult assembly with success/error semantics; (7) write unit tests that heavily mock PromptLibrary, RepomixRunner, and AnalysisEngine to cover success path, missing profile, missing PRD, Repomix failures, partial stage failures, history accumulation, and no-stages cases. Emphasize separation of orchestration logic from I/O for testability.",
        "updatedAt": "2025-12-14T02:00:17.617Z"
      },
      {
        "id": "8",
        "title": "Implement CLI Entrypoint",
        "description": "Create the CLI using Typer with the `metaagent refine` command that validates arguments and invokes the orchestrator.",
        "details": "Implement `src/metaagent/cli.py`:\n\n```python\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Optional\n\nimport typer\nfrom rich.console import Console\nfrom rich.logging import RichHandler\n\nfrom .config import Config\nfrom .orchestrator import Orchestrator\nfrom .prompts import PromptLibrary\n\napp = typer.Typer(\n    name='metaagent',\n    help='Meta-agent for automated codebase refinement from v0 to MVP'\n)\nconsole = Console()\n\ndef setup_logging(level: str):\n    logging.basicConfig(\n        level=level,\n        format='%(message)s',\n        handlers=[RichHandler(rich_tracebacks=True)]\n    )\n\n@app.command()\ndef refine(\n    profile: str = typer.Option(\n        ...,\n        '--profile', '-p',\n        help='Profile to use for refinement (e.g., automation_agent, backend_service)'\n    ),\n    repo: Path = typer.Option(\n        Path('.'),\n        '--repo', '-r',\n        help='Path to the repository to refine'\n    ),\n    prd: Optional[Path] = typer.Option(\n        None,\n        '--prd',\n        help='Path to PRD file (default: docs/prd.md in repo)'\n    ),\n    mock: bool = typer.Option(\n        False,\n        '--mock',\n        help='Use mock analysis engine (no API calls)'\n    ),\n    verbose: bool = typer.Option(\n        False,\n        '--verbose', '-v',\n        help='Enable verbose output'\n    )\n):\n    \"\"\"\n    Refine a codebase from v0 to MVP using automated analysis and planning.\n    \n    Example:\n        metaagent refine --profile automation_agent --repo /path/to/repo\n    \"\"\"\n    # Setup logging\n    log_level = 'DEBUG' if verbose else 'INFO'\n    setup_logging(log_level)\n    \n    # Resolve paths\n    repo_path = repo.resolve()\n    if not repo_path.exists():\n        console.print(f'[red]Error: Repository path does not exist: {repo_path}[/red]')\n        raise typer.Exit(1)\n    \n    # Load config\n    config = Config.from_env(repo_path)\n    if prd:\n        config.prd_path = prd.resolve()\n    \n    # Validate config\n    errors = config.validate()\n    if errors:\n        for err in errors:\n            console.print(f'[red]Configuration error: {err}[/red]')\n        raise typer.Exit(1)\n    \n    # Show available profiles if requested profile doesn't exist\n    prompt_library = PromptLibrary(config.config_dir)\n    if not prompt_library.get_profile(profile):\n        available = prompt_library.list_profiles()\n        console.print(f'[red]Error: Profile \"{profile}\" not found.[/red]')\n        console.print(f'Available profiles: {available}')\n        raise typer.Exit(1)\n    \n    # Run refinement\n    console.print(f'[bold blue]Starting refinement...[/bold blue]')\n    console.print(f'  Profile: {profile}')\n    console.print(f'  Repository: {repo_path}')\n    console.print(f'  Mock mode: {mock}')\n    console.print()\n    \n    orchestrator = Orchestrator(config, use_mock=mock)\n    result = orchestrator.refine(profile)\n    \n    # Report results\n    if result.success:\n        console.print('[bold green]Refinement completed successfully![/bold green]')\n        console.print(f'Plan written to: {result.plan_path}')\n    else:\n        console.print('[bold yellow]Refinement completed with warnings:[/bold yellow]')\n        for err in result.errors:\n            console.print(f'  - {err}')\n        if result.plan_path:\n            console.print(f'Plan written to: {result.plan_path}')\n    \n    console.print(f'\\nStages completed: {len(result.stage_results)}')\n\n@app.command()\ndef list_profiles(\n    config_dir: Optional[Path] = typer.Option(\n        None,\n        '--config-dir', '-c',\n        help='Path to config directory'\n    )\n):\n    \"\"\"List available refinement profiles.\"\"\"\n    config = Config.from_env()\n    if config_dir:\n        config.config_dir = config_dir.resolve()\n    \n    prompt_library = PromptLibrary(config.config_dir)\n    profiles = prompt_library.list_profiles()\n    \n    if not profiles:\n        console.print('[yellow]No profiles found.[/yellow]')\n        return\n    \n    console.print('[bold]Available Profiles:[/bold]')\n    for name in profiles:\n        profile = prompt_library.get_profile(name)\n        console.print(f'  {name}: {profile.description}')\n        console.print(f'    Stages: {profile.stages}')\n\ndef main():\n    app()\n\nif __name__ == '__main__':\n    main()\n```\n\nAdd to `src/metaagent/__init__.py`:\n```python\n__version__ = '0.1.0'\n```",
        "testStrategy": "Unit tests in `tests/test_cli.py`:\n1. Test `refine` command with valid args using CliRunner\n2. Test `refine` command fails gracefully with invalid profile\n3. Test `refine` command fails gracefully with non-existent repo\n4. Test `list-profiles` command outputs available profiles\n5. Test `--mock` flag passes through to orchestrator\n6. Test `--verbose` flag sets logging level correctly\n7. Test help text is displayed with `--help`",
        "priority": "high",
        "dependencies": [
          "2",
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Typer app, main() entrypoint, and package __version__",
            "description": "Create the Typer application object, wire up the main() entrypoint compatible with the pyproject console_script, and expose the package version in __init__.py.",
            "dependencies": [],
            "details": "Implement src/metaagent/cli.py with a module-level typer.Typer instance (name='metaagent') and a main() function that calls app(), suitable for use as the console_script entrypoint. Ensure __init__.py defines __version__ = '0.1.0' and that the CLI module can be imported without side effects beyond defining commands. Confirm the structure aligns with expected project layout so that `metaagent` runs the Typer app when installed.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement logging setup using RichHandler with configurable levels",
            "description": "Add a reusable logging setup function that configures logging with RichHandler and supports INFO/DEBUG levels via a parameter.",
            "dependencies": [
              1
            ],
            "details": "In cli.py, implement setup_logging(level: str) that calls logging.basicConfig with RichHandler(rich_tracebacks=True) and a simple message format. Use the provided code skeleton as reference. Ensure that calling setup_logging with 'DEBUG' or 'INFO' correctly changes the global log level and that it does not re-add duplicate handlers on multiple invocations in typical CLI use. Prepare for verbose flag integration in the refine command.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement `refine` command with argument parsing and orchestrator integration",
            "description": "Create the refine Typer command that parses CLI options, resolves paths, loads and validates config, checks profile existence, calls the orchestrator, and reports results.",
            "dependencies": [
              1,
              2
            ],
            "details": "Define @app.command() refine(...) with options: --profile/-p (required str), --repo/-r (Path, default '.'), --prd (Optional[Path]), --mock (bool), and --verbose/-v (bool). Inside, set log_level to 'DEBUG' when verbose is True, else 'INFO', and call setup_logging. Resolve repo path; if it does not exist, print a red Rich error message and exit with typer.Exit(1). Call Config.from_env(repo_path) to load configuration, apply prd override if provided, and run config.validate(); on validation errors, print each as a Rich red configuration error and exit 1. Instantiate PromptLibrary with config.config_dir, verify the requested profile exists via get_profile(), and if missing, print a red error plus a list of available profiles before exiting 1. On success, log a blue starting message with profile, repo, and mock mode. Create Orchestrator(config, use_mock=mock), call orchestrator.refine(profile), and then print green success output with plan path on success, or yellow warnings listing each error and plan path when present. Always print the count of completed stages at the end.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement `list_profiles` command with optional config_dir override",
            "description": "Add the list_profiles Typer command that loads configuration, applies an optional config directory override, and prints available profiles using PromptLibrary.",
            "dependencies": [
              1,
              3
            ],
            "details": "Define @app.command() list_profiles(config_dir: Optional[Path] = Option(None, '--config-dir', '-c', ...)). Inside, call Config.from_env() with default parameters, and if config_dir is provided, resolve it and assign to config.config_dir. Instantiate PromptLibrary(config.config_dir), call list_profiles(), and if the list is empty print a yellow notice and return without error. Otherwise, print a bold header and for each profile name, fetch the profile object with get_profile(name) and print its description and stages in a readable, indented format using Rich markup. Ensure this command exits with code 0 on normal completion.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Ensure consistent CLI error handling and exit codes with Rich messages",
            "description": "Review and refine CLI error handling so that all failure scenarios return appropriate exit codes and user-friendly Rich-formatted messages.",
            "dependencies": [
              3,
              4
            ],
            "details": "Audit refine and list_profiles commands to make sure all early-return error states use typer.Exit with non-zero codes (e.g., 1) for invalid repo paths, configuration validation failures, missing profiles, and other user errors, while successful paths exit with 0. Standardize Rich output styles for errors (red), warnings (yellow), and success (green/blue). Confirm no unhandled exceptions leak to the user in common failure scenarios, and that messages remain concise and informative across platforms. Avoid using sys.exit directly; rely on typer.Exit for consistency.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Write CLI tests with typer.testing.CliRunner covering success and failure paths",
            "description": "Create comprehensive tests for the CLI using CliRunner to exercise refine and list_profiles commands across success, failure, mock, verbose, and help scenarios without hitting real external APIs or subprocesses.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "In tests/test_cli.py, instantiate CliRunner and write tests that invoke the Typer app (e.g., runner.invoke(cli.app, [...])). Cover: (a) successful refine with valid args, mocking Config.from_env, PromptLibrary, and Orchestrator to avoid real file system, APIs, or subprocesses; (b) refine with invalid profile, ensuring proper error message and non-zero exit code; (c) refine with non-existent repo path, checking path handling is cross-platform safe (e.g., using tmp_path / 'missing'); (d) refine with --mock and --verbose ensuring they alter behavior/log level and output; (e) list_profiles showing available profiles, using mocked PromptLibrary; and (f) help text for top-level and refine/list_profiles commands. Use monkeypatch or similar to inject test doubles, and assert output strings and exit codes without relying on actual environment variables or network calls.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down Task 8 (Implement CLI Entrypoint) into 6 subtasks: (1) set up Typer app structure and main() entrypoint compatible with pyproject console_script; (2) implement logging setup with RichHandler and configurable levels; (3) implement refine command: argument parsing, repo path resolution, Config.from_env usage, optional PRD override, config validation, profile existence check, orchestrator invocation, and result reporting; (4) implement list_profiles command: config_dir override, PromptLibrary usage, and formatted output; (5) ensure CLI error handling exits with proper codes and user-friendly Rich messages; (6) write tests using typer.testing.CliRunner that exercise success and failure paths, mock mode, verbose flag, invalid repo/profile, and help text. Note cross-platform path considerations and how to avoid hitting real APIs/subprocesses in tests.",
        "updatedAt": "2025-12-14T02:00:22.408Z"
      },
      {
        "id": "9",
        "title": "Create Complete Configuration Files",
        "description": "Populate config/prompts.yaml and config/profiles.yaml with all prompt templates and profile definitions from the PRD.",
        "details": "Create `config/prompts.yaml` with exact content from PRD Section 7.1:\n\n```yaml\nprompts:\n  alignment_with_prd:\n    id: alignment_with_prd\n    goal: \"Identify gaps between current implementation and PRD requirements\"\n    stage: alignment\n    template: |\n      You are analyzing a codebase against its PRD.\n\n      ## PRD:\n      {{prd}}\n\n      ## Current Codebase:\n      {{code_context}}\n\n      ## Previous Analysis (if any):\n      {{history}}\n\n      Current Stage: {{current_stage}}\n\n      Please analyze and provide:\n      1. Summary of alignment gaps\n      2. Missing features or incomplete implementations\n      3. Prioritized task list to close gaps\n\n      Format your response as JSON with keys: summary, recommendations, tasks\n\n  architecture_sanity:\n    id: architecture_sanity\n    goal: \"Review architecture for best practices and maintainability\"\n    stage: architecture\n    template: |\n      Review this codebase for architectural quality.\n\n      ## PRD Context:\n      {{prd}}\n\n      ## Codebase:\n      {{code_context}}\n\n      Analyze:\n      1. Code organization and modularity\n      2. Separation of concerns\n      3. Error handling patterns\n      4. Dependency management\n\n      Format your response as JSON with keys: summary, recommendations, tasks\n\n  core_flow_hardening:\n    id: core_flow_hardening\n    goal: \"Identify robustness improvements for core flows\"\n    stage: hardening\n    template: |\n      Analyze core flows for robustness.\n\n      ## PRD:\n      {{prd}}\n\n      ## Codebase:\n      {{code_context}}\n\n      ## Analysis History:\n      {{history}}\n\n      Focus on:\n      1. Error handling and recovery\n      2. Retry logic for external calls\n      3. Input validation\n      4. Edge cases\n\n      Format your response as JSON with keys: summary, recommendations, tasks\n\n  test_suite_mvp:\n    id: test_suite_mvp\n    goal: \"Identify critical tests needed for MVP quality\"\n    stage: testing\n    template: |\n      Review test coverage for this codebase.\n\n      ## PRD:\n      {{prd}}\n\n      ## Codebase:\n      {{code_context}}\n\n      Identify:\n      1. Missing unit tests for core functions\n      2. Missing integration tests for main flows\n      3. Edge cases without test coverage\n\n      Format your response as JSON with keys: summary, recommendations, tasks\n```\n\nCreate `config/profiles.yaml` with content from PRD Section 7.2:\n\n```yaml\nprofiles:\n  automation_agent:\n    name: \"Automation Agent\"\n    description: \"Profile for CLI tools and automation agents\"\n    stages:\n      - alignment_with_prd\n      - architecture_sanity\n      - core_flow_hardening\n      - test_suite_mvp\n\n  backend_service:\n    name: \"Backend Service\"\n    description: \"Profile for API backends and services\"\n    stages:\n      - alignment_with_prd\n      - architecture_sanity\n      - core_flow_hardening\n      - test_suite_mvp\n\n  internal_tool:\n    name: \"Internal Tool\"\n    description: \"Profile for internal developer tools\"\n    stages:\n      - alignment_with_prd\n      - core_flow_hardening\n```",
        "testStrategy": "Validation tests:\n1. Validate prompts.yaml is valid YAML and parses correctly\n2. Validate profiles.yaml is valid YAML and parses correctly\n3. Test PromptLibrary loads all 4 prompts\n4. Test PromptLibrary loads all 3 profiles\n5. Test each prompt template renders without Jinja errors\n6. Verify all stages referenced in profiles exist in prompts",
        "priority": "medium",
        "dependencies": [
          "1",
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Author config/prompts.yaml with PRD templates",
            "description": "Create prompts.yaml file with all 4 prompt templates from PRD Section 7.1 including required Jinja placeholders",
            "dependencies": [],
            "details": "Copy exact content for alignment_with_prd, architecture_sanity, core_flow_hardening, test_suite_mvp prompts. Ensure all templates include {{prd}}, {{code_context}}, {{history}}, {{current_stage}} placeholders where appropriate. Verify YAML indentation and structure.",
            "status": "pending",
            "testStrategy": "Validate YAML parses correctly, check all 4 prompts load via PromptLibrary, test Jinja rendering with sample context for each template",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Author config/profiles.yaml with complete profiles",
            "description": "Create profiles.yaml with all 3 profiles and their stage sequences from PRD Section 7.2",
            "dependencies": [
              1
            ],
            "details": "Implement automation_agent, backend_service, internal_tool profiles with exact stage sequences. Ensure all referenced stage IDs exist in prompts.yaml (alignment_with_prd, architecture_sanity, core_flow_hardening, test_suite_mvp). Keep configs environment-agnostic.",
            "status": "pending",
            "testStrategy": "Validate YAML parses correctly, verify PromptLibrary loads all 3 profiles, check referential integrity between profiles.stages and prompts keys",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement validation tests for config files",
            "description": "Write tests/test_prompts.py to validate both YAML files load correctly and maintain integrity",
            "dependencies": [
              1,
              2
            ],
            "details": "Create pytest suite that: 1) Loads both YAML files via PromptLibrary, 2) Asserts 4 prompts and 3 profiles found, 3) Tests all prompt templates render without Jinja errors using mock context, 4) Verifies profile stage references exist in prompts, 5) Uses relative paths for CI compatibility.",
            "status": "pending",
            "testStrategy": "Run pytest tests/test_prompts.py. Should pass: YAML parsing, prompt/profile counts, template renderability, referential integrity checks. Tests must work in CI without environment-specific paths.",
            "parentId": "undefined"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down Task 9 (Create Complete Configuration Files) into 3 subtasks: (1) author prompts.yaml exactly per PRD, adding required Jinja placeholders (prd, code_context, history, current_stage) and validate formatting; (2) author profiles.yaml with all profiles and stage sequences, ensuring all referenced stages exist in prompts.yaml; (3) write small validation tests or scripts (e.g., in tests/test_prompts.py) to load both YAML files via PromptLibrary and assert prompt/profile counts, renderability, and referential integrity. Note how to keep these configs environment-agnostic for CI.",
        "updatedAt": "2025-12-14T02:00:33.045Z"
      },
      {
        "id": "10",
        "title": "Add Comprehensive Test Suite",
        "description": "Create a complete test suite covering all modules with unit tests, integration tests, and fixtures for testing.",
        "details": "Create comprehensive test files with pytest fixtures and mocks:\n\n`tests/conftest.py`:\n```python\nimport pytest\nfrom pathlib import Path\nimport tempfile\nimport shutil\n\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Create a temporary directory for tests.\"\"\"\n    d = Path(tempfile.mkdtemp())\n    yield d\n    shutil.rmtree(d, ignore_errors=True)\n\n@pytest.fixture\ndef sample_prd():\n    return '''# Test PRD\n## Overview\nThis is a test PRD for unit testing.\n## Requirements\n- REQ1: Feature one\n- REQ2: Feature two\n'''\n\n@pytest.fixture\ndef sample_code_context():\n    return '''# Packed Codebase\n## src/main.py\ndef main():\n    print(\"Hello World\")\n'''\n\n@pytest.fixture\ndef mock_prompts_yaml():\n    return '''prompts:\n  test_prompt:\n    id: test_prompt\n    goal: Test goal\n    stage: test\n    template: |\n      PRD: {{prd}}\n      Code: {{code_context}}\n'''\n\n@pytest.fixture\ndef mock_profiles_yaml():\n    return '''profiles:\n  test_profile:\n    name: Test Profile\n    description: For testing\n    stages:\n      - test_prompt\n'''\n\n@pytest.fixture\ndef config_dir(temp_dir, mock_prompts_yaml, mock_profiles_yaml):\n    \"\"\"Create a config directory with test YAML files.\"\"\"\n    config = temp_dir / 'config'\n    config.mkdir()\n    (config / 'prompts.yaml').write_text(mock_prompts_yaml)\n    (config / 'profiles.yaml').write_text(mock_profiles_yaml)\n    return config\n\n@pytest.fixture\ndef test_repo(temp_dir, sample_prd):\n    \"\"\"Create a test repository structure.\"\"\"\n    repo = temp_dir / 'test_repo'\n    repo.mkdir()\n    docs = repo / 'docs'\n    docs.mkdir()\n    (docs / 'prd.md').write_text(sample_prd)\n    (repo / 'src').mkdir()\n    (repo / 'src' / 'main.py').write_text('print(\"hello\")')\n    return repo\n```\n\nTest files to create:\n- `tests/test_config.py` - Config loading and validation\n- `tests/test_prompts.py` - Prompt/profile loading and rendering\n- `tests/test_repomix.py` - Repomix integration with mocked subprocess\n- `tests/test_analysis.py` - Analysis engine with mock and parsing\n- `tests/test_plan_writer.py` - Plan generation\n- `tests/test_orchestrator.py` - Full workflow with mocks\n- `tests/test_cli.py` - CLI commands with typer.testing.CliRunner\n\nEach test file should:\n- Use pytest fixtures from conftest.py\n- Mock external dependencies (subprocess, httpx)\n- Cover happy path and error cases\n- Be runnable with `pytest tests/`",
        "testStrategy": "Meta-test strategy:\n1. Run `pytest tests/ -v` to execute all tests\n2. Run `pytest tests/ --cov=metaagent --cov-report=html` for coverage\n3. Ensure >80% code coverage\n4. Verify all tests pass in CI environment (no API keys)\n5. Mark integration tests with @pytest.mark.integration\n6. Use `pytest -m \"not integration\"` for fast unit test runs",
        "priority": "medium",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create tests/conftest.py with shared fixtures",
            "description": "Implement conftest.py with fixtures for temp directories, sample PRD/code, mock YAML configs, and test repo structures.",
            "dependencies": [],
            "details": "Copy provided conftest.py code exactly. Add fixtures: temp_dir, sample_prd, sample_code_context, mock_prompts_yaml, mock_profiles_yaml, config_dir, test_repo. Ensure cleanup with shutil.rmtree.",
            "status": "pending",
            "testStrategy": "Run pytest tests/conftest.py -s to verify fixtures create/cleanup correctly without errors",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement tests/test_config.py",
            "description": "Create comprehensive tests for config.py covering env loading, defaults, validation, and path resolution.",
            "dependencies": [
              1
            ],
            "details": "Test Config loading from config_dir fixture, environment variable overrides, default values, path resolution with test_repo, validation errors for missing files/invalid YAML. Use parametrize for edge cases.",
            "status": "pending",
            "testStrategy": "pytest tests/test_config.py --cov=src/metaagent/config -v ensuring 90%+ coverage, happy path and validation errors pass",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement tests/test_prompts.py",
            "description": "Create tests for prompts.py module: loading YAML, rendering templates, error handling for missing/invalid files.",
            "dependencies": [
              1
            ],
            "details": "Test PromptLibrary loading from mock_prompts_yaml/profiles_yaml fixtures, Prompt.render() with jinja variables (prd, code_context, history, stage), get_prompts_for_profile(), list_profiles(), missing file errors.",
            "status": "pending",
            "testStrategy": "pytest tests/test_prompts.py -v verify rendering produces expected output strings, invalid YAML raises ValueError",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement tests/test_repomix.py with subprocess mocks",
            "description": "Test repomix.py with comprehensive subprocess.run mocking for success, failure, timeout, FileNotFoundError, truncation.",
            "dependencies": [
              1
            ],
            "details": "Use pytest mocker for subprocess.run. Test RepomixRunner with test_repo fixture, mock stdout/stderr/returncode, verify truncation logic, error handling paths, RepomixResult parsing.",
            "status": "pending",
            "testStrategy": "pytest tests/test_repomix.py::TestRepomix -v ensure all mock scenarios (0,1,timeout,FileNotFoundError) return expected RepomixResult objects",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement tests/test_analysis.py with httpx mocks",
            "description": "Test analysis.py: MockAnalysisEngine, create_analysis_engine(), _parse_response() with valid JSON and fallbacks.",
            "dependencies": [
              1
            ],
            "details": "Mock httpx.Client for real engine tests. Test mock mode returns deterministic AnalysisResult, parsing valid/invalid JSON responses, fallback parsing, use_mock=True/None api_key behavior.",
            "status": "pending",
            "testStrategy": "pytest tests/test_analysis.py -v verify MockAnalysisEngine.analyze() returns consistent AnalysisResult, parse_response handles malformed JSON gracefully",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement tests/test_plan_writer.py",
            "description": "Test plan_writer.py: file output generation, section formatting, priority sorting, edge case handling.",
            "dependencies": [
              1
            ],
            "details": "Use temp_dir fixture for output files. Test PlanWriter with sample AnalysisResult/tasks, verify markdown sections rendered correctly, priority sorting (high>medium>low), empty tasks edge case.",
            "status": "pending",
            "testStrategy": "pytest tests/test_plan_writer.py -v check generated markdown files match expected content via file.read_text()",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Implement orchestrator.py and CLI tests with coverage",
            "description": "Create tests/test_orchestrator.py and tests/test_cli.py. Add pytest markers, coverage config, CI-ready setup.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Test Orchestrator.refine() full workflow with mocks. Use typer.testing.CliRunner for CLI tests. Add pytest.ini with markers (slow, integration). Ensure no external deps. Mock all APIs/subprocess.",
            "status": "pending",
            "testStrategy": "pytest tests/ --cov=src/metaagent --cov-report=term-missing -m 'not slow' ensuring >80% coverage, all unit tests pass. Separate integration marker tests.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Break down Task 10 (Add Comprehensive Test Suite) into 7 subtasks: (1) create tests/conftest.py with shared fixtures for temp dirs, sample PRD/code, mock config YAMLs, and test repos; (2) implement tests for config.py (env loading, defaults, validation, path resolution); (3) implement tests for prompts.py (loading, rendering, missing files, invalid data); (4) implement tests for repomix.py with mocked subprocess.run covering success/failure/timeout/FileNotFoundError and truncation logic; (5) implement tests for analysis.py with mocked httpx, including mock engine and parse fallbacks; (6) implement tests for plan_writer.py (file output, sections, priority sorting, edge cases); (7) implement orchestrator and CLI tests using mocks/CliRunner, add coverage configuration, and mark slow/integration tests. Explicitly plan for deterministic, isolated tests suitable for CI with no external services or Node dependencies.",
        "updatedAt": "2025-12-14T02:00:39.501Z"
      },
      {
        "id": "11",
        "title": "Create Documentation and Environment Setup",
        "description": "Write README.md with installation instructions, usage examples, and update .env.example with all required environment variables.",
        "details": "Create `README.md`:\n\n```markdown\n# Meta-Agent\n\nA Python CLI tool for automated codebase refinement from v0 to MVP.\n\n## Overview\n\nMeta-Agent analyzes your codebase against a PRD and generates an improvement plan using AI-powered analysis. It integrates:\n- **Repomix** for codebase packing\n- **Perplexity API** for analysis and planning\n- Generates plans for **Claude Code** to implement\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone <repo-url>\ncd meta-agent\n\n# Install with uv (recommended)\nuv pip install -e .\n\n# Or with pip\npip install -e .\n```\n\n## Configuration\n\n1. Copy `.env.example` to `.env`\n2. Add your API keys:\n   - `PERPLEXITY_API_KEY` - Required for AI analysis\n   - `ANTHROPIC_API_KEY` - Optional, for Claude integration\n\n## Usage\n\n### Basic Usage\n\n```bash\n# Refine current directory with automation_agent profile\nmetaagent refine --profile automation_agent --repo .\n\n# Use mock mode for testing (no API calls)\nmetaagent refine --profile automation_agent --repo . --mock\n\n# List available profiles\nmetaagent list-profiles\n```\n\n### Profiles\n\n- `automation_agent` - For CLI tools and automation agents\n- `backend_service` - For API backends and services  \n- `internal_tool` - For internal developer tools\n\n### Output\n\nAfter running, find the improvement plan at `docs/mvp_improvement_plan.md`.\n\n## Development\n\n```bash\n# Install dev dependencies\nuv pip install -e \".[dev]\"\n\n# Run tests\npytest tests/ -v\n\n# Run with coverage\npytest tests/ --cov=metaagent --cov-report=html\n```\n\n## Project Structure\n\n```\nmeta-agent/\n├── src/metaagent/     # Main package\n│   ├── cli.py         # CLI entrypoint\n│   ├── orchestrator.py # Main workflow\n│   ├── repomix.py     # Repomix integration\n│   ├── prompts.py     # Prompt/profile loading\n│   ├── analysis.py    # LLM analysis engine\n│   └── plan_writer.py # Plan generation\n├── config/\n│   ├── prompts.yaml   # Prompt templates\n│   └── profiles.yaml  # Profile definitions\n└── tests/             # Test suite\n```\n\n## License\n\nMIT\n```\n\nUpdate `.env.example` for meta-agent:\n```bash\n# Meta-Agent Configuration\n\n# Required: Perplexity API key for analysis\nPERPLEXITY_API_KEY=\"pplx-...\"\n\n# Optional: Anthropic API key for Claude integration\nANTHROPIC_API_KEY=\"sk-ant-...\"\n\n# Optional: Logging and behavior\nMETAAGENT_LOG_LEVEL=INFO\nMETAAGENT_TIMEOUT=120\nMETAAGENT_MAX_TOKENS=100000\n```",
        "testStrategy": "Verification:\n1. README renders correctly in GitHub/GitLab\n2. Installation instructions work on clean environment\n3. All CLI examples in README work correctly\n4. .env.example contains all environment variables used in code\n5. No sensitive data in example files",
        "priority": "low",
        "dependencies": [
          "1",
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Write Comprehensive README.md",
            "description": "Create the full README.md file including overview, installation instructions for uv and pip, configuration steps, usage examples, profiles description, output details, development workflow, and project structure as specified.",
            "dependencies": [],
            "details": "Use the provided README template as base. Ensure all sections are complete: Overview with integrations (Repomix, Perplexity, Claude), Installation with git clone + uv/pip, Configuration with .env steps, Usage with basic/mock/list-profiles examples, Profiles list, Output location, Development with dev deps/tests/coverage, Project structure tree, and License. Make repo-url placeholder clear.",
            "status": "pending",
            "testStrategy": "Verify Markdown renders correctly in GitHub preview, check all code blocks are valid bash/python, ensure instructions match current CLI behavior from task dependencies.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create and Update .env.example File",
            "description": "Generate .env.example with all environment variables from Config.from_env including required/optional vars, safe placeholders, and explanatory comments.",
            "dependencies": [],
            "details": "Include PERPLEXITY_API_KEY (required), ANTHROPIC_API_KEY (optional), METAAGENT_LOG_LEVEL=INFO, METAAGENT_TIMEOUT=120, METAAGENT_MAX_TOKENS=100000. Match exactly Config dataclass fields: perplexity_api_key, anthropic_api_key, log_level, timeout, max_tokens. Add comments explaining purpose and defaults.",
            "status": "pending",
            "testStrategy": "Validate file is correct YAML-like format, cross-check against src/metaagent/config.py Config.from_env expected vars, ensure no real API keys, test loading with dotenv in clean env.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Validate Documentation in Clean Environment",
            "description": "Manually test README instructions and .env.example by following steps in a fresh environment, verify consistency with code and CLI behavior.",
            "dependencies": [
              1,
              2
            ],
            "details": "In clean dir: 1) Clone/follow install (uv/pip), 2) Copy .env.example to .env with dummy keys, 3) Run CLI examples (metaagent --help, list-profiles, refine --mock), 4) Check output/docs/mvp_improvement_plan.md generates, 5) Run dev commands (pytest). Note any discrepancies for future updates.",
            "status": "pending",
            "testStrategy": "Checklist: Installation succeeds without errors, all README CLI examples execute without crashes, .env vars load correctly per Config.from_env tests, docs match actual project structure/dependencies (tasks 1,2,8,9), no broken links/paths.",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down Task 11 (Create Documentation and Environment Setup) into 3 subtasks: (1) write README.md with clear overview, installation (uv and pip), configuration, usage examples, profiles description, output description, development workflow, and project structure; (2) create .env.example listing all relevant environment variables with safe placeholder values and comments; (3) manually validate docs by following the README in a clean environment and cross-checking that .env.example matches what Config.from_env expects and what the code actually uses. Note how to keep examples up to date with CLI and config behavior.",
        "updatedAt": "2025-12-14T02:00:46.066Z"
      },
      {
        "id": "12",
        "title": "End-to-End Integration Test with Mock Mode",
        "description": "Create an end-to-end test that exercises the complete workflow using mock mode, verifying the system generates a valid improvement plan.",
        "details": "Create `tests/test_e2e.py`:\n\n```python\nimport pytest\nfrom pathlib import Path\nfrom typer.testing import CliRunner\n\nfrom metaagent.cli import app\n\nrunner = CliRunner()\n\nclass TestEndToEnd:\n    \"\"\"End-to-end tests for the complete refinement workflow.\"\"\"\n    \n    @pytest.fixture\n    def sample_repo(self, tmp_path):\n        \"\"\"Create a complete sample repository for testing.\"\"\"\n        repo = tmp_path / 'sample_project'\n        repo.mkdir()\n        \n        # Create docs/prd.md\n        docs = repo / 'docs'\n        docs.mkdir()\n        (docs / 'prd.md').write_text('''\n# Sample Project PRD\n\n## Overview\nA simple calculator CLI application.\n\n## Requirements\n- FR1: Add two numbers\n- FR2: Subtract two numbers\n- FR3: Display help message\n\n## Success Criteria\n- All operations return correct results\n- Error handling for invalid input\n''')\n        \n        # Create source files\n        src = repo / 'src'\n        src.mkdir()\n        (src / 'calculator.py').write_text('''\ndef add(a, b):\n    return a + b\n\ndef subtract(a, b):\n    return a - b\n''')\n        (src / 'main.py').write_text('''\nfrom calculator import add, subtract\nimport sys\n\ndef main():\n    if len(sys.argv) < 4:\n        print(\"Usage: calc <add|sub> <a> <b>\")\n        return\n    op, a, b = sys.argv[1], int(sys.argv[2]), int(sys.argv[3])\n    if op == \"add\":\n        print(add(a, b))\n    elif op == \"sub\":\n        print(subtract(a, b))\n\nif __name__ == \"__main__\":\n    main()\n''')\n        \n        return repo\n    \n    def test_full_refinement_mock_mode(self, sample_repo):\n        \"\"\"Test complete refinement workflow in mock mode.\"\"\"\n        result = runner.invoke(app, [\n            'refine',\n            '--profile', 'automation_agent',\n            '--repo', str(sample_repo),\n            '--mock'\n        ])\n        \n        assert result.exit_code == 0\n        assert 'Refinement completed' in result.stdout\n        \n        # Verify plan was created\n        plan_path = sample_repo / 'docs' / 'mvp_improvement_plan.md'\n        assert plan_path.exists()\n        \n        plan_content = plan_path.read_text()\n        assert '# MVP Improvement Plan' in plan_content\n        assert 'automation_agent' in plan_content\n        assert 'Prioritized Task List' in plan_content\n        assert 'Instructions for Claude Code' in plan_content\n    \n    def test_refinement_with_internal_tool_profile(self, sample_repo):\n        \"\"\"Test refinement with internal_tool profile (fewer stages).\"\"\"\n        result = runner.invoke(app, [\n            'refine',\n            '--profile', 'internal_tool',\n            '--repo', str(sample_repo),\n            '--mock'\n        ])\n        \n        assert result.exit_code == 0\n        \n        plan_path = sample_repo / 'docs' / 'mvp_improvement_plan.md'\n        plan_content = plan_path.read_text()\n        \n        # internal_tool has fewer stages\n        assert 'alignment_with_prd' in plan_content\n        assert 'core_flow_hardening' in plan_content\n    \n    def test_refinement_fails_without_prd(self, tmp_path):\n        \"\"\"Test that refinement fails gracefully without PRD.\"\"\"\n        empty_repo = tmp_path / 'empty'\n        empty_repo.mkdir()\n        \n        result = runner.invoke(app, [\n            'refine',\n            '--profile', 'automation_agent',\n            '--repo', str(empty_repo),\n            '--mock'\n        ])\n        \n        # Should fail or warn about missing PRD\n        assert 'PRD' in result.stdout or result.exit_code != 0\n    \n    def test_list_profiles_command(self):\n        \"\"\"Test list-profiles command shows available profiles.\"\"\"\n        result = runner.invoke(app, ['list-profiles'])\n        \n        assert result.exit_code == 0\n        assert 'automation_agent' in result.stdout\n        assert 'backend_service' in result.stdout\n        assert 'internal_tool' in result.stdout\n```\n\nThis test validates:\n1. CLI invocation works correctly\n2. Mock mode bypasses API calls\n3. Plan file is generated with correct structure\n4. Different profiles produce appropriate output\n5. Error handling for missing PRD\n6. list-profiles command works",
        "testStrategy": "Run e2e tests:\n1. `pytest tests/test_e2e.py -v` to run all e2e tests\n2. Verify tests pass without any API keys configured\n3. Verify tests are isolated (use tmp_path fixtures)\n4. Verify generated plan files contain expected sections\n5. Run with `--tb=long` to debug any failures",
        "priority": "medium",
        "dependencies": [
          "7",
          "8",
          "9",
          "10"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and implement sample_repo fixture with minimal realistic project structure",
            "description": "Create a pytest fixture that builds a temporary sample repository with docs/prd.md and simple src code mirroring a realistic yet minimal project for end-to-end tests.",
            "dependencies": [],
            "details": "Implement the sample_repo fixture in tests/test_e2e.py using tmp_path to create an isolated directory tree (e.g., sample_project/docs/prd.md and src/*.py). Ensure prd.md contains PRD-style sections (Overview, Requirements, Success Criteria) and src includes a small but working CLI app (e.g., calculator main and helper module). Confirm paths match what metaagent refine expects (docs/prd.md and any default repo layout assumptions). Keep file contents minimal but sufficient for downstream PRD parsing and prompt rendering.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Write end-to-end tests invoking metaagent refine in mock mode for multiple profiles",
            "description": "Add pytest tests that use Typer’s CliRunner to invoke the metaagent CLI refine command in mock mode with different profiles, asserting exit codes and key output text.",
            "dependencies": [
              1
            ],
            "details": "In tests/test_e2e.py, use CliRunner (from typer.testing) to run app with commands like ['refine', '--profile', 'automation_agent', '--repo', str(sample_repo), '--mock']. Assert result.exit_code == 0 and that known success strings such as 'Refinement completed' (or equivalent success message) appear in result.stdout. Add a separate test for at least one additional profile (e.g., internal_tool) to ensure profile selection is wired correctly in the CLI and orchestrator. Ensure these tests rely only on mock mode (no network, no real API keys).",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Assert generation and structure of docs/mvp_improvement_plan.md for each profile",
            "description": "Verify that running refine in mock mode creates docs/mvp_improvement_plan.md and that the file contains required sections, headings, and profile-specific markers.",
            "dependencies": [
              2
            ],
            "details": "Extend the refine tests to compute plan_path = sample_repo / 'docs' / 'mvp_improvement_plan.md' and assert plan_path.exists(). Read the file and assert presence of required structural elements such as '# MVP Improvement Plan', a section describing a prioritized task list, and any expected headings like 'Prioritized Task List' and 'Instructions for Claude Code'. Also assert that the active profile name (e.g., 'automation_agent' or 'internal_tool') appears somewhere in the plan body so tests confirm profile-aware content. Keep expectations general enough that normal template copy changes do not cause brittle failures, but specific enough to validate correct plan template wiring.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add tests for failure scenarios and list-profiles CLI behavior",
            "description": "Create additional end-to-end tests covering graceful failure when PRD is missing and verifying that the list-profiles command reports all expected profiles.",
            "dependencies": [
              1,
              2
            ],
            "details": "Add a test that creates an empty temporary repo (e.g., empty_repo = tmp_path / 'empty') without docs/prd.md and runs refine in mock mode. Assert non-zero exit_code or that stdout contains a clear message mentioning missing PRD. Add a separate test invoking ['list-profiles'] and assert exit_code == 0 and that stdout contains all configured profile identifiers (e.g., automation_agent, backend_service, internal_tool). These tests should exercise the same CLI app object and use CliRunner, confirming both error handling paths and discovery/printing of available profiles.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Ensure e2e tests are isolated, fast, and optionally marked as integration tests",
            "description": "Harden the e2e test module by enforcing isolation with tmp_path fixtures, avoiding real network/API calls via mock mode, and optionally marking tests as integration for selective running.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Review tests/test_e2e.py to confirm all filesystem operations use tmp_path-based fixtures (no writes to the real project tree) and that refine is always invoked with --mock so the orchestrator never calls real external services. Optionally add a custom pytest marker like @pytest.mark.integration to the TestEndToEnd class or individual tests so they can be included/excluded via -m integration. Confirm that running pytest tests/test_e2e.py -v completes quickly and does not require any environment variables or API keys. Document any ordering dependencies on other tasks (e.g., requires CLI wiring, profiles/prompt config, and mock analysis path from tasks 7–10 to be implemented) in comments or task notes.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down Task 12 (End-to-End Integration Test with Mock Mode) into 5 subtasks: (1) design sample_repo fixture that mirrors a realistic but minimal project with docs/prd.md and simple src code; (2) write e2e tests using CliRunner to invoke `metaagent refine` in mock mode for different profiles, asserting exit codes and key output strings; (3) assert that docs/mvp_improvement_plan.md is created and contains required sections and profile info; (4) add tests for failure scenarios (e.g., missing PRD) and for list-profiles behavior; (5) ensure tests are isolated via tmp_path, run quickly using mock analysis (no network), and are optionally marked as integration tests. Highlight any ordering dependencies with other tasks/tests.",
        "updatedAt": "2025-12-14T02:00:52.678Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-14T02:33:18.776Z",
      "taskCount": 12,
      "completedCount": 12,
      "tags": [
        "master"
      ]
    }
  }
}
</file>

<file path=".taskmaster/templates/example_prd_rpg.txt">
<rpg-method>
# Repository Planning Graph (RPG) Method - PRD Template

This template teaches you (AI or human) how to create structured, dependency-aware PRDs using the RPG methodology from Microsoft Research. The key insight: separate WHAT (functional) from HOW (structural), then connect them with explicit dependencies.

## Core Principles

1. **Dual-Semantics**: Think functional (capabilities) AND structural (code organization) separately, then map them
2. **Explicit Dependencies**: Never assume - always state what depends on what
3. **Topological Order**: Build foundation first, then layers on top
4. **Progressive Refinement**: Start broad, refine iteratively

## How to Use This Template

- Follow the instructions in each `<instruction>` block
- Look at `<example>` blocks to see good vs bad patterns
- Fill in the content sections with your project details
- The AI reading this will learn the RPG method by following along
- Task Master will parse the resulting PRD into dependency-aware tasks

## Recommended Tools for Creating PRDs

When using this template to **create** a PRD (not parse it), use **code-context-aware AI assistants** for best results:

**Why?** The AI needs to understand your existing codebase to make good architectural decisions about modules, dependencies, and integration points.

**Recommended tools:**
- **Claude Code** (claude-code CLI) - Best for structured reasoning and large contexts
- **Cursor/Windsurf** - IDE integration with full codebase context
- **Gemini CLI** (gemini-cli) - Massive context window for large codebases
- **Codex/Grok CLI** - Strong code generation with context awareness

**Note:** Once your PRD is created, `task-master parse-prd` works with any configured AI model - it just needs to read the PRD text itself, not your codebase.
</rpg-method>

---

<overview>
<instruction>
Start with the problem, not the solution. Be specific about:
- What pain point exists?
- Who experiences it?
- Why existing solutions don't work?
- What success looks like (measurable outcomes)?

Keep this section focused - don't jump into implementation details yet.
</instruction>

## Problem Statement
[Describe the core problem. Be concrete about user pain points.]

## Target Users
[Define personas, their workflows, and what they're trying to achieve.]

## Success Metrics
[Quantifiable outcomes. Examples: "80% task completion via autopilot", "< 5% manual intervention rate"]

</overview>

---

<functional-decomposition>
<instruction>
Now think about CAPABILITIES (what the system DOES), not code structure yet.

Step 1: Identify high-level capability domains
- Think: "What major things does this system do?"
- Examples: Data Management, Core Processing, Presentation Layer

Step 2: For each capability, enumerate specific features
- Use explore-exploit strategy:
  * Exploit: What features are REQUIRED for core value?
  * Explore: What features make this domain COMPLETE?

Step 3: For each feature, define:
- Description: What it does in one sentence
- Inputs: What data/context it needs
- Outputs: What it produces/returns
- Behavior: Key logic or transformations

<example type="good">
Capability: Data Validation
  Feature: Schema validation
    - Description: Validate JSON payloads against defined schemas
    - Inputs: JSON object, schema definition
    - Outputs: Validation result (pass/fail) + error details
    - Behavior: Iterate fields, check types, enforce constraints

  Feature: Business rule validation
    - Description: Apply domain-specific validation rules
    - Inputs: Validated data object, rule set
    - Outputs: Boolean + list of violated rules
    - Behavior: Execute rules sequentially, short-circuit on failure
</example>

<example type="bad">
Capability: validation.js
  (Problem: This is a FILE, not a CAPABILITY. Mixing structure into functional thinking.)

Capability: Validation
  Feature: Make sure data is good
  (Problem: Too vague. No inputs/outputs. Not actionable.)
</example>
</instruction>

## Capability Tree

### Capability: [Name]
[Brief description of what this capability domain covers]

#### Feature: [Name]
- **Description**: [One sentence]
- **Inputs**: [What it needs]
- **Outputs**: [What it produces]
- **Behavior**: [Key logic]

#### Feature: [Name]
- **Description**:
- **Inputs**:
- **Outputs**:
- **Behavior**:

### Capability: [Name]
...

</functional-decomposition>

---

<structural-decomposition>
<instruction>
NOW think about code organization. Map capabilities to actual file/folder structure.

Rules:
1. Each capability maps to a module (folder or file)
2. Features within a capability map to functions/classes
3. Use clear module boundaries - each module has ONE responsibility
4. Define what each module exports (public interface)

The goal: Create a clear mapping between "what it does" (functional) and "where it lives" (structural).

<example type="good">
Capability: Data Validation
  → Maps to: src/validation/
    ├── schema-validator.js      (Schema validation feature)
    ├── rule-validator.js         (Business rule validation feature)
    └── index.js                  (Public exports)

Exports:
  - validateSchema(data, schema)
  - validateRules(data, rules)
</example>

<example type="bad">
Capability: Data Validation
  → Maps to: src/utils.js
  (Problem: "utils" is not a clear module boundary. Where do I find validation logic?)

Capability: Data Validation
  → Maps to: src/validation/everything.js
  (Problem: One giant file. Features should map to separate files for maintainability.)
</example>
</instruction>

## Repository Structure

```
project-root/
├── src/
│   ├── [module-name]/       # Maps to: [Capability Name]
│   │   ├── [file].js        # Maps to: [Feature Name]
│   │   └── index.js         # Public exports
│   └── [module-name]/
├── tests/
└── docs/
```

## Module Definitions

### Module: [Name]
- **Maps to capability**: [Capability from functional decomposition]
- **Responsibility**: [Single clear purpose]
- **File structure**:
  ```
  module-name/
  ├── feature1.js
  ├── feature2.js
  └── index.js
  ```
- **Exports**:
  - `functionName()` - [what it does]
  - `ClassName` - [what it does]

</structural-decomposition>

---

<dependency-graph>
<instruction>
This is THE CRITICAL SECTION for Task Master parsing.

Define explicit dependencies between modules. This creates the topological order for task execution.

Rules:
1. List modules in dependency order (foundation first)
2. For each module, state what it depends on
3. Foundation modules should have NO dependencies
4. Every non-foundation module should depend on at least one other module
5. Think: "What must EXIST before I can build this module?"

<example type="good">
Foundation Layer (no dependencies):
  - error-handling: No dependencies
  - config-manager: No dependencies
  - base-types: No dependencies

Data Layer:
  - schema-validator: Depends on [base-types, error-handling]
  - data-ingestion: Depends on [schema-validator, config-manager]

Core Layer:
  - algorithm-engine: Depends on [base-types, error-handling]
  - pipeline-orchestrator: Depends on [algorithm-engine, data-ingestion]
</example>

<example type="bad">
- validation: Depends on API
- API: Depends on validation
(Problem: Circular dependency. This will cause build/runtime issues.)

- user-auth: Depends on everything
(Problem: Too many dependencies. Should be more focused.)
</example>
</instruction>

## Dependency Chain

### Foundation Layer (Phase 0)
No dependencies - these are built first.

- **[Module Name]**: [What it provides]
- **[Module Name]**: [What it provides]

### [Layer Name] (Phase 1)
- **[Module Name]**: Depends on [[module-from-phase-0], [module-from-phase-0]]
- **[Module Name]**: Depends on [[module-from-phase-0]]

### [Layer Name] (Phase 2)
- **[Module Name]**: Depends on [[module-from-phase-1], [module-from-foundation]]

[Continue building up layers...]

</dependency-graph>

---

<implementation-roadmap>
<instruction>
Turn the dependency graph into concrete development phases.

Each phase should:
1. Have clear entry criteria (what must exist before starting)
2. Contain tasks that can be parallelized (no inter-dependencies within phase)
3. Have clear exit criteria (how do we know phase is complete?)
4. Build toward something USABLE (not just infrastructure)

Phase ordering follows topological sort of dependency graph.

<example type="good">
Phase 0: Foundation
  Entry: Clean repository
  Tasks:
    - Implement error handling utilities
    - Create base type definitions
    - Setup configuration system
  Exit: Other modules can import foundation without errors

Phase 1: Data Layer
  Entry: Phase 0 complete
  Tasks:
    - Implement schema validator (uses: base types, error handling)
    - Build data ingestion pipeline (uses: validator, config)
  Exit: End-to-end data flow from input to validated output
</example>

<example type="bad">
Phase 1: Build Everything
  Tasks:
    - API
    - Database
    - UI
    - Tests
  (Problem: No clear focus. Too broad. Dependencies not considered.)
</example>
</instruction>

## Development Phases

### Phase 0: [Foundation Name]
**Goal**: [What foundational capability this establishes]

**Entry Criteria**: [What must be true before starting]

**Tasks**:
- [ ] [Task name] (depends on: [none or list])
  - Acceptance criteria: [How we know it's done]
  - Test strategy: [What tests prove it works]

- [ ] [Task name] (depends on: [none or list])

**Exit Criteria**: [Observable outcome that proves phase complete]

**Delivers**: [What can users/developers do after this phase?]

---

### Phase 1: [Layer Name]
**Goal**:

**Entry Criteria**: Phase 0 complete

**Tasks**:
- [ ] [Task name] (depends on: [[tasks-from-phase-0]])
- [ ] [Task name] (depends on: [[tasks-from-phase-0]])

**Exit Criteria**:

**Delivers**:

---

[Continue with more phases...]

</implementation-roadmap>

---

<test-strategy>
<instruction>
Define how testing will be integrated throughout development (TDD approach).

Specify:
1. Test pyramid ratios (unit vs integration vs e2e)
2. Coverage requirements
3. Critical test scenarios
4. Test generation guidelines for Surgical Test Generator

This section guides the AI when generating tests during the RED phase of TDD.

<example type="good">
Critical Test Scenarios for Data Validation module:
  - Happy path: Valid data passes all checks
  - Edge cases: Empty strings, null values, boundary numbers
  - Error cases: Invalid types, missing required fields
  - Integration: Validator works with ingestion pipeline
</example>
</instruction>

## Test Pyramid

```
        /\
       /E2E\       ← [X]% (End-to-end, slow, comprehensive)
      /------\
     /Integration\ ← [Y]% (Module interactions)
    /------------\
   /  Unit Tests  \ ← [Z]% (Fast, isolated, deterministic)
  /----------------\
```

## Coverage Requirements
- Line coverage: [X]% minimum
- Branch coverage: [X]% minimum
- Function coverage: [X]% minimum
- Statement coverage: [X]% minimum

## Critical Test Scenarios

### [Module/Feature Name]
**Happy path**:
- [Scenario description]
- Expected: [What should happen]

**Edge cases**:
- [Scenario description]
- Expected: [What should happen]

**Error cases**:
- [Scenario description]
- Expected: [How system handles failure]

**Integration points**:
- [What interactions to test]
- Expected: [End-to-end behavior]

## Test Generation Guidelines
[Specific instructions for Surgical Test Generator about what to focus on, what patterns to follow, project-specific test conventions]

</test-strategy>

---

<architecture>
<instruction>
Describe technical architecture, data models, and key design decisions.

Keep this section AFTER functional/structural decomposition - implementation details come after understanding structure.
</instruction>

## System Components
[Major architectural pieces and their responsibilities]

## Data Models
[Core data structures, schemas, database design]

## Technology Stack
[Languages, frameworks, key libraries]

**Decision: [Technology/Pattern]**
- **Rationale**: [Why chosen]
- **Trade-offs**: [What we're giving up]
- **Alternatives considered**: [What else we looked at]

</architecture>

---

<risks>
<instruction>
Identify risks that could derail development and how to mitigate them.

Categories:
- Technical risks (complexity, unknowns)
- Dependency risks (blocking issues)
- Scope risks (creep, underestimation)
</instruction>

## Technical Risks
**Risk**: [Description]
- **Impact**: [High/Medium/Low - effect on project]
- **Likelihood**: [High/Medium/Low]
- **Mitigation**: [How to address]
- **Fallback**: [Plan B if mitigation fails]

## Dependency Risks
[External dependencies, blocking issues]

## Scope Risks
[Scope creep, underestimation, unclear requirements]

</risks>

---

<appendix>
## References
[Papers, documentation, similar systems]

## Glossary
[Domain-specific terms]

## Open Questions
[Things to resolve during development]
</appendix>

---

<task-master-integration>
# How Task Master Uses This PRD

When you run `task-master parse-prd <file>.txt`, the parser:

1. **Extracts capabilities** → Main tasks
   - Each `### Capability:` becomes a top-level task

2. **Extracts features** → Subtasks
   - Each `#### Feature:` becomes a subtask under its capability

3. **Parses dependencies** → Task dependencies
   - `Depends on: [X, Y]` sets task.dependencies = ["X", "Y"]

4. **Orders by phases** → Task priorities
   - Phase 0 tasks = highest priority
   - Phase N tasks = lower priority, properly sequenced

5. **Uses test strategy** → Test generation context
   - Feeds test scenarios to Surgical Test Generator during implementation

**Result**: A dependency-aware task graph that can be executed in topological order.

## Why RPG Structure Matters

Traditional flat PRDs lead to:
- ❌ Unclear task dependencies
- ❌ Arbitrary task ordering
- ❌ Circular dependencies discovered late
- ❌ Poorly scoped tasks

RPG-structured PRDs provide:
- ✅ Explicit dependency chains
- ✅ Topological execution order
- ✅ Clear module boundaries
- ✅ Validated task graph before implementation

## Tips for Best Results

1. **Spend time on dependency graph** - This is the most valuable section for Task Master
2. **Keep features atomic** - Each feature should be independently testable
3. **Progressive refinement** - Start broad, use `task-master expand` to break down complex tasks
4. **Use research mode** - `task-master parse-prd --research` leverages AI for better task generation
</task-master-integration>
</file>

<file path=".taskmaster/templates/example_prd.txt">
<context>
# Overview  
[Provide a high-level overview of your product here. Explain what problem it solves, who it's for, and why it's valuable.]

# Core Features  
[List and describe the main features of your product. For each feature, include:
- What it does
- Why it's important
- How it works at a high level]

# User Experience  
[Describe the user journey and experience. Include:
- User personas
- Key user flows
- UI/UX considerations]
</context>
<PRD>
# Technical Architecture  
[Outline the technical implementation details:
- System components
- Data models
- APIs and integrations
- Infrastructure requirements]

# Development Roadmap  
[Break down the development process into phases:
- MVP requirements
- Future enhancements
- Do not think about timelines whatsoever -- all that matters is scope and detailing exactly what needs to be build in each phase so it can later be cut up into tasks]

# Logical Dependency Chain
[Define the logical order of development:
- Which features need to be built first (foundation)
- Getting as quickly as possible to something usable/visible front end that works
- Properly pacing and scoping each feature so it is atomic but can also be built upon and improved as development approaches]

# Risks and Mitigations  
[Identify potential risks and how they'll be addressed:
- Technical challenges
- Figuring out the MVP that we can build upon
- Resource constraints]

# Appendix  
[Include any additional information:
- Research findings
- Technical specifications]
</PRD>
</file>

<file path=".taskmaster/CLAUDE.md">
# Task Master AI - Agent Integration Guide

## Essential Commands

### Core Workflow Commands

```bash
# Project Setup
task-master init                                    # Initialize Task Master in current project
task-master parse-prd .taskmaster/docs/prd.md       # Generate tasks from PRD document
task-master models --setup                        # Configure AI models interactively

# Daily Development Workflow
task-master list                                   # Show all tasks with status
task-master next                                   # Get next available task to work on
task-master show <id>                             # View detailed task information (e.g., task-master show 1.2)
task-master set-status --id=<id> --status=done    # Mark task complete

# Task Management
task-master add-task --prompt="description" --research        # Add new task with AI assistance
task-master expand --id=<id> --research --force              # Break task into subtasks
task-master update-task --id=<id> --prompt="changes"         # Update specific task
task-master update --from=<id> --prompt="changes"            # Update multiple tasks from ID onwards
task-master update-subtask --id=<id> --prompt="notes"        # Add implementation notes to subtask

# Analysis & Planning
task-master analyze-complexity --research          # Analyze task complexity
task-master complexity-report                      # View complexity analysis
task-master expand --all --research               # Expand all eligible tasks

# Dependencies & Organization
task-master add-dependency --id=<id> --depends-on=<id>       # Add task dependency
task-master move --from=<id> --to=<id>                       # Reorganize task hierarchy
task-master validate-dependencies                            # Check for dependency issues
task-master generate                                         # Update task markdown files (usually auto-called)
```

## Key Files & Project Structure

### Core Files

- `.taskmaster/tasks/tasks.json` - Main task data file (auto-managed)
- `.taskmaster/config.json` - AI model configuration (use `task-master models` to modify)
- `.taskmaster/docs/prd.md` - Product Requirements Document for parsing (`.md` extension recommended for better editor support)
- `.taskmaster/tasks/*.txt` - Individual task files (auto-generated from tasks.json)
- `.env` - API keys for CLI usage

**PRD File Format:** While both `.txt` and `.md` extensions work, **`.md` is recommended** because:
- Markdown syntax highlighting in editors improves readability
- Proper rendering when previewing in VS Code, GitHub, or other tools
- Better collaboration through formatted documentation

### Claude Code Integration Files

- `CLAUDE.md` - Auto-loaded context for Claude Code (this file)
- `.claude/settings.json` - Claude Code tool allowlist and preferences
- `.claude/commands/` - Custom slash commands for repeated workflows
- `.mcp.json` - MCP server configuration (project-specific)

### Directory Structure

```
project/
├── .taskmaster/
│   ├── tasks/              # Task files directory
│   │   ├── tasks.json      # Main task database
│   │   ├── task-1.md      # Individual task files
│   │   └── task-2.md
│   ├── docs/              # Documentation directory
│   │   ├── prd.md         # Product requirements (.md recommended)
│   ├── reports/           # Analysis reports directory
│   │   └── task-complexity-report.json
│   ├── templates/         # Template files
│   │   └── example_prd.md  # Example PRD template (.md recommended)
│   └── config.json        # AI models & settings
├── .claude/
│   ├── settings.json      # Claude Code configuration
│   └── commands/         # Custom slash commands
├── .env                  # API keys
├── .mcp.json            # MCP configuration
└── CLAUDE.md            # This file - auto-loaded by Claude Code
```

## MCP Integration

Task Master provides an MCP server that Claude Code can connect to. Configure in `.mcp.json`:

```json
{
  "mcpServers": {
    "task-master-ai": {
      "command": "npx",
      "args": ["-y", "task-master-ai"],
      "env": {
        "TASK_MASTER_TOOLS": "core",
        "ANTHROPIC_API_KEY": "your_key_here",
        "PERPLEXITY_API_KEY": "your_key_here",
        "OPENAI_API_KEY": "OPENAI_API_KEY_HERE",
        "GOOGLE_API_KEY": "GOOGLE_API_KEY_HERE",
        "XAI_API_KEY": "XAI_API_KEY_HERE",
        "OPENROUTER_API_KEY": "OPENROUTER_API_KEY_HERE",
        "MISTRAL_API_KEY": "MISTRAL_API_KEY_HERE",
        "AZURE_OPENAI_API_KEY": "AZURE_OPENAI_API_KEY_HERE",
        "OLLAMA_API_KEY": "OLLAMA_API_KEY_HERE"
      }
    }
  }
}
```

### MCP Tool Tiers

Default: `core` (7 tools). Set via `TASK_MASTER_TOOLS` env var.

| Tier | Count | Tools |
|------|-------|-------|
| `core` | 7 | `get_tasks`, `next_task`, `get_task`, `set_task_status`, `update_subtask`, `parse_prd`, `expand_task` |
| `standard` | 14 | core + `initialize_project`, `analyze_project_complexity`, `expand_all`, `add_subtask`, `remove_task`, `add_task`, `complexity_report` |
| `all` | 44+ | standard + dependencies, tags, research, autopilot, scoping, models, rules |

**Upgrade when tool unavailable:** Edit MCP config, change `TASK_MASTER_TOOLS` from `"core"` to `"standard"` or `"all"`, restart MCP.

### Essential MCP Tools

```javascript
help; // = shows available taskmaster commands
// Project setup
initialize_project; // = task-master init
parse_prd; // = task-master parse-prd

// Daily workflow
get_tasks; // = task-master list
next_task; // = task-master next
get_task; // = task-master show <id>
set_task_status; // = task-master set-status

// Task management
add_task; // = task-master add-task
expand_task; // = task-master expand
update_task; // = task-master update-task
update_subtask; // = task-master update-subtask
update; // = task-master update

// Analysis
analyze_project_complexity; // = task-master analyze-complexity
complexity_report; // = task-master complexity-report
```

## Claude Code Workflow Integration

### Standard Development Workflow

#### 1. Project Initialization

```bash
# Initialize Task Master
task-master init

# Create or obtain PRD, then parse it (use .md extension for better editor support)
task-master parse-prd .taskmaster/docs/prd.md

# Analyze complexity and expand tasks
task-master analyze-complexity --research
task-master expand --all --research
```

If tasks already exist, another PRD can be parsed (with new information only!) using parse-prd with --append flag. This will add the generated tasks to the existing list of tasks..

#### 2. Daily Development Loop

```bash
# Start each session
task-master next                           # Find next available task
task-master show <id>                     # Review task details

# During implementation, check in code context into the tasks and subtasks
task-master update-subtask --id=<id> --prompt="implementation notes..."

# Complete tasks
task-master set-status --id=<id> --status=done
```

#### 3. Multi-Claude Workflows

For complex projects, use multiple Claude Code sessions:

```bash
# Terminal 1: Main implementation
cd project && claude

# Terminal 2: Testing and validation
cd project-test-worktree && claude

# Terminal 3: Documentation updates
cd project-docs-worktree && claude
```

### Custom Slash Commands

Create `.claude/commands/taskmaster-next.md`:

```markdown
Find the next available Task Master task and show its details.

Steps:

1. Run `task-master next` to get the next task
2. If a task is available, run `task-master show <id>` for full details
3. Provide a summary of what needs to be implemented
4. Suggest the first implementation step
```

Create `.claude/commands/taskmaster-complete.md`:

```markdown
Complete a Task Master task: $ARGUMENTS

Steps:

1. Review the current task with `task-master show $ARGUMENTS`
2. Verify all implementation is complete
3. Run any tests related to this task
4. Mark as complete: `task-master set-status --id=$ARGUMENTS --status=done`
5. Show the next available task with `task-master next`
```

## Tool Allowlist Recommendations

Add to `.claude/settings.json`:

```json
{
  "allowedTools": [
    "Edit",
    "Bash(task-master *)",
    "Bash(git commit:*)",
    "Bash(git add:*)",
    "Bash(npm run *)",
    "mcp__task_master_ai__*"
  ]
}
```

## Configuration & Setup

### API Keys Required

At least **one** of these API keys must be configured:

- `ANTHROPIC_API_KEY` (Claude models) - **Recommended**
- `PERPLEXITY_API_KEY` (Research features) - **Highly recommended**
- `OPENAI_API_KEY` (GPT models)
- `GOOGLE_API_KEY` (Gemini models)
- `MISTRAL_API_KEY` (Mistral models)
- `OPENROUTER_API_KEY` (Multiple models)
- `XAI_API_KEY` (Grok models)

An API key is required for any provider used across any of the 3 roles defined in the `models` command.

### Model Configuration

```bash
# Interactive setup (recommended)
task-master models --setup

# Set specific models
task-master models --set-main claude-3-5-sonnet-20241022
task-master models --set-research perplexity-llama-3.1-sonar-large-128k-online
task-master models --set-fallback gpt-4o-mini
```

## Task Structure & IDs

### Task ID Format

- Main tasks: `1`, `2`, `3`, etc.
- Subtasks: `1.1`, `1.2`, `2.1`, etc.
- Sub-subtasks: `1.1.1`, `1.1.2`, etc.

### Task Status Values

- `pending` - Ready to work on
- `in-progress` - Currently being worked on
- `done` - Completed and verified
- `deferred` - Postponed
- `cancelled` - No longer needed
- `blocked` - Waiting on external factors

### Task Fields

```json
{
  "id": "1.2",
  "title": "Implement user authentication",
  "description": "Set up JWT-based auth system",
  "status": "pending",
  "priority": "high",
  "dependencies": ["1.1"],
  "details": "Use bcrypt for hashing, JWT for tokens...",
  "testStrategy": "Unit tests for auth functions, integration tests for login flow",
  "subtasks": []
}
```

## Claude Code Best Practices with Task Master

### Context Management

- Use `/clear` between different tasks to maintain focus
- This CLAUDE.md file is automatically loaded for context
- Use `task-master show <id>` to pull specific task context when needed

### Iterative Implementation

1. `task-master show <subtask-id>` - Understand requirements
2. Explore codebase and plan implementation
3. `task-master update-subtask --id=<id> --prompt="detailed plan"` - Log plan
4. `task-master set-status --id=<id> --status=in-progress` - Start work
5. Implement code following logged plan
6. `task-master update-subtask --id=<id> --prompt="what worked/didn't work"` - Log progress
7. `task-master set-status --id=<id> --status=done` - Complete task

### Complex Workflows with Checklists

For large migrations or multi-step processes:

1. Create a markdown PRD file describing the new changes: `touch task-migration-checklist.md` (prds can be .txt or .md)
2. Use Taskmaster to parse the new prd with `task-master parse-prd --append` (also available in MCP)
3. Use Taskmaster to expand the newly generated tasks into subtasks. Consdier using `analyze-complexity` with the correct --to and --from IDs (the new ids) to identify the ideal subtask amounts for each task. Then expand them.
4. Work through items systematically, checking them off as completed
5. Use `task-master update-subtask` to log progress on each task/subtask and/or updating/researching them before/during implementation if getting stuck

### Git Integration

Task Master works well with `gh` CLI:

```bash
# Create PR for completed task
gh pr create --title "Complete task 1.2: User authentication" --body "Implements JWT auth system as specified in task 1.2"

# Reference task in commits
git commit -m "feat: implement JWT auth (task 1.2)"
```

### Parallel Development with Git Worktrees

```bash
# Create worktrees for parallel task development
git worktree add ../project-auth feature/auth-system
git worktree add ../project-api feature/api-refactor

# Run Claude Code in each worktree
cd ../project-auth && claude    # Terminal 1: Auth work
cd ../project-api && claude     # Terminal 2: API work
```

## Troubleshooting

### AI Commands Failing

```bash
# Check API keys are configured
cat .env                           # For CLI usage

# Verify model configuration
task-master models

# Test with different model
task-master models --set-fallback gpt-4o-mini
```

### MCP Connection Issues

- Check `.mcp.json` configuration
- Verify Node.js installation
- Use `--mcp-debug` flag when starting Claude Code
- Use CLI as fallback if MCP unavailable

### Task File Sync Issues

```bash
# Regenerate task files from tasks.json
task-master generate

# Fix dependency issues
task-master fix-dependencies
```

DO NOT RE-INITIALIZE. That will not do anything beyond re-adding the same Taskmaster core files.

## Important Notes

### AI-Powered Operations

These commands make AI calls and may take up to a minute:

- `parse_prd` / `task-master parse-prd`
- `analyze_project_complexity` / `task-master analyze-complexity`
- `expand_task` / `task-master expand`
- `expand_all` / `task-master expand --all`
- `add_task` / `task-master add-task`
- `update` / `task-master update`
- `update_task` / `task-master update-task`
- `update_subtask` / `task-master update-subtask`

### File Management

- Never manually edit `tasks.json` - use commands instead
- Never manually edit `.taskmaster/config.json` - use `task-master models`
- Task markdown files in `tasks/` are auto-generated
- Run `task-master generate` after manual changes to tasks.json

### Claude Code Session Management

- Use `/clear` frequently to maintain focused context
- Create custom slash commands for repeated Task Master workflows
- Configure tool allowlist to streamline permissions
- Use headless mode for automation: `claude -p "task-master next"`

### Multi-Task Updates

- Use `update --from=<id>` to update multiple future tasks
- Use `update-task --id=<id>` for single task updates
- Use `update-subtask --id=<id>` for implementation logging

### Research Mode

- Add `--research` flag for research-based AI enhancement
- Requires a research model API key like Perplexity (`PERPLEXITY_API_KEY`) in environment
- Provides more informed task creation and updates
- Recommended for complex technical tasks

---

_This guide ensures Claude Code has immediate access to Task Master's essential functionality for agentic development workflows._
</file>

<file path=".taskmaster/config.json">
{
  "models": {
    "main": {
      "provider": "claude-code",
      "modelId": "opus",
      "maxTokens": 32000,
      "temperature": 0.2
    },
    "research": {
      "provider": "perplexity",
      "modelId": "sonar-pro",
      "maxTokens": 8700,
      "temperature": 0.1
    },
    "fallback": {
      "provider": "claude-code",
      "modelId": "opus",
      "maxTokens": 32000,
      "temperature": 0.2
    }
  },
  "global": {
    "logLevel": "info",
    "debug": false,
    "defaultNumTasks": 10,
    "defaultSubtasks": 5,
    "defaultPriority": "medium",
    "projectName": "Taskmaster",
    "ollamaBaseURL": "http://localhost:11434/api",
    "bedrockBaseURL": "https://bedrock.us-east-1.amazonaws.com",
    "responseLanguage": "English",
    "enableCodebaseAnalysis": true,
    "enableProxy": false,
    "anonymousTelemetry": true,
    "defaultTag": "master",
    "azureOpenaiBaseURL": "https://your-endpoint.openai.azure.com/",
    "userId": "1234567890"
  },
  "claudeCode": {},
  "codexCli": {},
  "grokCli": {
    "timeout": 120000,
    "workingDirectory": null,
    "defaultModel": "grok-4-latest"
  }
}
</file>

<file path=".taskmaster/state.json">
{
  "currentTag": "master",
  "lastSwitched": "2025-12-14T01:35:13.266Z",
  "branchTagMapping": {},
  "migrationNoticeShown": true
}
</file>

<file path="config/prompt_library/ansoff_matrix_analysis.md">
# Ansoff Matrix Analysis for Codebase

**Objective:** Analyze the codebase using the Ansoff Matrix framework to evaluate potential growth strategies and identify opportunities for expansion.

**Instructions:**

1. Review the codebase and identify the key features and functionalities it provides.
2. Analyze each of the four quadrants of the Ansoff Matrix in relation to the codebase:

   a. Market Penetration (Existing Products, Existing Markets):
      - How can the codebase increase its market share within its current market?
      - Are there opportunities for increasing usage or adoption among existing customers?

   b. Product Development (New Products, Existing Markets):
      - What new features or functionalities could be added to the codebase to better serve existing markets?
      - Are there opportunities for product line extensions or variations?

   c. Market Development (Existing Products, New Markets):
      - Are there new customer segments or geographic markets that the codebase could target?
      - How could the codebase be adapted or marketed to appeal to these new markets?

   d. Diversification (New Products, New Markets):
      - Are there opportunities for the codebase to expand into entirely new product areas and markets?
      - What synergies or advantages could the codebase leverage in pursuing diversification?

3. Evaluate the potential risks and rewards associated with each growth strategy.
4. Identify the most promising growth strategies for the codebase based on its current capabilities and market position.
5. Develop high-level recommendations for pursuing the identified growth opportunities.

**Expected Output:** A comprehensive Ansoff Matrix analysis of the codebase, including:
- Assessment of growth opportunities within each of the four quadrants
- Evaluation of risks and rewards associated with each growth strategy
- Identification of the most promising growth strategies for the codebase
- High-level recommendations for pursuing growth opportunities
</file>

<file path="config/prompt_library/architecture_api_client_code_generation.md">
## Generate API Client Code 

**Objective:** Generate client-side code in a specified programming language (e.g., JavaScript, Python) that can interact with an API, given the API specification or a codebase implementing the API. 

**Instructions:**

1. **Access the API definition:**  Obtain the API specification (e.g., OpenAPI/Swagger definition) or access the codebase that implements the API.
2. **Determine the target language:** Use the specified programming language for the generated client code.
3. **Generate the client code:**  
    * **API Methods:** Generate functions or methods for each API endpoint (e.g., `getUser(userId)`, `createOrder(orderData)`).
    * **Data Models:**  Create data structures or classes that represent request and response payloads for API interactions, matching the API specification. 
    * **Authentication:** If the API requires authentication, include necessary authentication mechanisms (e.g., API keys, OAuth) in the generated code. 
    * **Error Handling:** Implement robust error handling to handle different API response codes (e.g., 400 Bad Request, 500 Server Error) and provide helpful error messages. 
4. **Ensure code quality:** The generated code should be: 
    * **Readable and Well-Formatted:** Follow coding conventions and style guides for the target language.
    * **Well-Documented:**  Include comments to explain the purpose of different methods, classes, and how to use the client code.

**Expected Output:** Working API client code in the specified programming language that can:

* Authenticate with the API (if required). 
* Make requests to different API endpoints.
* Handle responses and errors gracefully.
* Be easily integrated into a larger project.
</file>

<file path="config/prompt_library/architecture_api_conformance_check.md">
## API Conformance Check

**Objective:** Given an API specification, analyze a codebase to identify any inconsistencies or deviations from the defined API contract.

**Instructions:**

1. **Obtain the API specification:**  Access the API definition in a suitable format (e.g., YAML, JSON).
2. **Map API endpoints to code:**  Identify the code modules or functions responsible for handling each API endpoint defined in the specification.
3. **Compare the API specification with the codebase implementation:**
    * **HTTP Methods:**  Verify that the implemented methods (GET, POST, PUT, DELETE, etc.) match those defined for each endpoint.
    * **Request Parameters:**  Check that the code handles all required and optional parameters as specified.
    * **Request and Response Bodies:**  Ensure that the data structures used in the code for request and response payloads match the data models defined in the specification. 
    * **Error Handling:**  Verify that the code returns the correct HTTP status codes and error responses for different scenarios, as defined in the specification. 
4. **Document inconsistencies:**
    * Clearly describe any discrepancies found between the API specification and the code implementation.
    * Provide specific code examples and line numbers where the deviations occur.
5. **Assess the severity of inconsistencies:** Determine if the inconsistencies are: 
    * Minor (e.g., differences in parameter naming)
    * Major (e.g., missing endpoints, different data structures).

**Expected Output:** A detailed report that:

1. Lists all identified inconsistencies between the API specification and the codebase.
2. Provides a clear description of each inconsistency, its location in the code, and its potential severity. 
3. Helps prioritize fixes by highlighting major conformance issues.
</file>

<file path="config/prompt_library/architecture_coupling_cohesion_analysis.md">
## Analyze Coupling and Cohesion

**Objective:** Evaluate the coupling and cohesion of modules or components within the codebase, identifying areas of high coupling or low cohesion that might indicate design flaws. 

**Instructions:**

1. **Analyze module dependencies:** Examine how different modules or components in the codebase depend on each other. Tools like dependency graphs can be helpful for visualization.
2. **Evaluate coupling:**
    * **Identify areas of high coupling:** Look for modules that depend heavily on many other modules or have complex, intertwined dependencies. 
    * **Explain the implications of high coupling:** For example, explain how high coupling can make modules harder to understand, test, and maintain independently. 
3. **Evaluate cohesion:**
    * **Identify areas of low cohesion:** Look for modules that contain unrelated functionalities or classes that don't seem to belong together logically. 
    * **Explain the implications of low cohesion:** For example, explain how low cohesion can make modules harder to understand and can lead to code that is more difficult to reuse. 
4. **Provide concrete examples:** Illustrate your findings with specific code examples from the codebase. Show instances of tight coupling, complex dependencies, or modules with low cohesion.
5. **Suggest potential improvements:**  Where applicable, suggest ways to refactor the code to reduce coupling and improve cohesion, such as:
    * Extracting shared functionality into separate modules. 
    * Applying design principles like the Single Responsibility Principle.

**Expected Output:**  A well-structured report that:

1. Provides an assessment of the codebase's overall coupling and cohesion.
2. Identifies specific areas of high coupling and low cohesion, supported by code examples.
3. Explains the potential negative consequences of these design issues.
4. Suggests actionable steps to improve the codebase's structure.
</file>

<file path="config/prompt_library/architecture_database_schema_documentation.md">
## Generate Database Schema Documentation

**Objective:** Create clear, comprehensive, and well-structured documentation for the provided database schema, including descriptions of tables, columns, relationships, indexes, and constraints. 

**Instructions:**

1. **Access the database schema:** Obtain the schema definition, including information about:
    * Tables: Names, primary keys, foreign keys, indexes, and any other constraints.
    * Columns: Names, data types, default values, constraints (e.g., NOT NULL, UNIQUE), and whether they are part of a primary or foreign key.
    * Relationships:  How tables are related to each other (one-to-one, one-to-many, many-to-many).
2. **Organize the documentation:**  Structure the documentation logically, for example:
    * By table: Provide a dedicated section for each table in the schema. 
    * By relationship: Group tables based on their relationships (e.g., orders and order items).
3. **Provide clear and concise descriptions:**  
    * **Table Descriptions:**  Explain the purpose of each table and the type of data it stores. 
    * **Column Descriptions:** Explain the purpose of each column, its data type, allowed values, and any relevant business rules.
    * **Relationship Descriptions:** Clearly describe how tables are related, including cardinality (one-to-one, etc.).  
4. **Consider using visuals:**  Incorporate diagrams (like Entity-Relationship Diagrams) to visually represent relationships between tables.

**Expected Output:**  Well-structured and informative database schema documentation that can be:

* Easily understood by both technical and non-technical audiences.
* Used for reference, onboarding new team members, or generating data dictionaries. 
* Output formats could be:
    * Markdown files (`.md`)
    * HTML pages 
    * Database schema documentation tools (e.g., SchemaSpy).
</file>

<file path="config/prompt_library/architecture_database_schema_review.md">
## Database Schema Review 

**Objective:** Review the database schema for normalization, indexing, and potential performance bottlenecks, providing recommendations for improvement based on best practices.

**Instructions:**

1. **Analyze the database schema:**  Obtain the schema definition (tables, columns, data types, relationships, keys, constraints, etc.) 
2. **Evaluate normalization levels:** Determine the normalization level of the schema (e.g., 1NF, 2NF, 3NF). Identify any tables that might benefit from further normalization to reduce data redundancy and improve data integrity.
3. **Assess indexing strategy:**
    * Analyze existing indexes and their effectiveness.
    * Identify columns or combinations of columns that are frequently used in query WHERE clauses or JOIN conditions and might benefit from indexing.
4. **Identify potential performance bottlenecks:** 
    * Look for large tables, complex queries, or inefficient data types that could negatively impact performance. 
    * Check for the appropriate use of primary and foreign keys for efficient data retrieval. 
5. **Suggest improvements and optimizations:**  Based on your analysis:
    * Recommend specific normalization steps if needed.
    * Suggest indexes to improve query performance. 
    * Recommend changes to data types or table structures to enhance efficiency.
    * Provide general recommendations for database optimization based on best practices. 

**Expected Output:** A comprehensive database schema review report that includes:

1. An assessment of normalization levels.
2. An evaluation of the indexing strategy.
3. Identification of potential performance bottlenecks.
4. Concrete and actionable recommendations for improving the schema design and performance.
</file>

<file path="config/prompt_library/architecture_design_pattern_identification.md">
## Identify Design Patterns

**Objective:** Analyze the codebase to identify and understand the implementation and purpose of common design patterns.

**Instructions:**

1. **Examine the codebase structure and logic:** Look for recurring code structures, relationships between classes, or ways that common software design problems are addressed.
2. **Identify design pattern instances:** Determine if any of the following design patterns (or others) are used:
    * **Creational Patterns:** Singleton, Factory, Abstract Factory, Builder, Prototype
    * **Structural Patterns:** Adapter, Bridge, Composite, Decorator, Facade, Flyweight, Proxy
    * **Behavioral Patterns:** Chain of Responsibility, Command, Interpreter, Iterator, Mediator, Memento, Observer, State, Strategy, Template Method, Visitor
3. **For each identified pattern:**
    * **Name the pattern.**
    * **Provide a brief description of the pattern** and its general purpose. 
    * **Explain the specific implementation details** within the codebase, including relevant classes, interfaces, and relationships.
    * **Explain the reasoning behind using the pattern** in the context of the codebase. What benefits does it provide? 

**Expected Output:** A structured analysis that:

1. Lists all identified design patterns found in the codebase.
2. Provides a clear description and explanation for each identified pattern instance.
3. Explains the reasoning and benefits of using each design pattern within the specific context of the codebase.
</file>

<file path="config/prompt_library/architecture_diagram_generation.md">
## Generate Architectural Diagram

**Objective:** Generate a clear and informative architectural diagram that visually represents the structure and components of the codebase, based on its actual structure and dependencies.

**Instructions:**

1. **Analyze the codebase:**  Examine the directory structure, modules, classes, and their relationships to understand the system's architecture. 
2. **Identify key components:** Determine the major building blocks of the system, such as:
    * User Interface components
    * APIs or services
    * Databases
    * External systems
    * Business logic modules
3. **Determine relationships:** Analyze how these components interact with each other. For example:
    * Which modules depend on others?
    * How do data flows between components?
    * What are the communication protocols used?
4. **Choose a suitable diagram type:** Select a diagram type that effectively represents the architecture. Common choices include:
    * Component diagrams
    * Layered architecture diagrams
    * Data flow diagrams 
5. **Generate the diagram:** Use a diagramming tool or library to create a visually appealing and informative diagram that includes:
    * Clearly labeled components
    * Well-defined relationships (e.g., arrows indicating data flow or dependencies)
    * A legend or key to explain symbols and notations

**Expected Output:** A visual representation of the codebase's architecture, either as an image file or in a text-based format that can be easily rendered (e.g., PlantUML, Mermaid). The diagram should be: 

* **Accurate:** It should correctly reflect the actual structure and dependencies in the codebase.
* **Clear and Concise:** Avoid clutter and use concise labels for easy understanding.
* **Informative:** The diagram should convey the key architectural elements and their interactions effectively.
</file>

<file path="config/prompt_library/architecture_layer_identification.md">
**Objective:** Analyze the codebase and identify different architectural layers (e.g., presentation, business logic, data access), highlighting inconsistencies or deviations from common architectural patterns.

**Instructions:**

1. **Analyze the codebase structure:** Examine the directory structure, modules, and classes to understand how code is organized.
2. **Identify distinct layers:** Look for code sections responsible for:
    * **Presentation:** Handling user interface, user input, and displaying information (e.g., UI components, views, controllers). 
    * **Business Logic:** Implementing business rules, workflows, and data processing (e.g., services, business objects, use case classes).
    * **Data Access:** Interacting with databases or external data sources (e.g., repositories, data access objects, API clients). 
3. **Document each identified layer:** 
    * Name the layer (e.g., "Presentation Layer", "Domain Layer", "Persistence Layer").
    * Describe its purpose and responsibilities.
    * List the key components or modules belonging to that layer.
4. **Analyze adherence to architectural patterns:** 
    * Determine if the codebase follows any recognizable architectural patterns (e.g., Model-View-Controller, Model-View-ViewModel, Layered Architecture).
    * Highlight any inconsistencies or deviations from these patterns. For example, if business logic is found within the presentation layer, explain the potential implications.
5. **Provide specific code examples:**  Illustrate your findings by referencing relevant code snippets that clearly demonstrate the separation (or lack thereof) between architectural layers.

**Expected Output:** A clear and well-structured report that:

1. Identifies the architectural layers present in the codebase.
2. Describes the purpose and responsibilities of each layer.
3. Provides concrete code examples to support the analysis.
4. Analyzes the codebase's adherence to common architectural patterns and highlights any inconsistencies or deviations.
</file>

<file path="config/prompt_library/architecture_refactoring_for_design_patterns.md">
## Suggest Refactoring for Design Patterns 

**Objective:** Analyze the codebase and identify opportunities to refactor code by implementing suitable design patterns to improve code maintainability, extensibility, or reusability.

**Instructions:**

1. **Analyze the codebase:**  Examine the existing code structure, identify areas that are complex, difficult to maintain, or lack flexibility.
2. **Identify potential design pattern applications:** Determine if any of the following situations exist:
    * **Code Smells:** Look for common code smells like "Large Class", "Long Method", "Shotgun Surgery" (many small changes across multiple classes), or "Divergent Change" (one class changing for multiple reasons) as indicators for refactoring. 
    * **Repetitive Code:** Identify duplicated code or logic that could be extracted and made reusable.
    * **Tight Coupling:** Look for areas where components are too tightly dependent on each other, making it difficult to change one without affecting others.
    * **Lack of Flexibility:** Identify code that is difficult to extend or modify to accommodate new requirements. 
3. **Suggest specific design patterns:**  Based on your analysis, recommend suitable design patterns to address the identified issues. For each suggestion:
    * **Name the pattern.** 
    * **Explain why the pattern is a good fit for the specific situation.**
    * **Describe how the pattern should be implemented:** Include specific details about which classes or modules should be created or modified.
    * **Illustrate with code examples (if possible):** Show how the code could be refactored using the suggested pattern. 

**Expected Output:** A report (or a series of suggestions) that:

1. Clearly identifies specific areas in the codebase that would benefit from refactoring.
2. Recommends appropriate design patterns to address each identified issue.
3. Provides detailed explanations and (ideally) code examples to guide the refactoring process.
</file>

<file path="config/prompt_library/bcg_matrix_analysis.md">
# BCG Growth-Share Matrix Analysis for Codebase

**Objective:** Analyze the codebase and its associated product portfolio using the BCG Growth-Share Matrix to assess resource allocation and identify growth opportunities.

**Instructions:**

1. Review the codebase and identify the key products or features it supports.
2. For each product or feature, analyze its position within the BCG Growth-Share Matrix:

   a. Relative Market Share:
      - How does the product's market share compare to that of its largest competitor?
      - Is the product a market leader or a follower?

   b. Market Growth Rate:
      - How fast is the market for this product growing?
      - Is the market in a high-growth or low-growth phase?

3. Based on the analysis, classify each product or feature into one of the four categories:

   a. Stars (High Market Share, High Market Growth):
      - These products are market leaders in high-growth markets.
      - They require significant investment to maintain their position.

   b. Cash Cows (High Market Share, Low Market Growth):
      - These products are market leaders in mature, slow-growth markets.
      - They generate significant cash flow that can be used to fund other products.

   c. Question Marks (Low Market Share, High Market Growth):
      - These products have low market share in high-growth markets.
      - They require investment to increase market share, with the potential to become stars.

   d. Dogs (Low Market Share, Low Market Growth):
      - These products have low market share in slow-growth markets.
      - They may generate little profit and consume resources that could be better used elsewhere.

4. Evaluate the overall balance and sustainability of the product portfolio.
5. Identify potential strategies for each product category (e.g., invest, maintain, harvest, divest).
6. Develop high-level recommendations for optimizing the product portfolio and allocating resources.

**Expected Output:** A comprehensive BCG Growth-Share Matrix analysis of the codebase and its associated product portfolio, including:
- Classification of each product or feature into the four categories (Stars, Cash Cows, Question Marks, Dogs)
- Evaluation of the overall balance and sustainability of the product portfolio
- Identification of potential strategies for each product category
- High-level recommendations for portfolio optimization and resource allocation
</file>

<file path="config/prompt_library/blue_ocean_strategy_analysis.md">
# Blue Ocean Strategy Analysis for Codebase

**Objective:** Analyze the codebase to identify opportunities for creating uncontested market space and generating new demand, based on the principles of Blue Ocean Strategy.

**Instructions:**

1. Review the codebase and identify key features and functionalities.
2. Analyze the industry landscape and existing solutions:
   - What are the common features and assumptions in the industry?
   - What factors do competitors typically compete on?
3. Identify opportunities for value innovation:
   - How can the codebase eliminate features that the industry takes for granted?
   - What features could be reduced well below the industry standard?
   - Which features could be raised well above the industry standard?
   - What features could be created that the industry has never offered?
4. Assess the potential for creating new market space:
   - How do the identified opportunities differentiate the codebase from existing solutions?
   - What new customer segments or needs could be addressed?
5. Evaluate the strategic fit and feasibility of pursuing these opportunities:
   - Are the opportunities aligned with the overall business strategy?
   - What are the potential challenges or risks in pursuing these opportunities?
6. Develop a high-level roadmap for implementing the identified blue ocean opportunities.

**Expected Output:** A comprehensive Blue Ocean Strategy analysis of the codebase, including:
- Identification of industry assumptions and factors of competition
- Opportunities for value innovation through the four actions framework (eliminate, reduce, raise, create)
- Assessment of the potential for creating uncontested market space
- Evaluation of strategic fit and feasibility
- High-level implementation roadmap
</file>

<file path="config/prompt_library/business_impact_analysis.md">
# Codebase Business Impact Analysis

**Objective:** Analyze the codebase to identify key features and their potential business impact.

**Instructions:**

1. Review the codebase and identify major features or functionalities.
2. For each feature, analyze its potential business impact:
   * Revenue generation potential
   * Cost-saving opportunities
   * Customer satisfaction improvements
   * Competitive advantage
3. Prioritize features based on their estimated business value.
4. Suggest potential enhancements or new features that could increase business value.

**Expected Output:** A report detailing the business impact of key features, their prioritization, and suggestions for value-adding enhancements.
</file>

<file path="config/prompt_library/business_model_canvas_analysis.md">
# Business Model Canvas Analysis for Codebase

**Objective:** Analyze the codebase using the Business Model Canvas framework to understand its business implications and potential.

**Instructions:**

1. Review the codebase and identify key features and functionalities.
2. For each element of the Business Model Canvas, analyze how the codebase contributes:

   a. Value Propositions:
      - What user problems does the code solve?
      - What value does it deliver to users?

   b. Customer Segments:
      - Who are the target users/customers for this code?
      - Are there different user groups with distinct needs?

   c. Channels:
      - How does the code facilitate reaching users?
      - Are there features for distribution or communication?

   d. Customer Relationships:
      - How does the code support user engagement and retention?
      - Are there features for customer support or community building?

   e. Revenue Streams:
      - How could this code generate revenue?
      - Are there monetization features implemented?

   f. Key Resources:
      - What are the critical technical assets in the codebase?
      - Are there unique algorithms or data structures?

   g. Key Activities:
      - What are the core functionalities that keep the application running?
      - What ongoing processes does the code support?

   h. Key Partnerships:
      - Does the code integrate with external services or APIs?
      - Are there dependencies on third-party libraries?

   i. Cost Structure:
      - What are the main cost drivers in running this code?
      - Are there features aimed at optimizing costs?

3. Identify potential gaps or opportunities in the business model based on the codebase analysis.
4. Suggest improvements or new features that could enhance the business model.

**Expected Output:** A comprehensive analysis of the codebase using the Business Model Canvas framework, highlighting how the code supports different aspects of the business model and identifying areas for improvement or expansion.
</file>

<file path="config/prompt_library/competitive_positioning_map.md">
# Competitive Positioning Map for Codebase

**Objective:** Create a visual map of the codebase's associated product and its competitors, positioning each based on key dimensions that are important to the target market.

**Instructions:**

1. Review the codebase and identify the key features and functionalities it provides.
2. Define the target market for the product and identify the main competitors.
3. Select two key dimensions that are important to the target market and can be used to differentiate the products. Examples might include:
   - Price vs. Quality
   - Ease of Use vs. Functionality
   - Performance vs. Customization
   - Brand Reputation vs. Innovation
4. Create a two-dimensional grid with the chosen dimensions as the axes.
5. Plot the codebase's associated product on the grid based on its relative position along the two dimensions.
6. Plot the main competitors on the grid based on their relative positions along the two dimensions.
7. Analyze the resulting map:
   - Where is the codebase's product positioned relative to competitors?
   - Are there any gaps or clusters in the market?
   - Is the codebase's product positioned in a way that aligns with its target market and value proposition?
8. Identify opportunities for differentiating or repositioning the codebase's product based on the competitive landscape.
9. Develop strategies for strengthening the product's position and competing effectively in the market.

**Expected Output:** A visual Competitive Positioning Map for the codebase's associated product, including:
- A two-dimensional grid with labeled axes representing the chosen key dimensions
- The codebase's product plotted on the grid based on its relative position
- Main competitors plotted on the grid based on their relative positions
- Analysis of the product's position relative to competitors and market gaps/clusters
- Identification of opportunities for differentiation and repositioning
- Strategies for strengthening the product's competitive position
</file>

<file path="config/prompt_library/customer_journey_map_analysis.md">
# Customer Journey Map Analysis for Codebase

**Objective:** Analyze the codebase to map out how different parts support various stages of the user's journey or experience.

**Instructions:**

1. Review the codebase and identify key features and functionalities.

2. Define the main stages of the customer journey that the code supports. Typical stages might include:
   - Awareness
   - Consideration
   - Decision
   - Onboarding
   - Usage
   - Support
   - Retention
   - Advocacy

3. For each stage of the journey:
   a. Identify the specific code components or features that support this stage.
   b. Analyze how these components contribute to the user's experience:
      - What actions can the user take?
      - What information or feedback does the user receive?
      - How does the code facilitate the user's progression to the next stage?

4. Evaluate the user's emotional state at each stage:
   - How might the user feel when interacting with this part of the code?
   - Are there potential pain points or moments of delight?

5. Identify touchpoints where the code interacts with the user:
   - User interface elements
   - Data input/output points
   - Notifications or alerts
   - Integration with external services or platforms

6. Analyze the flow and continuity between stages:
   - How smoothly does the code facilitate the user's transition between stages?
   - Are there any gaps or disconnects in the user's journey?

7. Identify opportunities for improvement:
   - Are there stages of the journey that lack adequate support in the code?
   - Could additional features enhance the user's experience at any stage?
   - Are there ways to streamline the journey or reduce friction points?

8. Consider personalization and adaptability:
   - Does the code account for different user personas or use cases?
   - How flexible is the implementation in supporting various journey paths?

**Expected Output:** A comprehensive analysis of how the codebase supports the customer journey, including:
- A mapping of code components to each stage of the journey
- Insights into the user's experience and emotional state at each stage
- Identified strengths and weaknesses in the current implementation
- Opportunities for enhancing the user journey through code improvements or new features

This analysis should provide a clear picture of how the code facilitates the user's experience from start to finish, and guide decisions for future development to optimize the customer journey.
</file>

<file path="config/prompt_library/evolution_code_churn_hotspot_analysis.md">
#### Code Churn Hotspot Analysis

**Objective:** Analyze the codebase's commit history to identify areas with high churn rates (frequent changes), which often indicate areas that are complex, error-prone, or in need of refactoring. 

**Instructions:**

1. **Access the Version Control System (VCS):**  Obtain the commit history from the codebase's version control system (e.g., Git).
2. **Calculate code churn:** For each file or code module, calculate the churn rate, which can be defined as:
    - Number of times the file has been modified
    - Number of lines of code added, deleted, or changed over a specific period. 
3. **Identify hotspots:**  Identify files or modules with significantly higher churn rates compared to the rest of the codebase. 
4. **Investigate the reasons for high churn:**  Try to understand why these hotspots are frequently modified. This might involve:
    -  Looking at commit messages associated with the changes.
    - Analyzing the types of changes made (bug fixes, new features, refactoring).
5. **Correlate with other metrics:** Combine churn analysis with other code quality metrics (e.g., complexity) to get a more comprehensive view of problematic areas. 

**Expected Output:**  A report or visualization (e.g., a heatmap) that highlights:

- Files or modules with the highest code churn rates.
- Trends in churn over time (e.g., increasing or decreasing churn).
-  Potential reasons for high churn based on commit history analysis.
- Recommendations for refactoring or further investigation.
</file>

<file path="config/prompt_library/evolution_code_evolution_report_generation.md">
**Objective:**  Create a comprehensive report summarizing the evolution of a codebase over time, using version control data and potentially other code quality metrics.

**Instructions:**

1. **Gather data from the VCS:** Access the codebase's version control system (e.g., Git) to retrieve historical data, including:
   - Commit history
   - Author information
   -  Dates and times of changes
   - Files changed in each commit
   - Lines of code added, deleted, or modified 
2. **Calculate relevant metrics:**  Compute metrics that provide insights into the codebase's evolution:
   - Code Churn: Measure the frequency of changes to different parts of the codebase. 
   - Code Complexity: Track the complexity of the code over time using metrics like cyclomatic complexity.
   - Contributor Activity: Analyze the number of contributors, their contributions over time, and the distribution of code ownership.
3. **Identify trends and patterns:**  Look for significant trends in the data, such as:
   -  Areas of the codebase with high churn.
   -  Increasing or decreasing code complexity.
   -  Changes in contributor activity. 
4. **Structure the report:**  Organize the report logically to present a clear narrative of the codebase's evolution, potentially including:
   -  An overall summary of key trends.
   -  Sections focused on specific metrics (churn, complexity, contributor activity).
   -  Visualizations (graphs, charts) to illustrate trends and patterns. 

**Expected Output:** A comprehensive report that:

-  Summarizes the codebase's evolution over time.
-  Provides insights into code churn, complexity trends, and contributor activity. 
-  Highlights important events or milestones in the codebase's history.
-  Uses visualizations to effectively communicate trends and patterns.
</file>

<file path="config/prompt_library/evolution_codebase_evolution_visualization.md">
**Objective:** Generate visualizations that effectively represent the evolution of a codebase over time, using data from version control and other relevant sources.  

**Instructions:**

1. **Determine the key aspects to visualize:**  Select the most relevant aspects of code evolution to display visually, such as:
    -  Code Churn
    -  Code Complexity
    -  Contributor Activity
    -  Growth in lines of code
    -  Evolution of directory structure
2. **Choose appropriate visualization types:**  Select chart types and visualization techniques that best represent the chosen data and insights, such as:
    -  Line charts for showing trends over time. 
    -  Heatmaps for highlighting areas of high churn or complexity.
    -  Treemaps for representing the evolution of directory structures or code modules.
    -  Network graphs for visualizing relationships between code entities.
3. **Use clear and informative labels:**  Provide meaningful titles, axis labels, and legends to make the visualizations easy to understand. 
4. **Focus on key insights:** Highlight the most important patterns or trends that the visualizations reveal about the codebase's evolution. 

**Expected Output:**  A set of clear and informative visualizations that:

-  Effectively communicate key aspects of the codebase's evolution.
-  Use appropriate visualization techniques to represent data and insights.
-  Include clear labels and annotations for easy understanding. 
-  Help identify important trends, patterns, or potential areas for improvement.
</file>

<file path="config/prompt_library/evolution_impact_analysis_of_code_changes.md">
**Objective:** Analyze the potential ripple effects of specific code changes (e.g., bug fixes, feature additions, refactoring) to identify areas of the codebase that might be affected and predict potential conflicts or regressions.

**Instructions:**

1. **Identify the code changes:**  Obtain a specific set of code changes, which could be:
    - A single commit
    - A pull request
    - A set of related modifications
2. **Analyze dependencies:**  Determine which modules, classes, or functions are directly or indirectly affected by the code changes by analyzing:
    -  Direct code dependencies (function calls, class inheritance, etc.).
    -  Shared data structures or global state that is modified by the changes.
3. **Predict potential impacts:**  Based on the analysis of dependencies:
    -  Identify areas where the changes might introduce bugs or regressions. 
    -  Look for potential conflicts with other parts of the codebase.
    -  Assess the risk level of the changes (e.g., low risk if changes are isolated, high risk if core components are affected).
4. **Suggest testing strategies:**  Recommend specific tests or test cases that should be run to validate the code changes and mitigate the risk of regressions.

**Expected Output:** An impact analysis report that:

-  Lists all potentially affected parts of the codebase.
-  Describes the nature of the potential impact (bugs, conflicts, performance regressions). 
-  Assesses the risk level of the code changes.
- Suggests targeted testing strategies to validate the changes and prevent regressions.
</file>

<file path="config/prompt_library/evolution_refactoring_recommendation_generation.md">
**Objective:** Provide specific refactoring recommendations based on the analysis of code evolution and historical data. Focus on areas where past code changes, churn, or technical debt indicate a need for improvement.

**Instructions:**

1. **Review code evolution analysis:**  Consider the results from code churn analysis, technical debt estimation, and other historical code assessments.
2. **Identify refactoring opportunities:** Look for areas where:
    - Code churn is consistently high, indicating potential design flaws or areas that are difficult to maintain.
    - Technical debt has accumulated, making the code hard to understand or modify.
    -  Architectural inconsistencies have emerged over time due to various changes and additions. 
3. **Suggest specific refactoring actions:** Provide concrete recommendations, such as:
    -  Extracting methods or classes to improve code organization and reduce complexity.
    -  Applying design patterns to improve flexibility and maintainability.
    -  Removing code smells or anti-patterns.
4. **Prioritize recommendations:**  Rank the suggestions based on:
    -  Their potential impact on code quality and maintainability.
    -  The feasibility and effort required to implement them. 

**Expected Output:** A prioritized list of refactoring recommendations that:

-  Clearly identifies the specific areas of the codebase that need improvement.
-  Provides specific refactoring actions to be taken (e.g., "Extract Method", "Apply Strategy Pattern").
-  Explains the rationale behind each recommendation and the expected benefits.
</file>

<file path="config/prompt_library/evolution_technical_debt_estimation.md">
**Objective:** Estimate the amount of technical debt present in the codebase based on historical code analysis, focusing on identifying areas where past code evolution has led to accumulated technical debt.

**Instructions:** 

1. **Analyze commit history:**  Examine commit messages, code changes, and refactoring patterns over time to identify potential sources of technical debt, such as:
    -  Hasty bug fixes or workarounds that weren't properly addressed.
    -  Lack of consistent coding standards or code reviews, leading to inconsistent code style and potential issues.
    -  Postponed refactoring or architectural improvements. 
2. **Identify code quality indicators:** Look for signs of technical debt based on historical changes, such as:
    -  Increased code complexity over time.
    -  High code churn in specific areas, indicating frequent rework or fixes.
    -  Presence of code smells or anti-patterns that have accumulated over time.
3. **Estimate the impact:**  Assess the potential consequences of the identified technical debt:
    -  Increased maintenance effort and costs. 
    -  Reduced development velocity due to difficult-to-understand or modify code.
    - Increased risk of bugs or regressions.
4. **Prioritize areas for refactoring:**  Rank areas with high technical debt based on their potential impact and the feasibility of addressing them. 

**Expected Output:** A technical debt report that provides:

- An overview of the estimated technical debt in the codebase.
- Identification of specific areas with high technical debt, supported by evidence from the code's history.
-  An assessment of the potential impact of the technical debt.
-  A prioritized list of recommendations for addressing the technical debt through refactoring or code improvements.
</file>

<file path="config/prompt_library/improvement_best_practice_analysis.md">
# Codebase Best Practice Analysis

**Objective:** Analyze the provided codebase and identify examples of both good and bad programming practices.

**Instructions:**

1. **Carefully review the attached code** and pinpoint instances of exemplary and problematic coding practices.
2. **For each example, provide a detailed analysis** that includes:
    * **What is good/bad about the specific solution?**
    * **What concepts or principles underpin the solution?**
    * **What are the potential positive/negative consequences of using this solution?**

**Expected Output:** A comprehensive report highlighting both positive and negative coding practices within the codebase, with in-depth explanations and analysis of their impact.
</file>

<file path="config/prompt_library/improvement_language_translation.md">
# Codebase Translation to Another Programming Language

**Objective:**  Translate the provided codebase from [Source Language] to [Target Language] while preserving its functionality and structure.

**Instructions:**

1. **Analyze the attached code** written in [Source Language] and understand its logic and functionalities.
2. **Translate the code** into [Target Language], ensuring that the translated code performs the same tasks as the original code.
3. **Maintain the original code's structure and organization** as much as possible in the translated version. 
4. **Adhere to the coding conventions and best practices** of the target language.
5. **Comment the translated code** to explain any significant changes or adaptations made during the translation process.

**Expected Output:** A functional codebase in [Target Language] that accurately reflects the functionality and structure of the original [Source Language] codebase.
</file>

<file path="config/prompt_library/improvement_refactoring.md">
# Codebase Refactoring for Improved Readability and Performance

**Objective:** Refactor the provided codebase to enhance its readability, maintainability, and performance.

**Instructions:**

1. **Analyze the attached code** and identify areas that can be improved in terms of code clarity, structure, and efficiency.
2. **Suggest specific code transformations and optimizations** to address the identified areas for improvement.
3. **Prioritize refactoring techniques** that improve code readability without introducing unnecessary complexity.
4. **Consider performance implications** of your suggested refactoring and aim for solutions that enhance efficiency without sacrificing clarity. 
5. **Provide clear explanations** for each refactoring suggestion, justifying its benefits and potential impact.

**Expected Output:**  A set of actionable refactoring suggestions with detailed explanations of their benefits and potential impact on code quality and performance.
</file>

<file path="config/prompt_library/jobs_to_be_done_analysis.md">
# Jobs to be Done (JTBD) Analysis for Codebase

**Objective:** Analyze the codebase using the Jobs to be Done framework to understand the core user needs it addresses and identify potential improvements or new opportunities.

**Instructions:**

1. Review the codebase and identify key features and functionalities.

2. For each major feature or component, analyze:

   a. Functional Job:
      - What practical task does this code help the user accomplish?
      - How does it make a process more efficient or effective?

   b. Emotional Job:
      - How does this code make the user feel?
      - Does it reduce anxiety, increase confidence, or provide a sense of accomplishment?

   c. Social Job:
      - How does this code help the user in their social or professional context?
      - Does it improve status, strengthen relationships, or enhance reputation?

3. Identify the circumstances that trigger the use of each feature:
   - What situation prompts the user to engage with this functionality?
   - What are the user's goals and constraints in this moment?

4. Analyze the current solutions and workarounds:
   - How are users currently addressing these jobs without your code?
   - What are the limitations or frustrations with existing solutions?

5. Evaluate how well the code fulfills each identified job:
   - Does it fully satisfy the user's needs?
   - Are there aspects of the job that are not adequately addressed?

6. Identify potential improvements or new opportunities:
   - How could existing features be enhanced to better fulfill the jobs?
   - Are there unaddressed jobs that present new development opportunities?

7. Prioritize potential enhancements based on their importance to users and alignment with business goals.

**Expected Output:** A comprehensive analysis of the codebase through the Jobs to be Done lens, highlighting how well it addresses user needs, identifying gaps, and suggesting prioritized improvements or new features to better fulfill user jobs.
</file>

<file path="config/prompt_library/kano_model_analysis.md">
# Kano Model Analysis for Codebase

**Objective:** Analyze the codebase and its associated product features using the Kano Model to prioritize development efforts based on their impact on customer satisfaction.

**Instructions:**

1. Review the codebase and identify the key features and functionalities it provides.
2. For each feature, analyze its potential impact on customer satisfaction using the Kano Model categories:

   a. Must-be (Basic) Features:
      - These are features that customers expect and take for granted.
      - Their absence leads to extreme dissatisfaction, but their presence does not increase satisfaction.

   b. One-dimensional (Performance) Features:
      - These features result in satisfaction when fulfilled and dissatisfaction when not fulfilled.
      - The level of satisfaction is proportional to the level of fulfillment.

   c. Attractive (Excitement) Features:
      - These features are not expected by customers but can lead to high satisfaction if present.
      - Their absence does not lead to dissatisfaction.

   d. Indifferent Features:
      - These features do not have a significant impact on customer satisfaction, whether present or absent.

   e. Reverse Features:
      - These features lead to dissatisfaction when present and satisfaction when absent.

3. Prioritize features based on their Kano Model classification and their alignment with business objectives.
4. Identify potential new features that could serve as Attractive (Excitement) features to delight customers.
5. Develop a roadmap for implementing and improving features based on their Kano Model prioritization.

**Expected Output:** A comprehensive Kano Model analysis of the codebase and its associated features, including:
- Classification of each feature into the Kano Model categories (Must-be, One-dimensional, Attractive, Indifferent, Reverse)
- Prioritization of features based on their Kano Model classification and business alignment
- Identification of potential new Attractive (Excitement) features
- Roadmap for feature implementation and improvement based on Kano Model prioritization
</file>

<file path="config/prompt_library/lean_canvas_analysis.md">
# Lean Canvas Analysis for Codebase

**Objective:** Analyze the codebase using the Lean Canvas framework to evaluate its business potential and identify areas for improvement or pivot.

**Instructions:**

1. Review the codebase and identify key features and functionalities.

2. For each element of the Lean Canvas, analyze how the codebase contributes:

   a. Problem:
      - What user problems does the code address?
      - Are there existing alternatives or workarounds?

   b. Solution:
      - How does the code solve the identified problems?
      - What are the key features that address user needs?

   c. Unique Value Proposition:
      - What makes this code unique or innovative?
      - How does it differ from existing solutions?

   d. Unfair Advantage:
      - Does the code leverage any unique technologies or algorithms?
      - Are there any hard-to-replicate aspects of the implementation?

   e. Customer Segments:
      - Who are the target users for this code?
      - Are there different user groups with distinct needs?

   f. Key Metrics:
      - What metrics could be used to measure the success of this code?
      - Are there features for tracking user engagement or performance?

   g. Channels:
      - How could this code reach its target users?
      - Are there built-in distribution or marketing features?

   h. Cost Structure:
      - What are the main cost drivers in developing and maintaining this code?
      - Are there features aimed at optimizing costs?

   i. Revenue Streams:
      - How could this code generate revenue?
      - Are there monetization features implemented?

3. Identify potential gaps or inconsistencies in the Lean Canvas based on the codebase analysis.

4. Suggest improvements or new features that could enhance the overall business model.

5. Consider potential pivot opportunities if the current approach doesn't align well with the Lean Canvas analysis.

**Expected Output:** A comprehensive analysis of the codebase using the Lean Canvas framework, highlighting how the code supports different aspects of a lean business model, identifying areas for improvement, and suggesting potential pivots or new directions for development.
</file>

<file path="config/prompt_library/learning_algorithmic_storytelling.md">
# Algorithmic Storytelling

**Objective:** Generate engaging narratives that explain the logic and flow of key algorithms in the codebase, making them more accessible and memorable for learners.

**Instructions:**
1. Identify the most important or complex algorithms in the codebase.
2. For each algorithm, break down its steps and key decision points.
3. Create a narrative that personifies the algorithm, using characters, dialogue, and metaphors to represent its logic and flow.
4. Ensure the story accurately reflects the algorithm's behavior while being engaging and easy to follow.
5. Provide code snippets to illustrate how the story maps to the actual implementation.

**Expected Output:** A collection of algorithmic stories that make the codebase's key algorithms more understandable and memorable, along with accompanying code snippets.
</file>

<file path="config/prompt_library/learning_backend_api_documentation.md">
# Backend API Documentation

**Objective:** Generate documentation for backend APIs, including endpoints, request/response formats, and authentication requirements.

**Instructions:**
1. Identify the main backend API endpoints in the codebase.
2. For each endpoint, document its HTTP method, URL path, and purpose.
3. Specify the expected request format, including headers, query parameters, and request body (if applicable).
4. Describe the response format, including the structure of the response data and any relevant status codes.
5. Detail any authentication or authorization requirements for accessing the endpoint.
6. Provide clear and concise code examples demonstrating how to interact with the API using popular tools or libraries.

**Expected Output:** Comprehensive documentation for the backend APIs, including endpoints, request/response formats, authentication requirements, and code examples, ready to be integrated into the project's documentation system.
</file>

<file path="config/prompt_library/learning_backend_code_analysis.md">
# Backend Code Analysis

**Objective:** Analyze the backend codebase to identify best practices, potential improvements, and common pitfalls.

**Instructions:**
1. Review the backend codebase, focusing on the architecture, design patterns, and libraries used.
2. Identify areas where best practices are followed and highlight them as examples of good code.
3. Spot potential improvements in code organization, performance, scalability, or maintainability, and provide specific suggestions.
4. Look for common pitfalls or anti-patterns in the backend code and offer alternative approaches.
5. Provide code snippets to illustrate your findings and recommendations.

**Expected Output:** A detailed report on the backend codebase, including best practices, potential improvements, and common pitfalls, along with illustrative code snippets and recommendations.
</file>

<file path="config/prompt_library/learning_code_analogies_metaphors.md">
# Code-Inspired Analogies and Metaphors

**Objective:** Generate analogies and metaphors inspired by the codebase to help explain complex technical concepts to non-technical stakeholders.

**Instructions:**
1. Identify complex technical concepts, architectures, or design patterns in the codebase that may be difficult for non-technical stakeholders to understand.
2. For each concept, brainstorm analogies or metaphors that map the technical ideas to more familiar, real-world concepts.
3. Ensure the analogies and metaphors accurately capture the essential aspects of the technical concepts while being easy to grasp and remember.
4. Provide clear explanations of how the analogies or metaphors relate to the actual code, using snippets or diagrams where appropriate.
5. Test the effectiveness of the analogies and metaphors by presenting them to non-technical stakeholders and gathering feedback.

**Expected Output:** A collection of code-inspired analogies and metaphors that can be used to bridge the gap between technical and non-technical understanding of the codebase.
</file>

<file path="config/prompt_library/learning_code_evolution_visualization.md">
# Code Evolution Visualization

**Objective:** Create visualizations that illustrate how the codebase has evolved over time, highlighting key milestones, refactorings, and architectural changes.

**Instructions:**
1. Analyze the version control history of the codebase to identify significant changes and milestones.
2. Design a timeline visualization that showcases the major events in the codebase's evolution, such as feature additions, refactorings, and architectural shifts.
3. Use color-coding, annotations, and icons to differentiate between different types of changes and their impact on the codebase.
4. Provide brief descriptions or links to relevant documentation for each milestone to give context and facilitate further exploration.
5. Consider creating interactive visualizations that allow users to zoom in on specific time periods or filter changes by type or author.

**Expected Output:** A set of engaging and informative visualizations that help developers understand the historical context of the codebase and appreciate the work that has gone into its development.
</file>

<file path="config/prompt_library/learning_code_pattern_recognition.md">
# Code Pattern Recognition and Explanation

**Objective:** Identify and explain design patterns, architectural patterns, and common coding idioms used in the codebase to facilitate understanding and knowledge sharing.

**Instructions:**
1. Analyze the codebase to identify instances of design patterns, architectural patterns, and coding idioms.
2. For each identified pattern or idiom, provide a clear explanation of its purpose, benefits, and trade-offs.
3. Use code snippets from the codebase to illustrate how the pattern or idiom is implemented in practice.
4. Discuss how the use of these patterns and idioms contributes to the overall quality, maintainability, and performance of the codebase.
5. Suggest potential improvements or alternative approaches where applicable.

**Expected Output:** A comprehensive report documenting the design patterns, architectural patterns, and coding idioms used in the codebase, along with explanations and illustrative code snippets.
</file>

<file path="config/prompt_library/learning_code_refactoring_exercises.md">
# Code Refactoring Exercises

**Objective:** Generate code refactoring exercises based on the codebase to help engineers improve their refactoring skills.

**Instructions:**
1. Identify areas in the codebase that could benefit from refactoring, such as duplicated code, long functions, or complex conditional statements.
2. For each identified area, create a refactoring exercise that challenges engineers to improve the code's design, readability, and maintainability.
3. Provide a clear problem statement, including the current code snippet and the desired outcomes of the refactoring exercise.
4. Include any necessary constraints or guidelines for the refactoring process.
5. Develop a set of test cases to help engineers validate the correctness of their refactored code.

**Expected Output:** A collection of code refactoring exercises, each with a problem statement, code snippet, desired outcomes, constraints, and test cases, suitable for use in training or skill development sessions.
</file>

<file path="config/prompt_library/learning_code_review_checklist.md">
# Code Review Checklist Generation

**Objective:** Generate a checklist of important points to consider during code reviews, based on the codebase's specific requirements and best practices.

**Instructions:**
1. Review the codebase and identify the key aspects that should be considered during code reviews, such as code style, performance, security, and maintainability.
2. Organize the identified aspects into categories to create a structured checklist.
3. For each category, provide specific questions or points that reviewers should consider when evaluating the code.
4. Include references to the codebase's specific conventions, guidelines, or best practices, where applicable.
5. Ensure the checklist is comprehensive yet concise, and can be easily integrated into the team's code review process.

**Expected Output:** A well-structured, codebase-specific code review checklist that helps reviewers consistently evaluate code quality, adherence to best practices, and potential issues.
</file>

<file path="config/prompt_library/learning_code_style_readability_analysis.md">
# Code Style and Readability Analysis

**Objective:** Evaluate the codebase's overall style and readability, providing suggestions for improvement.

**Instructions:**
1. Review the codebase and assess its adherence to a consistent code style and naming conventions.
2. Identify areas where the code's readability could be improved, such as through better variable names, comments, or function organization.
3. Provide specific suggestions for enhancing the code's style and readability, following industry best practices and standards.
4. Use code snippets to demonstrate your recommendations and show "before" and "after" examples.

**Expected Output:** A report on the codebase's code style and readability, including specific suggestions for improvement and illustrative code snippets.
</file>

<file path="config/prompt_library/learning_codebase_trivia_game.md">
# Codebase Trivia Game Generation

**Objective:** Generate trivia questions and answers based on the codebase to gamify learning and encourage team engagement.

**Instructions:**
1. Analyze the codebase to identify interesting facts, quirks, and historical details that could serve as the basis for trivia questions.
2. Create a diverse set of multiple-choice questions covering various aspects of the codebase, such as its architecture, design patterns, performance optimizations, and notable bug fixes.
3. Ensure the questions are challenging but not overly obscure, striking a balance between testing knowledge and promoting learning.
4. Provide clear and concise explanations for each answer to reinforce understanding and share knowledge.
5. Organize the questions into categories or difficulty levels to cater to different levels of expertise.

**Expected Output:** A collection of codebase-specific trivia questions and answers that can be used to create engaging learning games for the development team.
</file>

<file path="config/prompt_library/learning_frontend_code_analysis.md">
# Frontend Code Analysis

**Objective:** Analyze the frontend codebase to identify best practices, potential improvements, and common pitfalls.

**Instructions:**
1. Review the frontend codebase, focusing on the structure, design patterns, and libraries used.
2. Identify areas where best practices are followed and highlight them as examples of good code.
3. Spot potential improvements in code organization, performance, or maintainability, and provide specific suggestions.
4. Look for common pitfalls or anti-patterns in the frontend code and offer alternative approaches.
5. Provide code snippets to illustrate your findings and recommendations.

**Expected Output:** A detailed report on the frontend codebase, including best practices, potential improvements, and common pitfalls, along with illustrative code snippets and recommendations.
</file>

<file path="config/prompt_library/learning_frontend_component_documentation.md">
# Frontend Component Documentation

**Objective:** Generate documentation for frontend components, including props, usage examples, and best practices.

**Instructions:**
1. Identify the main frontend components in the codebase.
2. For each component, document its purpose, props (including their types and default values), and any relevant usage notes.
3. Provide clear and concise code examples demonstrating how to use the component in various scenarios.
4. Include best practices and tips for effectively utilizing and customizing the component.
5. Ensure the documentation is well-organized, easy to navigate, and follows a consistent format.

**Expected Output:** Comprehensive documentation for the frontend components, including props, usage examples, and best practices, ready to be integrated into the project's documentation system.
</file>

<file path="config/prompt_library/learning_mini_lesson_generation.md">
# Code-Based Mini-Lesson Generation

**Objective:** Create a comprehensive series of mini-lessons that explain the key concepts implemented within the provided codebase, suitable for developers of varying skill levels.

**Instructions:**

1. **Analyze and categorize the codebase:**
   * Identify major components (e.g., frontend, backend, data processing)
   * List key technologies and frameworks used

2. **Structure the mini-lessons:**
   * Create a lesson plan with 5-10 lessons
   * Ensure a logical progression from basic to advanced concepts
   * Aim for lessons that can be completed in 15-30 minutes each

3. **For each mini-lesson:**
   * **Title:** Provide a clear, concise title
   * **Learning Objectives:** List 2-3 specific learning outcomes
   * **Introduction:** Brief overview of the concept (2-3 sentences)
   * **Main Content:**
     - Explain the concept in detail
     - Use code examples from the application (minimum 2 per lesson)
     - Highlight best practices and potential pitfalls
   * **Hands-on Exercise:** Design a practical task related to the lesson
   * **Quiz:** Include 3-5 multiple-choice questions to test understanding
   * **Additional Resources:** Provide links to relevant documentation or articles

4. **Incorporate real-world context:**
   * Explain how each concept fits into the overall application architecture
   * Discuss practical applications beyond the current codebase

5. **Ensure accessibility and engagement:**
   * Use clear, jargon-free language (or explain technical terms)
   * Include diagrams or flowcharts where appropriate
   * Suggest points for class discussion or peer programming exercises

**Expected Output:**

1. **Lesson Plan Overview:**
   * List of lessons with titles and brief descriptions
   * Estimated time for each lesson

2. **Individual Lessons:**
   * Structured as per the instructions above
   * Clearly formatted for easy reading (use markdown for code snippets)

3. **Supplementary Materials:**
   * Any additional diagrams, cheat sheets, or reference guides
   * Suggestions for further learning or advanced topics

4. **Instructor Notes:**
   * Tips for effectively delivering each lesson
   * Potential areas where students might struggle and how to address them

**Additional Guidelines:**

* Tailor explanations to an audience with basic programming knowledge but potentially unfamiliar with specific technologies used
* Encourage critical thinking and problem-solving rather than rote memorization
* Where possible, highlight connections between different parts of the codebase
* Consider including optional 'deep dive' sections for more advanced learners

**Deliverable Format:** Provide the lesson plan and all lessons in a single markdown document, with clear section dividers and a table of contents.
</file>

<file path="config/prompt_library/learning_personal_development_recommendations.md">
# Personal Development Recommendations

**Objective:** Analyze the codebase and provide personalized recommendations for areas where the engineer can improve their skills.

**Instructions:**
1. Review the codebase and identify areas where the engineer's work is present.
2. Assess the engineer's code contributions in terms of design, implementation, and best practices.
3. Identify areas where the engineer excels and areas where there is room for improvement.
4. Provide specific, actionable recommendations for how the engineer can enhance their skills in the identified areas.
5. Suggest relevant resources, such as tutorials, books, or courses, that can support the engineer's learning journey.

**Expected Output:** A personalized report for the engineer, outlining their strengths and areas for improvement, along with specific recommendations and resources for skill development.
</file>

<file path="config/prompt_library/learning_socratic_dialogue_code_review.md">
# Socratic Dialogue Generation for Code Review

**Objective:** Generate Socratic-style dialogues that explore the reasoning behind code design decisions and encourage critical thinking during code reviews.

**Instructions:**
1. Select a code snippet or module that has recently undergone a code review.
2. Generate a dialogue between two fictional characters: a curious developer and an experienced mentor.
3. Have the curious developer ask probing questions about the code's design, performance, readability, and maintainability.
4. Let the experienced mentor provide thoughtful responses that explain the rationale behind the code's design and implementation.
5. Encourage the characters to explore alternative approaches and discuss their pros and cons.
6. Ensure the dialogue remains respectful, constructive, and focused on learning and improvement.

**Expected Output:** A series of Socratic dialogues that stimulate critical thinking and encourage developers to reflect on code design decisions during the code review process.
</file>

<file path="config/prompt_library/learning_user_story_reconstruction.md">
# Revised Prompt: Structured User Story Analysis for Software Development

## Objective
Reconstruct and structure the user stories that likely served as the basis for the development of the provided codebase, and identify potential enhancements.

## Instructions

1. Analyze the attached code to identify the core functionalities of the application.
2. Infer the user needs that each functionality aims to address.
3. Categorize the functionalities into key areas (e.g., Core Features, User Interface, Data Processing, etc.).
4. For each category, formulate user stories using the following enhanced template:
   ```
   User Role: [Specific role or type of user]
   Need: [What the user wants to accomplish]
   Functionality: [The feature or capability that addresses the need]
   Benefit: [The value or advantage gained by the user]
   ```
5. Identify potential missing user stories, categorizing them similarly.
6. Suggest functionalities that could be added to the application to better meet user needs.

## Expected Output

1. **Reconstructed User Stories**
   * Organized by categories (e.g., Core Features, User Interface, etc.)
   * Each category should contain:
     - A brief description of the category
     - A numbered list of user stories in the enhanced template format

2. **Potential Missing User Stories**
   * Organized by categories (similar to reconstructed stories)
   * Each category should contain:
     - A brief description of why these stories might be valuable
     - A numbered list of potential user stories in the enhanced template format

3. **Enhancement Suggestions**
   * A brief overview of key areas for potential improvement
   * A prioritized list of suggested new functionalities or features, with a brief rationale for each

## Additional Guidelines

* Aim for clarity and specificity in user roles and needs.
* Ensure that each user story is distinct and non-redundant.
* Consider various stakeholders (e.g., end-users, administrators, developers) when formulating stories.
* For missing stories and enhancements, focus on additions that would significantly improve the application's value or usability.
</file>

<file path="config/prompt_library/mckinsey_7s_analysis.md">
# McKinsey 7S Framework Analysis for Codebase

**Objective:** Analyze the codebase and its associated product using the McKinsey 7S Framework to evaluate the internal factors that contribute to organizational effectiveness and identify areas for improvement.

**Instructions:**

1. Review the codebase and identify the key features and functionalities it provides.
2. Analyze each of the seven elements of the McKinsey 7S Framework in relation to the codebase and its product:

   a. Strategy:
      - What is the overall strategy and objectives for the product?
      - How does the codebase support and align with this strategy?

   b. Structure:
      - How is the team or organization responsible for the codebase structured?
      - Does the structure support effective development, maintenance, and growth of the product?

   c. Systems:
      - What processes, tools, and technologies are used in the development and management of the codebase?
      - Are these systems efficient, reliable, and scalable?

   d. Shared Values:
      - What are the core values, culture, and ethical standards that guide the team or organization responsible for the codebase?
      - How are these values reflected in the codebase and the product?

   e. Style:
      - What is the leadership and management style within the team or organization responsible for the codebase?
      - How does this style impact the development and success of the product?

   f. Staff:
      - What are the skills, capabilities, and experience of the team members working on the codebase?
      - Are there any gaps or areas for improvement in the team's expertise?

   g. Skills:
      - What are the key technical and non-technical skills required for the successful development and maintenance of the codebase?
      - How are these skills developed and maintained within the team?

3. Evaluate the alignment and consistency among the seven elements of the framework.
4. Identify areas of strength and weakness based on the analysis.
5. Develop recommendations for improving the internal factors to enhance the effectiveness of the team and the success of the product.

**Expected Output:** A comprehensive McKinsey 7S Framework analysis of the codebase and its associated product, including:
- Analysis of each of the seven elements and their impact on the product
- Evaluation of the alignment and consistency among the elements
- Identification of areas of strength and weakness
- Recommendations for improving internal factors to enhance team effectiveness and product success
</file>

<file path="config/prompt_library/meta_triage.md">
# Codebase Triage and Prompt Selection

**Objective:** Analyze the codebase against its PRD and determine which analysis prompts should be run next to improve the code.

**Context:** You are a senior software architect reviewing a codebase. Your job is to:
1. Understand the current state of the codebase
2. Compare it against the Product Requirements Document (PRD)
3. Identify the most pressing issues or gaps
4. Select which analysis prompt(s) should be run next

**Available Prompts:**

Architecture:
- `architecture_layer_identification` - Identify architectural layers and patterns
- `architecture_design_pattern_identification` - Find design patterns in use
- `architecture_coupling_cohesion_analysis` - Analyze coupling and cohesion
- `architecture_api_conformance_check` - Check API design standards
- `architecture_database_schema_review` - Review database schema

Quality:
- `quality_error_analysis` - Find errors and inconsistencies
- `quality_code_complexity_analysis` - Analyze code complexity
- `quality_code_duplication_analysis` - Find duplicated code
- `quality_code_style_consistency_analysis` - Check code style consistency
- `quality_risk_assessment` - Assess codebase risks

Performance:
- `performance_bottleneck_identification` - Find performance bottlenecks
- `performance_scalability_analysis` - Analyze scalability concerns
- `performance_code_optimization_suggestions` - Suggest optimizations

Security:
- `security_vulnerability_analysis` - Find security vulnerabilities

Testing:
- `testing_unit_test_generation` - Generate unit test suggestions

Evolution:
- `evolution_technical_debt_estimation` - Estimate technical debt
- `evolution_refactoring_recommendation_generation` - Suggest refactoring

Improvement:
- `improvement_refactoring` - Suggest code improvements
- `improvement_best_practice_analysis` - Check best practices

**Instructions:**

1. Review the codebase structure and contents provided
2. Review the PRD requirements
3. Identify the most critical gaps or issues that need addressing
4. Select 1-3 prompts that would be most valuable to run next
5. If the codebase is in good shape and meets PRD requirements, respond with "DONE"

**Expected Output:** Respond with JSON in this exact format:

```json
{
  "assessment": "Brief 2-3 sentence assessment of current codebase state",
  "priority_issues": ["Issue 1", "Issue 2"],
  "selected_prompts": ["prompt_id_1", "prompt_id_2"],
  "reasoning": "Why these prompts were selected",
  "done": false
}
```

If the codebase is complete and meets requirements:
```json
{
  "assessment": "The codebase meets PRD requirements and is production-ready",
  "priority_issues": [],
  "selected_prompts": [],
  "reasoning": "No further analysis needed",
  "done": true
}
```
</file>

<file path="config/prompt_library/okr_analysis.md">
# OKR (Objectives and Key Results) Analysis for Codebase

**Objective:** Analyze the codebase to align its features and capabilities with potential business Objectives and Key Results (OKRs).

**Instructions:**

1. Review the codebase and identify key features, functionalities, and architectural decisions.

2. Based on the codebase analysis, infer potential high-level business objectives that the code might support. For each objective:
   a. Formulate a clear, ambitious yet achievable objective statement.
   b. Identify 3-5 key results that would indicate progress towards this objective.
   c. Explain how specific aspects of the code contribute to these key results.

3. Consider the following categories of objectives:
   - User Acquisition and Growth
   - User Engagement and Retention
   - Revenue Generation
   - Operational Efficiency
   - Product Innovation
   - Market Expansion

4. For each set of OKRs:
   a. Evaluate how well the current codebase supports achieving the key results.
   b. Identify any gaps or limitations in the code that might hinder achieving the OKRs.
   c. Suggest potential code improvements or new features that could better support the OKRs.

5. Analyze the overall alignment of the codebase with the inferred OKRs:
   - Are there features that don't clearly contribute to any of the key results?
   - Are there important OKRs that lack sufficient support in the current codebase?

6. Propose a prioritized list of development initiatives that would improve the codebase's alignment with the most critical OKRs.

**Expected Output:** A comprehensive analysis of how the codebase aligns with potential business OKRs, including:
- A set of inferred Objectives and Key Results based on the codebase capabilities
- An evaluation of how well the code supports each OKR
- Identified gaps and limitations in the current implementation
- Prioritized suggestions for code improvements or new features to better support critical OKRs

This analysis should provide insights into how the technical implementation aligns with potential business goals and guide future development priorities.
</file>

<file path="config/prompt_library/performance_bottleneck_identification.md">
#### Identify Performance Bottlenecks

**Objective:** Analyze the codebase to pinpoint specific areas that negatively impact performance, such as inefficient algorithms, excessive database queries, or slow network requests.

**Instructions:**

1. **Profile the codebase:** Use profiling tools to identify functions or code blocks with high execution time, excessive function calls, or significant resource consumption.
2. **Analyze algorithms:** Review algorithms for complexity and efficiency. Look for opportunities to use more optimal algorithms or data structures.
3. **Inspect database interactions:**
   -  Identify queries that are executed frequently or take a long time to complete.
   - Analyze query plans to identify inefficient joins, missing indexes, or other database-related bottlenecks.
4. **Examine network communication:**
    -  Analyze network requests to identify slow responses, excessive data transfer, or unnecessary round trips.
    -  Look for opportunities to implement caching or optimize network communication patterns.
5. **Prioritize based on impact:** Categorize identified bottlenecks based on their potential impact on overall system performance.

**Expected Output:** A prioritized list of performance bottlenecks with clear explanations of:

- The specific code, query, or network operation causing the bottleneck.
- The estimated impact on performance.
-  Potential solutions or optimization strategies.
</file>

<file path="config/prompt_library/performance_code_optimization_suggestions.md">
#### Suggest Code Optimization Techniques

**Objective:** Provide specific and actionable code optimization suggestions to address identified performance bottlenecks. 

**Instructions:**

1. **Review the analysis of performance bottlenecks:** Understand the nature of the bottlenecks (e.g., inefficient algorithms, slow database queries).
2. **Suggest targeted optimizations:** Based on the type of bottleneck:
    - **Algorithms and Data Structures:**
        -  Recommend more efficient algorithms or data structures (e.g., using a hash table instead of linear search).
        -  Suggest techniques like memoization or dynamic programming to avoid redundant computations. 
    - **Database Optimization:**
        -  Recommend optimizing database queries by adding indexes, rewriting inefficient joins, or using more efficient SQL constructs.
        -  Suggest caching frequently accessed data to reduce database load.
    - **Network Communication:**
        -  Recommend techniques like data compression or reducing the size of data payloads. 
        -  Suggest using asynchronous requests or persistent connections to minimize network latency. 
3. **Prioritize suggestions:**  Rank suggested optimizations based on their potential impact on performance and the effort required to implement them.

**Expected Output:** A prioritized list of code optimization recommendations, each including:

-  A clear description of the optimization technique.
-  The specific code area where the optimization should be applied.
-  The expected performance benefit or gain.
- The estimated effort or complexity of implementing the suggestion.
</file>

<file path="config/prompt_library/performance_concurrency_synchronization_analysis.md">
#### Concurrency and Synchronization Analysis 

**Objective:**  Analyze the codebase to identify potential issues related to concurrency, such as race conditions or deadlocks, and suggest solutions to improve thread safety and synchronization mechanisms.

**Instructions:**

1. **Identify concurrent code:**  Locate code sections that are executed by multiple threads or processes concurrently, especially those accessing shared resources. 
2. **Check for race conditions:** Analyze code for potential scenarios where multiple threads access and modify shared data simultaneously, leading to unpredictable or incorrect results.
3. **Detect potential deadlocks:** Identify situations where two or more threads are blocked indefinitely, each waiting for the other to release the resources it needs.
4. **Review synchronization mechanisms:**
    - Analyze the use of locks, mutexes, semaphores, or other synchronization primitives for correctness and efficiency.
    -  Look for potential issues like:
       - Deadlocks caused by incorrect locking order.
       -  Performance bottlenecks due to excessive locking or contention.
5. **Suggest improvements:** Provide recommendations to fix or prevent concurrency issues:
    -  Use appropriate synchronization mechanisms to protect shared resources. 
    - Ensure correct locking order to avoid deadlocks.
    -  Consider lock-free data structures or algorithms where applicable to minimize contention.
    - Implement strategies for efficient thread management and communication. 

**Expected Output:** A detailed report that:

-  Identifies potential concurrency issues in the codebase.
-  Explains the nature of each issue (race condition, deadlock, etc.) and its potential impact.
-  Suggests code-level solutions, refactoring recommendations, or design pattern implementations to enhance thread safety and prevent concurrency problems.
</file>

<file path="config/prompt_library/performance_configuration_tuning.md">
**Objective:** Provide recommendations for optimal configuration settings of databases, application servers, or other infrastructure components to enhance performance.

**Instructions:**

1. **Gather system information:** Identify the specific software versions and hardware specifications of the infrastructure components (database, application server, etc.).
2. **Understand performance goals:**  Consider the desired performance targets for response times, throughput, or resource utilization.
3. **Research configuration parameters:**  Refer to the documentation of each infrastructure component to understand the available configuration parameters and their impact on performance. 
4. **Suggest optimized settings:** Based on best practices and the specific system context, recommend optimal values for key configuration parameters, such as:
    -  **Database:**  Connection pool size, buffer cache size, query cache size, indexing settings.
    - **Application Server:** Thread pool size, JVM heap size, garbage collection settings, session timeout.
    -  **Web Server:** Keep-alive settings, caching configuration, number of worker processes.
5. **Explain the rationale:**  Provide clear justifications for the recommended settings, explaining how they are expected to improve performance. 

**Expected Output:** A configuration tuning guide that includes:

- A list of key configuration parameters for each relevant infrastructure component.
- Recommended optimal values for those parameters.
-  Clear explanations of the rationale behind each suggestion and its potential impact on performance.  
-  Cautions or considerations for applying the configuration changes (e.g., potential impact on other aspects of the system).
</file>

<file path="config/prompt_library/performance_resource_usage_profiling.md">
#### Resource Usage Profiling 

**Objective:**  Analyze the codebase to identify areas that excessively consume resources like CPU, memory, or disk I/O. Provide insights to guide optimization efforts for more efficient resource utilization.

**Instructions:**

1. **Use profiling tools:** Employ resource profiling tools to monitor and analyze CPU usage, memory allocation, and disk I/O during application execution. 
2. **Identify resource-intensive operations:** Pinpoint code blocks, functions, or processes that contribute most significantly to high CPU load, excessive memory consumption, or frequent disk accesses.
3. **Analyze memory management:**
    -  Look for memory leaks, where objects are not properly released after they are no longer needed.
    -  Investigate areas with high object creation rates or large object sizes.
4. **Examine I/O operations:**
    - Identify areas with frequent file system reads or writes.
    - Analyze if data can be cached or if I/O operations can be batched for better performance.
5. **Correlate resource usage with code:** Connect the identified resource-intensive areas back to specific code segments, functions, or modules to guide optimization efforts.

**Expected Output:** A comprehensive report detailing:

- Code areas with high CPU usage, including call graphs and execution times.
-  Memory allocation hotspots, including object sizes, allocation frequencies, and potential leaks.
- Disk I/O intensive operations and recommendations for reducing or optimizing them.
- Overall insights into the codebase's resource consumption patterns and areas for potential improvement.
</file>

<file path="config/prompt_library/performance_scalability_analysis.md">
#### Scalability Analysis

**Objective:** Assess the codebase's ability to handle increasing loads and identify potential limitations that might hinder scalability, suggesting improvements to enhance scalability. 

**Instructions:**

1. **Understand the current architecture:**  Analyze the codebase's architecture to determine its components, their interactions, and data flow.
2. **Identify scaling bottlenecks:**  Look for potential scaling limits in areas like:
    - Database: Analyze query performance under load, database connection pooling, and potential for sharding or replication.
    - Application Servers: Assess the capacity to handle concurrent requests, session management, and load balancing capabilities.
    - Network:  Evaluate network bandwidth, latency, and capacity to manage increased traffic. 
    - Caching: Determine if caching mechanisms are in place and if they can be optimized for scalability.
3. **Consider different scaling strategies:** Evaluate the suitability of:
    - Vertical scaling (upgrading hardware)
    - Horizontal scaling (adding more instances) 
4. **Suggest architectural improvements:** Recommend changes to the codebase or infrastructure to address scalability limitations:
    - Introduce asynchronous processing or queuing mechanisms.
    - Optimize database interactions for concurrency.
    - Implement distributed caching solutions.
    -  Consider microservices architecture for independent scaling of components. 

**Expected Output:** A detailed report covering:

- An assessment of the codebase's current scalability.
-  Identification of potential bottlenecks and their impact. 
-  Specific recommendations for architectural changes, code optimization, or infrastructure adjustments to improve scalability.
</file>

<file path="config/prompt_library/performance_test_scenario_generation.md">
**Objective:** Create realistic performance test scenarios that can be used to simulate high load conditions and identify potential bottlenecks in the codebase.

**Instructions:**

1. **Consider realistic user behavior:** Analyze typical user workflows and interactions with the system. 
2. **Identify critical user flows:** Focus on simulating user actions that are most likely to stress the system under high load, such as:
    -  User login/authentication
    -  Searching for products or content
    -  Adding items to a shopping cart
    -  Placing orders
3. **Determine user load and distribution:**  
    - Estimate the expected number of concurrent users during peak hours.
    - Define the geographic distribution of users, if relevant.
4. **Specify performance metrics:** Clearly define the key performance indicators (KPIs) to be measured during the tests, such as:
    -  Response time
    -  Throughput (transactions per second)
    -  Error rate
    - Resource utilization (CPU, memory, network)
5. **Use a performance testing tool:** Structure the test scenarios in a format compatible with a performance testing tool like JMeter, Gatling, or Locust.

**Expected Output:** A set of performance test scenarios, defined in a suitable format for a chosen testing tool, that:

-  Realistically simulate user behavior and load patterns.
-  Target critical user flows to stress the system under high load. 
-  Include clear performance metrics and thresholds for evaluating system performance.
</file>

<file path="config/prompt_library/pestel_analysis.md">
# PESTEL Analysis for Codebase

**Objective:** Analyze the macro-environmental factors that may impact the codebase and its associated product using the PESTEL framework.

**Instructions:**

1. Review the codebase and identify the key features and functionalities it provides.
2. Analyze each of the PESTEL factors in relation to the codebase and its product:

   a. Political Factors:
      - Are there any government policies, regulations, or political stability issues that could impact the product?
      - Are there any upcoming changes in the political landscape that could affect the product?

   b. Economic Factors:
      - How do economic conditions (e.g., economic growth, inflation, exchange rates) affect the demand for the product?
      - Are there any economic trends or cycles that could impact the product's success?

   c. Social Factors:
      - What are the demographic trends, cultural attitudes, and lifestyle changes that could influence the adoption of the product?
      - Are there any social movements or shifts in consumer behavior that could impact the product?

   d. Technological Factors:
      - What are the technological advancements or disruptions that could affect the product or its market?
      - Are there any emerging technologies that could be leveraged to improve the product or create new opportunities?

   e. Environmental Factors:
      - Are there any environmental concerns, regulations, or trends that could impact the product or its perception?

   f. Legal Factors:
      - What are the relevant laws, regulations, and legal requirements that the product must comply with?
      - Are there any upcoming legal changes or potential legal risks that could affect the product?

3. Identify the key opportunities and threats for the codebase and its product based on the PESTEL analysis.
4. Develop strategies for leveraging opportunities and mitigating threats identified in the analysis.
5. Incorporate the insights from the PESTEL analysis into the product's development, marketing, and strategic planning.

**Expected Output:** A comprehensive PESTEL analysis of the codebase and its associated product, including:
- Analysis of each PESTEL factor and its potential impact on the product
- Identification of key opportunities and threats based on the analysis
- Strategies for leveraging opportunities and mitigating threats
- Recommendations for incorporating PESTEL insights into product development, marketing, and strategic planning
</file>

<file path="config/prompt_library/porters_five_forces_analysis.md">
# Porter's Five Forces Analysis for Codebase

**Objective:** Analyze the codebase and its market environment using Porter's Five Forces framework to understand the competitive forces shaping the industry.

**Instructions:**

1. Review the codebase and identify the key features and functionalities it provides.
2. Analyze each of Porter's Five Forces in relation to the codebase:

   a. Threat of New Entrants:
      - How easy is it for new competitors to enter the market?
      - Are there significant barriers to entry (e.g., proprietary technology, high investment costs)?

   b. Bargaining Power of Suppliers:
      - How much bargaining power do suppliers (e.g., technology providers, data vendors) have?
      - Are there many suppliers or just a few dominant ones?

   c. Bargaining Power of Buyers:
      - How much bargaining power do customers have?
      - Are there many buyers or just a few large ones?
      - How easy is it for buyers to switch to alternative solutions?

   d. Threat of Substitute Products or Services:
      - Are there substitute products or services that could replace the codebase's functionality?
      - How likely are customers to switch to these substitutes?

   e. Rivalry Among Existing Competitors:
      - How intense is the competition among existing players in the market?
      - Are there many competitors or just a few dominant ones?
      - How do competitors differentiate themselves?

3. Identify the overall competitive intensity in the industry based on the analysis of the five forces.
4. Determine the codebase's competitive position and potential strategies for success within this competitive landscape.

**Expected Output:** A comprehensive Porter's Five Forces analysis of the codebase and its market environment, including:
- Assessment of each of the five forces and their impact on the codebase
- Identification of the overall competitive intensity in the industry
- Evaluation of the codebase's competitive position and potential strategies for success
</file>

<file path="config/prompt_library/product_market_fit_analysis.md">
# Product/Market Fit Analysis for Codebase

**Objective:** Analyze the codebase and its associated product to evaluate how well it meets the needs of its target market and identify opportunities for improving product/market fit.

**Instructions:**

1. Review the codebase and identify the key features and functionalities it provides.
2. Define the target market for the product:
   - Who are the ideal customers?
   - What are their characteristics, needs, and pain points?
3. Analyze how well the product addresses the needs of the target market:
   - Does the product solve a real problem for the target customers?
   - How well do the product's features and functionalities meet customer needs?
   - Is the product easy to use and understand for the target customers?
4. Evaluate the product's unique value proposition:
   - How does the product differentiate itself from competitors?
   - Does the product offer compelling benefits that are difficult for competitors to replicate?
5. Assess customer feedback and engagement:
   - What do customers say about the product in reviews, surveys, or other feedback channels?
   - Are customers actively using and engaging with the product?
   - Are there any common complaints or feature requests from customers?
6. Identify opportunities for improving product/market fit:
   - What changes could be made to the product to better meet customer needs?
   - Are there any unmet needs or underserved segments within the target market?
   - How could the product's value proposition be strengthened or clarified?
7. Develop a plan for iterating and improving the product based on the product/market fit analysis.

**Expected Output:** A comprehensive Product/Market Fit analysis of the codebase and its associated product, including:
- Definition of the target market and customer needs
- Evaluation of how well the product meets customer needs and differentiates itself
- Assessment of customer feedback and engagement
- Identification of opportunities for improving product/market fit
- Plan for iterating and improving the product based on the analysis
</file>

<file path="config/prompt_library/quality_code_complexity_analysis.md">
# Code Complexity Analysis

**Objective:** Analyze the codebase to identify areas with high cyclomatic complexity, deep nesting, or excessive method lengths, providing insights into potential readability and maintainability issues.

**Instructions:**

1. Review the codebase and identify areas with:
   - High cyclomatic complexity
   - Deep nesting
   - Excessive method lengths

2. For each identified issue, analyze:
   a. Location:
      - File path
      - Line number(s)

   b. Description:
      - Brief explanation of the complexity issue

   c. Impact:
      - Potential effects on code readability
      - Potential effects on code maintainability

   d. Refactoring Suggestions:
      - Propose methods to simplify or restructure the code
      - Suggest alternative approaches to reduce complexity

3. If no significant issues are found, provide a summary stating that the codebase has acceptable complexity levels.

4. Consider the overall complexity trends in the codebase:
   - Are there specific areas or modules that consistently show higher complexity?
   - Are there patterns in the types of complexity issues encountered?

5. Suggest general best practices or coding standards that could help prevent future complexity issues.

**Expected Output:** A comprehensive analysis of the codebase's complexity, detailing specific issues found, their impact, and suggestions for improvement. The report should include:

1. An overview of the complexity analysis findings
2. Detailed breakdowns of each identified issue
3. General recommendations for maintaining code simplicity
4. If applicable, a summary of complexity trends or patterns observed in the codebase

For each identified issue, use the following format:

File: [file path]
Line(s): [line number(s)]
Issue: [brief description]
Impact: [potential impact on readability and maintainability]
Suggestions:
- [refactoring suggestion 1]
- [refactoring suggestion 2]
</file>

<file path="config/prompt_library/quality_code_documentation_coverage_analysis.md">
# Code Documentation Coverage Analysis

**Objective:** Analyze the codebase to determine the coverage and quality of code documentation, identifying areas lacking documentation or with insufficient explanations.

**Instructions:**

1. Review the codebase and assess the documentation coverage and quality.

2. For each file or module, analyze:

   a. Documentation Coverage:
      - Percentage of documented code elements (classes, functions, methods)
      - Identification of undocumented or poorly documented areas

   b. Documentation Quality:
      - Completeness of explanations
      - Clarity and readability of documentation
      - Consistency in documentation style

   c. Code Elements:
      - Classes: presence and quality of class-level documentation
      - Methods/Functions: presence and quality of method-level documentation
      - Variables: presence and quality of variable descriptions, especially for complex or non-obvious variables

   d. Special Documentation:
      - Presence and quality of module-level documentation
      - Inline comments for complex algorithms or logic
      - Usage examples or code snippets where applicable

3. Identify patterns or trends in documentation practices across the codebase.

4. Assess the overall documentation strategy and its effectiveness.

5. Suggest improvements for documentation coverage and quality, considering:
   - Priority areas based on code complexity and importance
   - Standardization of documentation practices
   - Tools or processes to enhance documentation efforts

**Expected Output:** A comprehensive analysis of the codebase's documentation coverage and quality, including:

1. An overview of the documentation analysis findings
2. Detailed breakdowns for each file or module
3. General recommendations for improving documentation practices
4. If applicable, a summary of documentation trends or patterns observed in the codebase

For each file or module, use the following format:

File: [file path]
Documentation Coverage: [percentage of documented code elements]%
Quality Assessment: [brief assessment of documentation quality]
Suggestions:
- [improvement suggestion 1]
- [improvement suggestion 2]
- ...

Conclude with an overall summary of the documentation coverage and quality across the entire codebase, including recommendations for prioritizing documentation efforts based on the importance and complexity of the code.
</file>

<file path="config/prompt_library/quality_code_duplication_analysis.md">
# Code Duplication Analysis

**Objective:** Analyze the codebase to identify duplicated code fragments, providing insights into potential maintainability issues and suggesting refactoring opportunities.

**Instructions:**

1. Review the codebase and identify instances of duplicated code.

2. For each identified duplication, analyze:

   a. Location:
      - File paths of affected files
      - Line numbers in each file

   b. Duplication Details:
      - Length of the duplicated code fragment (in lines of code)
      - Content or purpose of the duplicated code

   c. Impact Assessment:
      - Potential impact on code maintainability
      - Risks associated with the duplication (e.g., inconsistent updates)

   d. Refactoring Opportunities:
      - Suggestions for extracting duplicated code into reusable functions or classes
      - Potential design patterns or architectural changes to eliminate duplication

3. Identify patterns or trends in code duplication across the codebase.

4. Assess the overall impact of code duplication on the project's maintainability and scalability.

5. Suggest improvements to reduce code duplication, considering:
   - Priority areas based on the extent and impact of duplication
   - Refactoring strategies that align with the project's architecture
   - Tools or processes to prevent future code duplication

**Expected Output:** A comprehensive analysis of the codebase's code duplication, including:

1. An overview of the duplication analysis findings
2. Detailed breakdowns for each identified duplication
3. General recommendations for reducing and preventing code duplication
4. If applicable, a summary of duplication trends or patterns observed in the codebase

For each identified duplication, use the following format:

Files:
- [file path 1]
- [file path 2]
Line(s): 
- [file 1 line numbers]
- [file 2 line numbers]
Length: [number of duplicated lines]
Impact: [potential impact on maintainability]
Suggestions:
- [refactoring suggestion 1]
- [refactoring suggestion 2]
- ...

If no significant duplications are found, provide a summary stating that the codebase has minimal code duplication.

Conclude with an overall assessment of the code duplication in the project, including recommendations for addressing the most critical instances and strategies for maintaining a DRY (Don't Repeat Yourself) codebase.
</file>

<file path="config/prompt_library/quality_code_style_consistency_analysis.md">
# Code Style Consistency Analysis

**Objective:** Conduct a thorough analysis of the codebase to evaluate consistency in code style, naming conventions, and formatting, identifying deviations from the project's style guide or industry best practices, and providing actionable recommendations for improvement.

**Instructions:**

1. Review the entire codebase, focusing on:
   - Naming conventions (variables, functions, classes, modules)
   - Code formatting (indentation, line length, whitespace usage)
   - Comment style and frequency
   - File organization and structure
   - Language-specific idioms and best practices

2. For each identified inconsistency or style issue, analyze:

   a. Location:
      - File path
      - Line number(s) or range

   b. Issue Details:
      - Type of inconsistency (e.g., naming, formatting, structure)
      - Description of the deviation from expected style
      - Comparison with the correct style (if applicable)

   c. Impact Assessment:
      - Effect on code readability
      - Potential for introducing bugs or maintenance issues
      - Impact on team collaboration and onboarding of new developers

   d. Correction Suggestions:
      - Specific recommendations for addressing the issue
      - Code snippets demonstrating the correct style (if applicable)
      - Explanation of the rationale behind the suggested changes

3. Identify patterns and trends in style inconsistencies across the codebase:
   - Are certain types of issues more prevalent?
   - Do inconsistencies cluster in specific modules or areas of the codebase?
   - Are there differences in style between different team members or over time?

4. Assess the overall adherence to the project's style guide or industry standards:
   - Percentage of code adhering to expected style
   - Areas of high compliance and areas needing improvement
   - Comparison with similar projects or industry benchmarks (if available)

5. Analyze the effectiveness of current style enforcement mechanisms:
   - Evaluate the use of linters, formatters, or other automated tools
   - Assess the consistency of code review practices related to style

6. Provide strategic recommendations for improving code style consistency:
   - Suggest updates or clarifications to the project's style guide
   - Recommend tools or processes to automate style enforcement
   - Propose training or knowledge-sharing initiatives to improve team awareness
   - Suggest a phased approach for addressing identified issues, prioritizing based on impact and effort

**Expected Output:** A comprehensive analysis of the codebase's style consistency, including:

1. Executive summary of findings and key recommendations
2. Detailed analysis of style inconsistencies, grouped by category and severity
3. Statistical overview of style adherence across the codebase
4. In-depth discussion of patterns and trends in style issues
5. Evaluation of current style enforcement mechanisms
6. Strategic recommendations for improving overall code style consistency
7. Appendices with detailed examples and code snippets

For each identified issue, use the following format:

File: [file path]
Line(s): [line number(s) or range]
Issue Type: [naming/formatting/structure/etc.]
Description: [detailed description of the inconsistency]
Impact:
  - Readability: [low/medium/high] - [brief explanation]
  - Maintainability: [low/medium/high] - [brief explanation]
  - Collaboration: [low/medium/high] - [brief explanation]
Correct Style:
```[language]
[code snippet demonstrating correct style]
```
Suggestion:
  - [specific correction recommendation]
  - Rationale: [explanation of why this change improves consistency]

Conclude with an overall assessment of the code style consistency in the project, including a roadmap for addressing identified issues and implementing long-term improvements in coding practices and team collaboration.
</file>

<file path="config/prompt_library/quality_documentation_generation.md">
# Codebase Documentation Generation

**Objective:** Generate comprehensive and user-friendly documentation for the provided codebase.

**Instructions:**

1. **Analyze the attached code** and identify key components, functionalities, and APIs.
2. **Generate documentation that includes:**
    * API specifications with detailed descriptions of endpoints, parameters, and responses.
    * Function descriptions with clear explanations of their purpose, inputs, and outputs.
    * Usage examples demonstrating how to interact with the codebase effectively.
3. **Structure the documentation logically** and use a consistent format for clarity.
4. **Prioritize clarity, conciseness, and accuracy** in your documentation.

**Expected Output:**  Well-structured and informative documentation that facilitates understanding and utilization of the codebase by developers and other stakeholders.
</file>

<file path="config/prompt_library/quality_error_analysis.md">
# Codebase Error and Inconsistency Analysis

**Objective:** Identify potential errors and inconsistencies within the provided codebase.

**Instructions:**

1. **Analyze the attached code** for the following:
    * Syntax errors and logical flaws.
    * Inconsistencies in variable and function naming conventions.
    * Code duplication.
    * Performance bottlenecks.
    * Violations of established coding best practices. 
2. **Structure your analysis clearly**, pinpointing specific code snippets and providing detailed descriptions of the identified issues.
3. **Prioritize clarity and conciseness** in your explanations.

**Expected Output:** A comprehensive report detailing errors and inconsistencies, organized by code section or error type, with actionable insights for improvement.
</file>

<file path="config/prompt_library/quality_risk_assessment.md">
# Codebase Risk Assessment

**Objective:** Identify code segments within the provided codebase that could potentially lead to future issues.

**Instructions:**

1. **Analyze the attached code** with a focus on:
    * Code that is difficult to understand and maintain (code smells).
    * Fragments that might cause errors under specific conditions (edge cases).
    * Code that deviates from established coding standards.
2. **Provide detailed justifications for your concerns**, explaining the potential risks associated with each identified segment.
3. **Suggest potential solutions or mitigation strategies** to address the identified risks.

**Expected Output:** A report highlighting potential risk areas within the codebase, with clear explanations of the risks and actionable recommendations for improvement.
</file>

<file path="config/prompt_library/security_vulnerability_analysis.md">
# Security Vulnerability Analysis of Codebase

**Objective:** Identify potential security vulnerabilities within the provided codebase.

**Instructions:**

1. **Analyze the attached code** with a focus on identifying common security weaknesses such as:
    * SQL injection.
    * Cross-site scripting (XSS).
    * Cross-site request forgery (CSRF).
    * Authentication and authorization bypasses.
    * Data exposure.
2. **For each identified vulnerability, provide a detailed explanation** of:
    * The nature of the vulnerability.
    * The potential impact of exploitation.
    * Recommendations for mitigation using secure coding practices.
3. **Prioritize vulnerabilities based on their severity and potential impact.**

**Expected Output:** A comprehensive security report highlighting potential vulnerabilities within the codebase, along with clear explanations of their risks and actionable recommendations for remediation.
</file>

<file path="config/prompt_library/stakeholder_persona_generation.md">
# Codebase Stakeholder Persona Generation

**Objective:** Infer potential stakeholder personas based on the functionalities present in the codebase.

**Instructions:**

1. Analyze the codebase to identify distinct user roles and interactions.
2. For each identified role, create a stakeholder persona:
   * Name and role
   * Goals and motivations
   * Pain points and challenges
   * Key interactions with the system
3. Map specific code functionalities to each persona's needs.
4. Identify potential gaps in meeting stakeholder needs.

**Expected Output:** A set of detailed stakeholder personas derived from the codebase, including their goals, challenges, and how the current system addresses their needs.
</file>

<file path="config/prompt_library/swot_analysis.md">
# SWOT Analysis for Codebase

**Objective:** Conduct a SWOT (Strengths, Weaknesses, Opportunities, Threats) analysis of the codebase to evaluate its current state and future potential.

**Instructions:**

1. Review the codebase thoroughly, considering its architecture, features, and implementation.

2. Analyze the codebase according to the SWOT framework:

   a. Strengths:
      - What does the code do exceptionally well?
      - What unique features or efficient implementations stand out?
      - Are there any innovative algorithms or data structures?
      - How scalable or maintainable is the code?

   b. Weaknesses:
      - What are the limitations or shortcomings of the current implementation?
      - Are there any performance bottlenecks or scalability issues?
      - Are there areas where the code quality could be improved?
      - Are there missing features or incomplete implementations?

   c. Opportunities:
      - What market trends or user needs could the code capitalize on?
      - Are there potential new features or functionalities that could be added?
      - Could the code be adapted for new platforms or use cases?
      - Are there opportunities for integration with other systems or services?

   d. Threats:
      - Are there competing solutions that might make this code obsolete?
      - Are there potential security vulnerabilities or compliance issues?
      - Could changes in technology or standards negatively impact the code?
      - Are there dependencies on external libraries or services that pose risks?

3. For each point in the SWOT analysis, provide specific examples from the codebase to support your assessment.

4. Suggest strategies to:
   - Leverage strengths
   - Address weaknesses
   - Capitalize on opportunities
   - Mitigate threats

**Expected Output:** A comprehensive SWOT analysis of the codebase, providing insights into its current state and future potential, with actionable recommendations for improvement and strategic direction.
</file>

<file path="config/prompt_library/tech_adoption_lifecycle_analysis.md">
# Technology Adoption Lifecycle Analysis for Codebase

**Objective:** Analyze the codebase and its associated product using the Technology Adoption Lifecycle model to understand its current market position and identify strategies for growth.

**Instructions:**

1. Review the codebase and identify the key features and functionalities it provides.
2. Determine the current stage of the product in the Technology Adoption Lifecycle:

   a. Innovators:
      - These are the first users to adopt a new technology or product.
      - They are willing to take risks and are often technology enthusiasts.

   b. Early Adopters:
      - These users are opinion leaders who embrace new technologies early but with more caution than innovators.
      - They are often visionaries seeking a competitive advantage.

   c. Early Majority:
      - These users adopt new technologies once they have been proven by early adopters.
      - They are pragmatists who seek reliable and cost-effective solutions.

   d. Late Majority:
      - These users are more conservative and risk-averse, adopting new technologies only after they have become mainstream.
      - They prioritize ease of use and low cost.

   e. Laggards:
      - These are the last users to adopt a new technology, often due to limited resources or resistance to change.
      - They may only adopt when they have no other choice.

3. Analyze the characteristics and needs of users in the current and adjacent stages of the lifecycle.
4. Identify potential strategies for moving the product through the lifecycle stages:
   - How can the product be positioned to appeal to the next stage of adopters?
   - What features, marketing messages, or support might be needed to facilitate this transition?
5. Develop a roadmap for evolving the product and its marketing based on the lifecycle analysis.

**Expected Output:** A comprehensive Technology Adoption Lifecycle analysis of the codebase and its associated product, including:
- Identification of the product's current stage in the lifecycle
- Analysis of user characteristics and needs in the current and adjacent stages
- Strategies for moving the product through the lifecycle stages
- Roadmap for product evolution and marketing based on the lifecycle analysis
</file>

<file path="config/prompt_library/testing_unit_test_generation.md">
# Unit Test Generation for Codebase

**Objective:** Generate unit tests for the provided codebase to ensure code correctness and prevent regressions.

**Instructions:**

1. **Analyze the attached code** and identify its core functions and methods.
2. **Generate unit tests** that cover a wide range of input values and expected outputs for each function/method.
3. **Follow best practices for unit testing**, including:
    * **Test one function/method per test case.**
    * **Use descriptive test names.**
    * **Assert expected outcomes clearly.**
    * **Keep tests independent and isolated.**
4. **Prioritize test coverage for critical functionalities** and edge cases.

**Expected Output:** A comprehensive suite of unit tests that can be used to verify the correctness of the codebase and prevent regressions during future development.
</file>

<file path="config/prompt_library/value_chain_analysis.md">
# Value Chain Analysis for Codebase

**Objective:** Analyze the codebase to understand how it fits into and supports the larger value creation process of the business.

**Instructions:**

1. Review the codebase and identify key features, functionalities, and architectural components.

2. Map the codebase to the following primary activities of the value chain:

   a. Inbound Logistics:
      - How does the code handle data or resource inputs?
      - Are there features for data acquisition, validation, or initial processing?

   b. Operations:
      - What are the core processing or transformation functions in the code?
      - How does the code support the main business operations?

   c. Outbound Logistics:
      - How does the code manage output or delivery of results?
      - Are there features for data export, reporting, or product delivery?

   d. Marketing and Sales:
      - Does the code include features that support marketing efforts?
      - Are there components that facilitate sales processes or customer acquisition?

   e. Service:
      - How does the code support customer service or after-sales support?
      - Are there features for user feedback, issue tracking, or support ticket management?

3. Analyze how the code supports the following support activities:

   f. Procurement:
      - Does the code interface with systems for acquiring resources or services?
      - Are there features for managing suppliers or resources?

   g. Technology Development:
      - What innovative or unique technological aspects does the code implement?
      - How does the code leverage or advance technology in its domain?

   h. Human Resource Management:
      - Does the code support HR functions or employee management?
      - Are there features for performance tracking, training, or collaboration?

   i. Firm Infrastructure:
      - How does the code support overall business management and strategy?
      - Are there features for data analytics, reporting, or decision support?

4. For each value chain activity:
   - Identify the specific code components or features that support this activity.
   - Evaluate how effectively the code contributes to value creation in this area.
   - Suggest potential improvements or new features to enhance value creation.

5. Analyze the linkages between different activities:
   - How well does the code facilitate the flow between different value chain activities?
   - Are there any bottlenecks or inefficiencies in these linkages?

6. Identify opportunities for competitive advantage:
   - Which areas of the value chain does the code particularly excel in supporting?
   - Are there unique or innovative aspects of the code that provide a competitive edge?

7. Consider scalability and future growth:
   - How well does the current code architecture support scaling of value chain activities?
   - Are there areas where the code might limit future expansion or diversification?

**Expected Output:** A comprehensive analysis of how the codebase supports and enhances the organization's value chain, including:
- Detailed mapping of code components to value chain activities
- Evaluation of the code's effectiveness in supporting each activity
- Identified strengths, weaknesses, and opportunities for improvement
- Suggestions for enhancing the code's contribution to the overall value chain

This analysis should provide insights into how the codebase contributes to the organization's value creation process and guide decisions for future development to optimize this contribution.
</file>

<file path="config/prompt_library/value_proposition_canvas_analysis.md">
# Value Proposition Canvas Analysis for Codebase

**Objective:** Analyze the codebase using the Value Proposition Canvas to align technical features with user needs and benefits.

**Instructions:**

1. Review the codebase and identify key features and functionalities.
2. For each main component of the Value Proposition Canvas, analyze the codebase:

   Customer Profile:
   a. Customer Jobs:
      - What tasks is the user trying to perform?
      - What problems is the user trying to solve?

   b. Pains:
      - What frustrations or challenges might the user face?
      - What risks is the user trying to avoid?

   c. Gains:
      - What benefits is the user seeking?
      - What would make the user's life easier or better?

   Value Map:
   d. Products and Services:
      - What specific features or services does the code provide?
      - How do these align with the customer jobs?

   e. Pain Relievers:
      - How does the code address user frustrations or challenges?
      - What features mitigate risks for the user?

   f. Gain Creators:
      - How does the code deliver benefits to the user?
      - What features exceed user expectations?

3. Evaluate the fit between the Customer Profile and the Value Map:
   - How well do the code's features address customer jobs, pains, and gains?
   - Are there any misalignments or gaps?

4. Identify opportunities for improvement:
   - Suggest new features or modifications to better address user needs.
   - Highlight any over-engineered features that may not provide significant value.

**Expected Output:** A detailed analysis of how the codebase delivers value to users, identifying strengths, weaknesses, and opportunities for improvement based on the Value Proposition Canvas framework.
</file>

<file path="config/profiles.yaml">
profiles:
  automation_agent:
    name: "Automation Agent"
    description: "Profile for CLI tools and automation agents. Focuses on architecture, quality, and testing."
    stages:
      - architecture_layer_identification
      - architecture_design_pattern_identification
      - quality_error_analysis
      - quality_code_complexity_analysis
      - testing_unit_test_generation
      - improvement_best_practice_analysis

  backend_service:
    name: "Backend Service"
    description: "Profile for API backends and services. Includes security review and API analysis."
    stages:
      - architecture_layer_identification
      - architecture_api_conformance_check
      - architecture_database_schema_review
      - security_vulnerability_analysis
      - quality_error_analysis
      - performance_bottleneck_identification
      - testing_unit_test_generation

  internal_tool:
    name: "Internal Tool"
    description: "Lighter profile for internal developer tools. Focuses on core functionality."
    stages:
      - architecture_layer_identification
      - quality_error_analysis
      - improvement_refactoring

  quick_review:
    name: "Quick Review"
    description: "Fast review focusing on errors and code quality. Good for initial assessment."
    stages:
      - quality_error_analysis
      - quality_code_complexity_analysis

  full_review:
    name: "Full Review"
    description: "Comprehensive review including architecture, quality, security, performance, and testing."
    stages:
      - architecture_layer_identification
      - architecture_design_pattern_identification
      - architecture_coupling_cohesion_analysis
      - quality_error_analysis
      - quality_code_complexity_analysis
      - quality_code_duplication_analysis
      - quality_code_style_consistency_analysis
      - security_vulnerability_analysis
      - performance_bottleneck_identification
      - performance_scalability_analysis
      - testing_unit_test_generation
      - evolution_technical_debt_estimation
      - improvement_best_practice_analysis

  architecture_deep_dive:
    name: "Architecture Deep Dive"
    description: "In-depth architectural analysis including patterns, layers, and coupling."
    stages:
      - architecture_layer_identification
      - architecture_design_pattern_identification
      - architecture_coupling_cohesion_analysis
      - architecture_diagram_generation
      - architecture_refactoring_for_design_patterns

  quality_audit:
    name: "Quality Audit"
    description: "Comprehensive code quality analysis including complexity, duplication, and documentation."
    stages:
      - quality_error_analysis
      - quality_code_complexity_analysis
      - quality_code_duplication_analysis
      - quality_code_style_consistency_analysis
      - quality_code_documentation_coverage_analysis
      - quality_risk_assessment

  performance_analysis:
    name: "Performance Analysis"
    description: "Performance-focused analysis including bottlenecks, scalability, and optimization."
    stages:
      - performance_bottleneck_identification
      - performance_code_optimization_suggestions
      - performance_scalability_analysis
      - performance_resource_usage_profiling
      - performance_concurrency_synchronization_analysis

  security_audit:
    name: "Security Audit"
    description: "Security-focused analysis and vulnerability detection."
    stages:
      - security_vulnerability_analysis
      - quality_error_analysis
      - quality_risk_assessment

  evolution_assessment:
    name: "Evolution Assessment"
    description: "Analyze codebase evolution, technical debt, and refactoring opportunities."
    stages:
      - evolution_technical_debt_estimation
      - evolution_code_churn_hotspot_analysis
      - evolution_refactoring_recommendation_generation
      - evolution_impact_analysis_of_code_changes
      - improvement_refactoring

  api_review:
    name: "API Review"
    description: "API-focused analysis for backend services."
    stages:
      - architecture_api_conformance_check
      - architecture_api_client_code_generation
      - learning_backend_api_documentation

  database_review:
    name: "Database Review"
    description: "Database schema and data access analysis."
    stages:
      - architecture_database_schema_review
      - architecture_database_schema_documentation

  documentation_generation:
    name: "Documentation Generation"
    description: "Generate comprehensive documentation for the codebase."
    stages:
      - quality_documentation_generation
      - learning_backend_api_documentation
      - learning_frontend_component_documentation
      - architecture_diagram_generation
</file>

<file path="config/prompts.yaml">
prompts:
  alignment_with_prd:
    id: alignment_with_prd
    goal: "Identify gaps between current implementation and PRD requirements"
    stage: alignment
    template: |
      You are analyzing a codebase against its PRD to identify implementation gaps.

      ## Product Requirements Document:
      {{ prd }}

      ## Current Codebase:
      {{ code_context }}

      ## Previous Analysis (if any):
      {{ history }}

      Current Stage: {{ current_stage }}

      ## Your Task:
      Analyze the codebase against the PRD and identify:
      1. Features described in the PRD that are missing or incomplete
      2. Requirements that are partially implemented
      3. Deviations from the specified architecture or approach

      Provide your response as JSON with this structure:
      ```json
      {
        "summary": "Brief overview of alignment status",
        "recommendations": ["List of high-level recommendations"],
        "tasks": [
          {
            "title": "Task title",
            "description": "What needs to be done",
            "priority": "critical|high|medium|low",
            "file": "path/to/relevant/file.py"
          }
        ]
      }
      ```

  architecture_sanity:
    id: architecture_sanity
    goal: "Review architecture for best practices and maintainability"
    stage: architecture
    template: |
      You are reviewing a codebase for architectural quality and best practices.

      ## PRD Context:
      {{ prd }}

      ## Codebase:
      {{ code_context }}

      ## Previous Analysis:
      {{ history }}

      Current Stage: {{ current_stage }}

      ## Your Task:
      Analyze the codebase architecture for:
      1. Code organization and modularity
      2. Separation of concerns
      3. Dependency management and coupling
      4. Adherence to Python best practices
      5. Potential scalability issues

      Provide your response as JSON with this structure:
      ```json
      {
        "summary": "Overview of architectural health",
        "recommendations": ["Architectural improvements to consider"],
        "tasks": [
          {
            "title": "Refactoring task",
            "description": "Specific changes needed",
            "priority": "critical|high|medium|low",
            "file": "path/to/file.py"
          }
        ]
      }
      ```

  core_flow_hardening:
    id: core_flow_hardening
    goal: "Identify robustness improvements for core flows"
    stage: hardening
    template: |
      You are analyzing a codebase to improve robustness and error handling.

      ## PRD:
      {{ prd }}

      ## Codebase:
      {{ code_context }}

      ## Analysis History:
      {{ history }}

      Current Stage: {{ current_stage }}

      ## Your Task:
      Analyze the core flows and identify improvements for:
      1. Error handling and recovery
      2. Input validation
      3. Edge case handling
      4. Retry logic for external calls (APIs, file I/O, subprocess)
      5. Graceful degradation
      6. Logging and observability

      Provide your response as JSON with this structure:
      ```json
      {
        "summary": "Overview of robustness status",
        "recommendations": ["Key hardening recommendations"],
        "tasks": [
          {
            "title": "Hardening task",
            "description": "Specific improvement needed",
            "priority": "critical|high|medium|low",
            "file": "path/to/file.py"
          }
        ]
      }
      ```

  test_suite_mvp:
    id: test_suite_mvp
    goal: "Identify critical tests needed for MVP quality"
    stage: testing
    template: |
      You are reviewing test coverage for a codebase.

      ## PRD:
      {{ prd }}

      ## Codebase:
      {{ code_context }}

      ## Previous Analysis:
      {{ history }}

      Current Stage: {{ current_stage }}

      ## Your Task:
      Analyze the codebase and identify:
      1. Missing unit tests for core functions
      2. Missing integration tests for main flows
      3. Edge cases that need test coverage
      4. Test infrastructure improvements needed

      Focus on tests that provide the most value for MVP quality.

      Provide your response as JSON with this structure:
      ```json
      {
        "summary": "Overview of test coverage",
        "recommendations": ["Testing strategy recommendations"],
        "tasks": [
          {
            "title": "Test to add",
            "description": "What the test should cover",
            "priority": "critical|high|medium|low",
            "file": "tests/test_*.py"
          }
        ]
      }
      ```

  security_review:
    id: security_review
    goal: "Identify security vulnerabilities and improvements"
    stage: security
    template: |
      You are performing a security review of a codebase.

      ## PRD:
      {{ prd }}

      ## Codebase:
      {{ code_context }}

      ## Previous Analysis:
      {{ history }}

      Current Stage: {{ current_stage }}

      ## Your Task:
      Analyze the codebase for security issues including:
      1. Input validation and sanitization
      2. Authentication and authorization
      3. Secrets management
      4. Injection vulnerabilities
      5. Data exposure risks
      6. Dependency vulnerabilities

      Provide your response as JSON with this structure:
      ```json
      {
        "summary": "Security posture overview",
        "recommendations": ["Security improvements to prioritize"],
        "tasks": [
          {
            "title": "Security fix",
            "description": "What needs to be secured",
            "priority": "critical|high|medium|low",
            "file": "path/to/file.py"
          }
        ]
      }
      ```
</file>

<file path="docs/mvp_improvement_plan.md">
# MVP Improvement Plan

**Generated:** 2025-12-14 10:17:33
**Profile:** Quick Review
**Status:** Ready for implementation

---

## PRD Summary

# Product Requirements Document (PRD)

**Project:** Meta-Agent for Automated Codebase Refinement
**Owner:** Developer
**Date:** 2025-12-14

---

## Executive Summary

This document describes a Python CLI "meta-agent" that refines an existing codebase from v0 to MVP. The system integrates with:
- A codebase packer (Repomix) to generate a single-file representation of the repo
- An analysis/planning LLM (like Perplexity) to run prompt-library driven analyses
- A coding assistant (like Claude Code) to apply code changes and run tests

The system uses a prompt library and profile system that define stages such as:
- `alignment_with_prd`
- `architecture_sanity`
- `core_flow_hardening`
- `test_suite_mvp`

*[PRD truncated for brevity]*

## Analysis Stages

### Identify gaps between current implementation and PRD requirements

Mock analysis completed successfully.

**Recommendations:**
- This is a mock recommendation for testing.
- Consider running with real API keys for actual analysis.


## Implementation Tasks

### [MEDIUM] Medium Priority

- [ ] **Mock task from analysis** (`src/example.py`)
  - This is a placeholder task from mock analysis.


---

## Instructions for Claude Code

To implement this plan, open Claude Code in the repository and use the following prompt:

```
Read docs/mvp_improvement_plan.md and implement the tasks in order of priority.
For each task:
1. Understand the requirement
2. Make the necessary code changes
3. Run relevant tests
4. Mark the checkbox as complete when done

Start with the highest priority tasks first.
```

### Implementation Notes

- Work through tasks systematically, starting with Critical/High priority
- Run tests after each significant change
- Commit changes incrementally with descriptive messages
- If a task is unclear, review the relevant stage summary above for context
</file>

<file path="docs/prd.md">
# Product Requirements Document (PRD)

**Project:** Meta-Agent for Automated Codebase Refinement
**Owner:** Developer
**Date:** 2025-12-14

---

## Executive Summary

This document describes a Python CLI "meta-agent" that refines an existing codebase from v0 to MVP. The system integrates with:
- A codebase packer (Repomix) to generate a single-file representation of the repo
- An analysis/planning LLM (like Perplexity) to run prompt-library driven analyses
- A coding assistant (like Claude Code) to apply code changes and run tests

The system uses a prompt library and profile system that define stages such as:
- `alignment_with_prd`
- `architecture_sanity`
- `core_flow_hardening`
- `test_suite_mvp`

---

## 1. Problem Statement

Building an initial v0 from a PRD with Claude Code is now relatively fast, but turning that v0 into a robust, production-ready MVP still requires a lot of manual review, planning, and refactoring. Human time is spent on repetitive tasks: scanning the codebase, deciding which Codebase-Digest-style prompts to use, interpreting results, and translating them into concrete implementation work.

The goal is to create a **meta-agent system** that can automatically analyze a codebase, choose appropriate prompts from a prompt library (inspired by Codebase Digest), and orchestrate Perplexity + Claude/Claude Code to iteratively refine a project from "v0 that runs" into "viable MVP," with minimal human intervention.

---

## 2. Goals & Non-Goals

### 2.1 Goals

- **G1 – Automate post-v0 refinement:**
  Given a repository and its PRD, the system should automatically run analysis cycles, generate improvement plans, and invoke Claude Code to apply changes.

- **G2 – Prompt-driven analysis engine:**
  Maintain a configurable prompt library (similar to Codebase Digest's) and allow the system to select and apply prompts in stages (alignment, architecture, hardening, tests).

- **G3 – Tool integration:**
  Integrate at least these tools into a coherent pipeline:
  - Repomix (codebase packing)
  - Perplexity (analysis/planning)
  - Claude (review/summary)
  - Claude Code (implementation)

- **G4 – Project profiles:**
  Support multiple "profiles" (e.g., backend service, automation/agent, internal tool) that define which stages and prompts to run.

- **G5 – Transparent plans and diffs:**
  Every cycle should produce:
  - A human-readable analysis summary.
  - A prioritized task list / implementation plan.
  - Links or references to actual code changes (diffs) created by Claude Code.

### 2.2 Non-Goals

- Not trying to fully replace human review for production-critical code; human sign-off is still expected.
- Not building a generic LLM platform; this is a **developer-centric pipeline** optimized for your own workflows.
- Not designing a UI beyond a basic CLI or minimal web dashboard in v1.

---

## 3. System Architecture

### 3.1 Project Structure

```
meta-agent/
├── pyproject.toml                 # Project configuration (uv/pip compatible)
├── README.md                      # Setup and usage documentation
├── .env.example                   # Environment variable template
├── src/
│   └── metaagent/
│       ├── __init__.py
│       ├── cli.py                 # CLI entrypoint (Click/Typer)
│       ├── orchestrator.py        # Main refinement orchestration logic
│       ├── repomix.py             # Repomix subprocess integration
│       ├── prompts.py             # Prompt/profile loading and rendering
│       ├── analysis.py            # Analysis LLM integration (Perplexity)
│       ├── plan_writer.py         # Plan file generation
│       └── config.py              # Configuration management
├── config/
│   ├── prompts.yaml               # Prompt templates library
│   └── profiles.yaml              # Profile definitions
├── docs/
│   └── prd.md                     # Project PRD (this document)
└── tests/
    ├── __init__.py
    ├── test_cli.py
    ├── test_orchestrator.py
    ├── test_repomix.py
    └── test_prompts.py
```

### 3.2 Key Components

#### CLI Entrypoint (`cli.py`)
- Provides `metaagent refine --profile <profile> --repo <path>` command
- Handles argument parsing and validation
- Initializes and invokes the orchestrator

#### Orchestrator (`orchestrator.py`)
- Main refinement loop coordinator
- Loads PRD, prompts, and profile configuration
- Executes stages in order defined by profile
- Aggregates results and produces improvement plan

#### Repomix Integration (`repomix.py`)
- Runs Repomix CLI via subprocess
- Reads and returns packed codebase content
- Handles errors and timeouts

#### Prompt/Profile Loading (`prompts.py`)
- Loads `config/prompts.yaml` and `config/profiles.yaml`
- Renders prompt templates with variables:
  - `{{prd}}` - PRD content
  - `{{code_context}}` - Packed codebase
  - `{{history}}` - Previous analysis summaries
  - `{{current_stage}}` - Current stage name

#### Analysis Engine (`analysis.py`)
- Wraps Perplexity API calls
- Sends rendered prompts
- Parses structured responses (summary, recommendations, tasks)
- Provides mock mode for testing

#### Plan Writer (`plan_writer.py`)
- Generates `docs/mvp_improvement_plan.md`
- Includes PRD recap, stage summaries, and prioritized task list
- Formats tasks with checkboxes for Claude Code consumption

---

## 4. High-Level User Flows

### 4.1 Flow A: PRD to v0 (baseline - outside this system)

1. User writes a PRD for a new project and saves it as `docs/prd.md` in an empty repo.
2. User opens the repo in Claude Code.
3. User instructs Claude Code to implement v0.
4. Claude Code builds the initial implementation and passes tests.
5. User commits v0 to version control.

*(This phase is manual and outside this system, but assumed as a prerequisite.)*

### 4.2 Flow B: Meta-Agent MVP Refinement

1. User runs the meta-agent CLI:
   ```bash
   metaagent refine --profile automation_agent --repo /path/to/repo
   ```

2. Meta-agent:
   - Reads `docs/prd.md`
   - Runs Repomix on the repo to produce a packed codebase file
   - Loads the configured profile and its stages

3. Stage 1 – **PRD Alignment Analysis**:
   - Selects the `alignment_with_prd` prompt
   - Calls Perplexity with PRD + Repomix output + prompt template
   - Receives: summary of gaps vs PRD, task list to close those gaps

4. Stage 2 – **Architecture / Best Practices**:
   - Selects `architecture_sanity` and/or `best_practices_analysis` prompts
   - Calls Perplexity again
   - Receives: architecture issues, refactor suggestions, and tasks

5. Stage 3 – **Feature-specific Hardening**:
   - Runs `core_flow_hardening` (retry logic, error handling)
   - Calls Perplexity with the packed code + PRD + current history
   - Receives: detailed implementation plan for robustness

6. Stage 4 – **Test Suite MVP**:
   - Runs a testing prompt (e.g., `test_suite_mvp`)
   - Receives: list of missing tests (file names, test cases)

7. Meta-agent merges all tasks into a single `mvp_improvement_plan.md`

8. User opens Claude Code on the repo and feeds in `mvp_improvement_plan.md` with instructions to execute tasks in order

9. After implementation:
   - Meta-agent can re-run to confirm improvements and suggest final tweaks

---

## 5. Functional Requirements

### 5.1 CLI / Orchestrator

- **FR1:** Provide a CLI command `metaagent refine --profile <profile> --repo <path>`
- **FR2:** Detect and load:
  - PRD file (default `docs/prd.md`)
  - Prompt library configuration (YAML)
  - Profile configuration (mapping stages to prompts)
- **FR3:** Run Repomix on the repo to produce a packed code representation
- **FR4:** Maintain a simple "history log" for each run

### 5.2 Prompt Library & Profiles

- **FR5:** Store prompt templates in `config/prompts.yaml` including:
  - `id`
  - `goal`
  - `template`
  - `stage` or `category`
  - Optional `dependencies` or `when_to_use` hints
- **FR6:** Store profiles in `config/profiles.yaml` mapping:
  - Profile name → ordered list of stages
- **FR7:** Allow selection of prompts per stage based on profile

### 5.3 Analysis Engine (Perplexity)

- **FR8:** Construct Perplexity prompts including:
  - PRD text
  - Truncated Repomix output (within context budget)
  - Run history summary
  - Stage's prompt template
- **FR9:** Expect structured responses:
  - `summary` (what was found)
  - `recommendations`
  - `tasks` (actionable items with file references)
- **FR10:** Aggregate tasks from all stages into ordered improvement plan

### 5.4 Plan & Handoff to Claude Code

- **FR11:** Write aggregated plan to `docs/mvp_improvement_plan.md`:
  - Short recap of PRD
  - Stage summaries
  - Prioritized task list with checkboxes
- **FR12:** Provide standard instruction block for Claude Code
- **FR13:** Optionally provide separate prompt for Claude to create polished review docs

### 5.5 Iteration / Re-analysis

- **FR14:** Support re-running refinement after code changes
- **FR15:** Track whether "Must-fix" tasks are resolved

---

## 6. Non-Functional Requirements

- **NFR1:** Implementation language: Python 3.10+
- **NFR2:** All orchestration via CLI; no GUI required for v1
- **NFR3:** Configurable timeouts and max token sizes for LLM calls
- **NFR4:** Keep secrets (API keys) in environment variables
- **NFR5:** Easy to extend prompt library and profiles without changing Python code

---

## 7. Configuration Layer Design

### 7.1 config/prompts.yaml Format

```yaml
prompts:
  alignment_with_prd:
    id: alignment_with_prd
    goal: "Identify gaps between current implementation and PRD requirements"
    stage: alignment
    template: |
      You are analyzing a codebase against its PRD.

      ## PRD:
      {{prd}}

      ## Current Codebase:
      {{code_context}}

      ## Previous Analysis (if any):
      {{history}}

      Current Stage: {{current_stage}}

      Please analyze and provide:
      1. Summary of alignment gaps
      2. Missing features or incomplete implementations
      3. Prioritized task list to close gaps

      Format your response as JSON with keys: summary, recommendations, tasks

  architecture_sanity:
    id: architecture_sanity
    goal: "Review architecture for best practices and maintainability"
    stage: architecture
    template: |
      Review this codebase for architectural quality.

      ## PRD Context:
      {{prd}}

      ## Codebase:
      {{code_context}}

      Analyze:
      1. Code organization and modularity
      2. Separation of concerns
      3. Error handling patterns
      4. Dependency management

      Format your response as JSON with keys: summary, recommendations, tasks

  core_flow_hardening:
    id: core_flow_hardening
    goal: "Identify robustness improvements for core flows"
    stage: hardening
    template: |
      Analyze core flows for robustness.

      ## PRD:
      {{prd}}

      ## Codebase:
      {{code_context}}

      ## Analysis History:
      {{history}}

      Focus on:
      1. Error handling and recovery
      2. Retry logic for external calls
      3. Input validation
      4. Edge cases

      Format your response as JSON with keys: summary, recommendations, tasks

  test_suite_mvp:
    id: test_suite_mvp
    goal: "Identify critical tests needed for MVP quality"
    stage: testing
    template: |
      Review test coverage for this codebase.

      ## PRD:
      {{prd}}

      ## Codebase:
      {{code_context}}

      Identify:
      1. Missing unit tests for core functions
      2. Missing integration tests for main flows
      3. Edge cases without test coverage

      Format your response as JSON with keys: summary, recommendations, tasks
```

### 7.2 config/profiles.yaml Format

```yaml
profiles:
  automation_agent:
    name: "Automation Agent"
    description: "Profile for CLI tools and automation agents"
    stages:
      - alignment_with_prd
      - architecture_sanity
      - core_flow_hardening
      - test_suite_mvp

  backend_service:
    name: "Backend Service"
    description: "Profile for API backends and services"
    stages:
      - alignment_with_prd
      - architecture_sanity
      - core_flow_hardening
      - test_suite_mvp

  internal_tool:
    name: "Internal Tool"
    description: "Profile for internal developer tools"
    stages:
      - alignment_with_prd
      - core_flow_hardening
```

---

## 8. Milestones

### M1 – Minimal Orchestrator (MVP)
- CLI command implementation
- Repomix integration
- Single profile with 2 stages: `alignment_with_prd`, `core_flow_hardening`
- Generates basic `mvp_improvement_plan.md`
- Mock analysis function with clear interface for future LLM integration

### M2 – Full Profile + Prompt Library
- Add `architecture_sanity` and `test_suite_mvp` stages
- Config-driven prompts and profiles
- Basic run history logging
- Full Perplexity API integration

### M3 – Iteration Support
- Re-run refinement after changes
- Detect remaining gaps
- Simple rule-based logic for skipping/repeating stages

### M4 – Optional Review Generation
- Claude integration for polished review documents
- Diff tracking and reporting

---

## 9. Integrations & Dependencies

| Tool | Purpose | Integration Method |
|------|---------|-------------------|
| Repomix | Codebase packing | CLI subprocess |
| Perplexity API | Analysis/planning | HTTP API |
| Claude API | Review generation (optional) | HTTP API |
| Claude Code | Implementation execution | Plan file handoff |

---

## 10. Extension Points

The following functions should be clearly marked as extension points:

```python
def run_analysis(prompt: str) -> dict:
    """
    Extension point for LLM analysis calls.

    Args:
        prompt: Rendered prompt template

    Returns:
        dict with keys: summary, recommendations, tasks
    """
    pass

def generate_review_document(analysis_results: list, prd: str) -> str:
    """
    Extension point for generating polished review documents.

    Args:
        analysis_results: List of analysis results from all stages
        prd: Original PRD content

    Returns:
        Markdown formatted review document
    """
    pass
```

---

## 11. Environment Variables

```
PERPLEXITY_API_KEY=<your-perplexity-api-key>
ANTHROPIC_API_KEY=<your-anthropic-api-key>  # Optional, for Claude integration
METAAGENT_LOG_LEVEL=INFO
METAAGENT_TIMEOUT=120
METAAGENT_MAX_TOKENS=100000
```

---

## 12. Success Criteria

1. Running `metaagent refine --profile automation_agent --repo .` produces a valid `mvp_improvement_plan.md`
2. The plan contains actionable tasks derived from PRD alignment analysis
3. The system can be extended with new prompts/profiles via YAML configuration only
4. All core functionality works with mock analysis (no API keys required for testing)
</file>

<file path="src/metaagent/__init__.py">
"""Meta-agent for automated codebase refinement."""

__version__ = "0.1.0"
</file>

<file path="src/metaagent/analysis.py">
"""Analysis engine for running LLM analysis on codebases."""

from __future__ import annotations

import json
import re
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any, Optional

import httpx


@dataclass
class AnalysisResult:
    """Result from an analysis stage."""

    summary: str
    recommendations: list[str] = field(default_factory=list)
    tasks: list[dict[str, Any]] = field(default_factory=list)
    raw_response: str = ""
    success: bool = True
    error: Optional[str] = None


class AnalysisEngine(ABC):
    """Abstract base class for analysis engines."""

    @abstractmethod
    def analyze(self, prompt: str) -> AnalysisResult:
        """Run analysis with the given prompt.

        Args:
            prompt: The rendered prompt to send to the LLM.

        Returns:
            AnalysisResult with the analysis output.
        """
        pass


class MockAnalysisEngine(AnalysisEngine):
    """Mock analysis engine for testing without API calls."""

    def __init__(self, responses: Optional[dict[str, AnalysisResult]] = None):
        """Initialize mock engine with optional predefined responses.

        Args:
            responses: Optional dict mapping prompt substrings to responses.
        """
        self.responses = responses or {}
        self.call_count = 0
        self.last_prompt: Optional[str] = None

    def analyze(self, prompt: str) -> AnalysisResult:
        """Return a mock analysis result.

        Args:
            prompt: The rendered prompt (stored for inspection).

        Returns:
            Mock AnalysisResult.
        """
        self.call_count += 1
        self.last_prompt = prompt

        # Check for predefined responses
        for key, response in self.responses.items():
            if key in prompt:
                return response

        # Default mock response
        return AnalysisResult(
            summary="Mock analysis completed successfully.",
            recommendations=[
                "This is a mock recommendation for testing.",
                "Consider running with real API keys for actual analysis.",
            ],
            tasks=[
                {
                    "id": f"mock-task-{self.call_count}",
                    "title": "Mock task from analysis",
                    "description": "This is a placeholder task from mock analysis.",
                    "priority": "medium",
                    "file": "src/example.py",
                },
            ],
            raw_response="Mock response",
            success=True,
        )


class PerplexityAnalysisEngine(AnalysisEngine):
    """Analysis engine using Perplexity API."""

    API_URL = "https://api.perplexity.ai/chat/completions"

    def __init__(self, api_key: str, timeout: int = 120, model: str = "llama-3.1-sonar-large-128k-online"):
        """Initialize Perplexity analysis engine.

        Args:
            api_key: Perplexity API key.
            timeout: Request timeout in seconds.
            model: Model to use for analysis.
        """
        self.api_key = api_key
        self.timeout = timeout
        self.model = model
        self.client = httpx.Client(timeout=timeout)

    def analyze(self, prompt: str) -> AnalysisResult:
        """Run analysis using Perplexity API.

        Args:
            prompt: The rendered prompt to send.

        Returns:
            AnalysisResult with the analysis output.
        """
        try:
            response = self.client.post(
                self.API_URL,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": self.model,
                    "messages": [
                        {
                            "role": "system",
                            "content": "You are a code analysis expert. Analyze codebases and provide structured feedback in JSON format with keys: summary, recommendations, tasks.",
                        },
                        {
                            "role": "user",
                            "content": prompt,
                        },
                    ],
                },
            )
            response.raise_for_status()

            data = response.json()
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "")

            return self._parse_response(content)

        except httpx.HTTPStatusError as e:
            return AnalysisResult(
                summary="",
                success=False,
                error=f"HTTP error from Perplexity API: {e.response.status_code}",
                raw_response=str(e),
            )
        except httpx.TimeoutException:
            return AnalysisResult(
                summary="",
                success=False,
                error=f"Request to Perplexity API timed out after {self.timeout}s",
            )
        except Exception as e:
            return AnalysisResult(
                summary="",
                success=False,
                error=f"Unexpected error calling Perplexity API: {e}",
            )

    def _parse_response(self, content: str) -> AnalysisResult:
        """Parse the LLM response into structured result.

        Args:
            content: Raw response content from the LLM.

        Returns:
            Parsed AnalysisResult.
        """
        # Try to extract JSON from the response
        json_match = re.search(r"```(?:json)?\s*([\s\S]*?)```", content)
        if json_match:
            json_str = json_match.group(1)
        else:
            json_str = content

        try:
            data = json.loads(json_str)
            return AnalysisResult(
                summary=data.get("summary", ""),
                recommendations=data.get("recommendations", []),
                tasks=data.get("tasks", []),
                raw_response=content,
                success=True,
            )
        except json.JSONDecodeError:
            # Fallback: treat the whole response as summary
            return AnalysisResult(
                summary=content,
                recommendations=[],
                tasks=[],
                raw_response=content,
                success=True,
            )


def create_analysis_engine(
    api_key: Optional[str] = None,
    mock_mode: bool = False,
    timeout: int = 120,
) -> AnalysisEngine:
    """Factory function to create the appropriate analysis engine.

    Args:
        api_key: Perplexity API key (required if not mock mode).
        mock_mode: If True, return a mock engine.
        timeout: Request timeout in seconds.

    Returns:
        AnalysisEngine instance.

    Raises:
        ValueError: If api_key is missing and not in mock mode.
    """
    if mock_mode:
        return MockAnalysisEngine()

    if not api_key:
        raise ValueError("API key is required when not in mock mode")

    return PerplexityAnalysisEngine(api_key=api_key, timeout=timeout)
</file>

<file path="src/metaagent/cli.py">
"""CLI entrypoint for meta-agent."""

from __future__ import annotations

import logging
import sys
from pathlib import Path
from typing import Optional

import typer
from rich.console import Console
from rich.logging import RichHandler
from rich.table import Table

from . import __version__
from .config import Config
from .orchestrator import Orchestrator
from .prompts import PromptLibrary

# Initialize Typer app
app = typer.Typer(
    name="metaagent",
    help="Meta-agent for automated codebase refinement from v0 to MVP.",
    add_completion=False,
)

console = Console()


def setup_logging(verbose: bool = False) -> None:
    """Configure logging with Rich handler.

    Args:
        verbose: If True, set DEBUG level; otherwise INFO.
    """
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(message)s",
        datefmt="[%X]",
        handlers=[RichHandler(console=console, rich_tracebacks=True)],
    )


def version_callback(value: bool) -> None:
    """Print version and exit."""
    if value:
        console.print(f"metaagent version {__version__}")
        raise typer.Exit()


@app.callback()
def main(
    version: bool = typer.Option(
        None,
        "--version",
        "-v",
        callback=version_callback,
        is_eager=True,
        help="Show version and exit.",
    ),
) -> None:
    """Meta-agent for automated codebase refinement."""
    pass


@app.command()
def refine(
    repo: Path = typer.Option(
        Path.cwd(),
        "--repo",
        "-r",
        help="Path to the repository to refine.",
    ),
    prd: Optional[Path] = typer.Option(
        None,
        "--prd",
        help="Path to PRD file (default: docs/prd.md in repo).",
    ),
    max_iterations: int = typer.Option(
        10,
        "--max-iterations",
        "-n",
        help="Maximum number of refinement iterations.",
    ),
    mock: bool = typer.Option(
        False,
        "--mock",
        "-m",
        help="Run in mock mode (no API calls).",
    ),
    verbose: bool = typer.Option(
        False,
        "--verbose",
        help="Enable verbose output.",
    ),
) -> None:
    """Run iterative refinement with AI-driven analysis.

    The AI analyzes your codebase, decides which analysis prompts to run,
    implements changes, commits to GitHub, and repeats until done.
    """
    setup_logging(verbose)

    # Resolve paths
    repo_path = repo.resolve()
    if not repo_path.exists():
        console.print(f"[red]Error:[/red] Repository path does not exist: {repo_path}")
        raise typer.Exit(1)

    # Load configuration
    config = Config.from_env(repo_path)

    # Override with CLI options
    if prd:
        config.prd_path = prd.resolve()
    if mock:
        config.mock_mode = True

    # Validate configuration
    errors = config.validate()
    if errors:
        console.print("[red]Configuration errors:[/red]")
        for error in errors:
            console.print(f"  - {error}")
        raise typer.Exit(1)

    # Run iterative refinement
    console.print("\n[bold]Starting iterative refinement[/bold]")
    console.print(f"[dim]Repository:[/dim] {repo_path}")
    console.print(f"[dim]Max iterations:[/dim] {max_iterations}")
    console.print(f"[dim]Mock mode:[/dim] {'enabled' if config.mock_mode else 'disabled'}\n")

    orchestrator = Orchestrator(config)
    result = orchestrator.refine(max_iterations=max_iterations)

    # Display results
    if result.success:
        console.print("\n[green]Refinement completed successfully![/green]\n")
    else:
        console.print("\n[yellow]Refinement completed with issues.[/yellow]\n")

    console.print(f"Iterations completed: {len(result.iterations)}")
    console.print(f"Stages completed: {result.stages_completed}")
    console.print(f"Stages failed: {result.stages_failed}")

    if result.iterations:
        console.print("\n[bold]Iteration Summary:[/bold]")
        for it in result.iterations:
            status = "[green]committed[/green]" if it.committed else "[yellow]no changes[/yellow]"
            console.print(f"  {it.iteration}. {', '.join(it.prompts_run)} - {status}")

    if result.plan_path:
        console.print(f"\n[bold]Final plan written to:[/bold] {result.plan_path}")

    if result.error:
        console.print(f"\n[red]Error:[/red] {result.error}")
        raise typer.Exit(1)


@app.command("list-profiles")
def list_profiles(
    config_dir: Optional[Path] = typer.Option(
        None,
        "--config-dir",
        "-c",
        help="Path to config directory.",
    ),
) -> None:
    """List available refinement profiles."""
    # Determine config directory
    cfg_dir = config_dir.resolve() if config_dir else Path.cwd() / "config"

    if not cfg_dir.exists():
        console.print(f"[red]Error:[/red] Config directory not found: {cfg_dir}")
        raise typer.Exit(1)

    try:
        prompt_library = PromptLibrary(
            prompts_path=cfg_dir / "prompts.yaml",
            profiles_path=cfg_dir / "profiles.yaml",
            prompt_library_path=cfg_dir / "prompt_library",
        )
        prompt_library.load()
    except FileNotFoundError as e:
        console.print(f"[red]Error:[/red] {e}")
        raise typer.Exit(1)

    profiles = prompt_library.list_profiles()

    if not profiles:
        console.print("[yellow]No profiles found.[/yellow]")
        return

    table = Table(title="Available Profiles")
    table.add_column("Name", style="cyan")
    table.add_column("Description")
    table.add_column("Stages", style="dim")

    for profile in profiles:
        stages = ", ".join(profile.stages[:3])
        if len(profile.stages) > 3:
            stages += f" (+{len(profile.stages) - 3} more)"
        table.add_row(profile.name, profile.description, stages)

    console.print(table)


@app.command("list-prompts")
def list_prompts(
    config_dir: Optional[Path] = typer.Option(
        None,
        "--config-dir",
        "-c",
        help="Path to config directory.",
    ),
    category: Optional[str] = typer.Option(
        None,
        "--category",
        help="Filter by category (e.g., 'quality', 'architecture').",
    ),
) -> None:
    """List available analysis prompts from codebase-digest."""
    # Determine config directory
    cfg_dir = config_dir.resolve() if config_dir else Path.cwd() / "config"

    if not cfg_dir.exists():
        console.print(f"[red]Error:[/red] Config directory not found: {cfg_dir}")
        raise typer.Exit(1)

    prompt_library = PromptLibrary(
        prompts_path=cfg_dir / "prompts.yaml",
        profiles_path=cfg_dir / "profiles.yaml",
        prompt_library_path=cfg_dir / "prompt_library",
    )
    prompt_library.load()

    by_category = prompt_library.list_prompts_by_category()

    if not by_category:
        console.print("[yellow]No prompts found.[/yellow]")
        return

    # Filter by category if specified
    if category:
        if category not in by_category:
            console.print(f"[red]Error:[/red] Category '{category}' not found.")
            console.print(f"Available categories: {', '.join(sorted(by_category.keys()))}")
            raise typer.Exit(1)
        by_category = {category: by_category[category]}

    total = sum(len(prompts) for prompts in by_category.values())
    console.print(f"\n[bold]Available Prompts ({total} total)[/bold]\n")

    for cat_name in sorted(by_category.keys()):
        prompts = by_category[cat_name]
        table = Table(title=f"{cat_name.title()} ({len(prompts)} prompts)")
        table.add_column("ID", style="cyan")
        table.add_column("Goal")

        for prompt in sorted(prompts, key=lambda p: p.id):
            goal = prompt.goal[:60] + "..." if len(prompt.goal) > 60 else prompt.goal
            table.add_row(prompt.id, goal)

        console.print(table)
        console.print()


if __name__ == "__main__":
    app()
</file>

<file path="src/metaagent/codebase_digest.py">
"""Codebase-digest integration for directory tree and metrics."""

from __future__ import annotations

import subprocess
import tempfile
from dataclasses import dataclass
from pathlib import Path
from typing import Optional


@dataclass
class DigestResult:
    """Result from running codebase-digest on a repository."""

    tree: str  # Directory tree structure
    metrics: str  # File counts, sizes, token estimates
    content: str  # Full output including file contents (optional)
    success: bool
    error: Optional[str] = None
    format: str = "markdown"


class CodebaseDigestRunner:
    """Runs codebase-digest to generate directory tree and metrics."""

    def __init__(
        self,
        max_depth: Optional[int] = None,
        output_format: str = "markdown",
        include_content: bool = False,
        max_size_kb: int = 500,
    ):
        """Initialize the codebase-digest runner.

        Args:
            max_depth: Maximum directory traversal depth (None for unlimited).
            output_format: Output format (text, json, markdown, xml, html).
            include_content: Whether to include file contents in output.
            max_size_kb: Maximum file size to include (in KB).
        """
        self.max_depth = max_depth
        self.output_format = output_format
        self.include_content = include_content
        self.max_size_kb = max_size_kb

    def analyze(self, repo_path: Path) -> DigestResult:
        """Analyze a repository using codebase-digest.

        Args:
            repo_path: Path to the repository to analyze.

        Returns:
            DigestResult with tree, metrics, and optionally full content.
        """
        if not repo_path.exists():
            return DigestResult(
                tree="",
                metrics="",
                content="",
                success=False,
                error=f"Repository path does not exist: {repo_path}",
            )

        try:
            # Create temp file for output
            ext = self._get_extension()
            with tempfile.NamedTemporaryFile(
                mode="w", suffix=ext, delete=False
            ) as tmp_file:
                output_path = Path(tmp_file.name)

            # Build command
            cmd = ["cdigest", str(repo_path)]

            if self.max_depth is not None:
                cmd.extend(["-d", str(self.max_depth)])

            cmd.extend(["-o", self.output_format])
            cmd.extend(["--max-size", str(self.max_size_kb)])
            cmd.append("--show-size")

            if not self.include_content:
                cmd.append("--no-content")

            cmd.extend(["-f", str(output_path)])

            # Run codebase-digest (suppress debug output by redirecting stderr)
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=60,
                cwd=repo_path,
            )

            # cdigest may return 0 even with warnings, check if output file exists
            content = ""
            if output_path.exists():
                try:
                    content = output_path.read_text(encoding="utf-8")
                except Exception:
                    pass
                finally:
                    output_path.unlink(missing_ok=True)

            # If we got content, consider it a success regardless of return code
            if content and len(content) > 50:
                # Filter out debug lines if they made it into content
                lines = content.split("\n")
                filtered_lines = [
                    line for line in lines
                    if not line.startswith("Debug:") and not line.startswith("Analyzing:")
                ]
                content = "\n".join(filtered_lines)

                # Parse the output to extract tree and metrics
                tree, metrics = self._parse_output(content)

                return DigestResult(
                    tree=tree,
                    metrics=metrics,
                    content=content if self.include_content else "",
                    success=True,
                    format=self.output_format,
                )

            # No content - check for errors
            if result.returncode != 0:
                error_msg = result.stderr.strip() if result.stderr else "Unknown error"
                return DigestResult(
                    tree="",
                    metrics="",
                    content="",
                    success=False,
                    error=f"codebase-digest failed: {error_msg}",
                )

            return DigestResult(
                tree="",
                metrics="",
                content="",
                success=False,
                error="codebase-digest produced no output",
            )

        except subprocess.TimeoutExpired:
            return DigestResult(
                tree="",
                metrics="",
                content="",
                success=False,
                error="codebase-digest timed out after 60 seconds",
            )
        except FileNotFoundError:
            return DigestResult(
                tree="",
                metrics="",
                content="",
                success=False,
                error="codebase-digest not found. Install with: pip install codebase-digest",
            )
        except Exception as e:
            return DigestResult(
                tree="",
                metrics="",
                content="",
                success=False,
                error=f"Unexpected error running codebase-digest: {e}",
            )

    def _get_extension(self) -> str:
        """Get file extension for output format."""
        extensions = {
            "text": ".txt",
            "json": ".json",
            "markdown": ".md",
            "xml": ".xml",
            "html": ".html",
        }
        return extensions.get(self.output_format, ".txt")

    def _parse_output(self, content: str) -> tuple[str, str]:
        """Parse output to extract tree and metrics sections.

        Args:
            content: Full codebase-digest output.

        Returns:
            Tuple of (tree_section, metrics_section).
        """
        # For markdown format, look for sections
        if self.output_format == "markdown":
            tree = ""
            metrics = ""

            lines = content.split("\n")
            current_section = None
            section_lines: list[str] = []

            for line in lines:
                if line.startswith("## ") or line.startswith("# "):
                    # Save previous section
                    if current_section == "tree":
                        tree = "\n".join(section_lines)
                    elif current_section == "metrics":
                        metrics = "\n".join(section_lines)

                    # Start new section
                    section_lines = [line]
                    lower_line = line.lower()
                    if "tree" in lower_line or "structure" in lower_line or "directory" in lower_line:
                        current_section = "tree"
                    elif "metric" in lower_line or "statistic" in lower_line or "summary" in lower_line:
                        current_section = "metrics"
                    else:
                        current_section = None
                else:
                    section_lines.append(line)

            # Save last section
            if current_section == "tree":
                tree = "\n".join(section_lines)
            elif current_section == "metrics":
                metrics = "\n".join(section_lines)

            # If parsing didn't find sections, return full content as tree
            if not tree and not metrics:
                return content, ""

            return tree, metrics

        # For other formats, return full content
        return content, ""


def get_codebase_context(
    repo_path: Path,
    include_tree: bool = True,
    include_metrics: bool = True,
    max_depth: int = 5,
) -> str:
    """Get codebase context from codebase-digest.

    Convenience function for getting formatted codebase context.

    Args:
        repo_path: Path to the repository.
        include_tree: Include directory tree in output.
        include_metrics: Include metrics in output.
        max_depth: Maximum directory depth.

    Returns:
        Formatted string with codebase context.
    """
    runner = CodebaseDigestRunner(
        max_depth=max_depth,
        output_format="markdown",
        include_content=False,
    )

    result = runner.analyze(repo_path)

    if not result.success:
        return f"[Codebase-digest failed: {result.error}]"

    sections = []

    if include_tree and result.tree:
        sections.append("### Directory Structure\n")
        sections.append(result.tree)

    if include_metrics and result.metrics:
        sections.append("\n### Codebase Metrics\n")
        sections.append(result.metrics)

    return "\n".join(sections) if sections else "[No codebase context available]"
</file>

<file path="src/metaagent/config.py">
"""Configuration management for meta-agent."""

from __future__ import annotations

import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional

from dotenv import load_dotenv


@dataclass
class Config:
    """Configuration settings for the meta-agent."""

    # API Keys
    perplexity_api_key: Optional[str] = None
    anthropic_api_key: Optional[str] = None

    # Paths
    repo_path: Path = field(default_factory=Path.cwd)
    config_dir: Path = field(default_factory=lambda: Path.cwd() / "config")
    prd_path: Optional[Path] = None

    # LLM Settings
    timeout: int = 120
    max_tokens: int = 100000

    # Runtime Settings
    log_level: str = "INFO"
    mock_mode: bool = False

    @classmethod
    def from_env(cls, repo_path: Optional[Path] = None) -> Config:
        """Load configuration from environment variables.

        Args:
            repo_path: Optional path to the repository. Defaults to CWD.

        Returns:
            Config instance populated from environment.
        """
        load_dotenv()

        repo = Path(repo_path) if repo_path else Path.cwd()

        return cls(
            perplexity_api_key=os.getenv("PERPLEXITY_API_KEY"),
            anthropic_api_key=os.getenv("ANTHROPIC_API_KEY"),
            repo_path=repo,
            config_dir=repo / "config",
            prd_path=repo / "docs" / "prd.md",
            timeout=int(os.getenv("METAAGENT_TIMEOUT", "120")),
            max_tokens=int(os.getenv("METAAGENT_MAX_TOKENS", "100000")),
            log_level=os.getenv("METAAGENT_LOG_LEVEL", "INFO"),
            mock_mode=os.getenv("METAAGENT_MOCK_MODE", "").lower() in ("true", "1", "yes"),
        )

    def validate(self) -> list[str]:
        """Validate configuration and return list of errors.

        Returns:
            List of validation error messages. Empty if valid.
        """
        errors = []

        if not self.mock_mode and not self.perplexity_api_key:
            errors.append("PERPLEXITY_API_KEY is required when not in mock mode")

        if not self.repo_path.exists():
            errors.append(f"Repository path does not exist: {self.repo_path}")

        if not self.config_dir.exists():
            errors.append(f"Config directory does not exist: {self.config_dir}")

        return errors

    @property
    def prompts_file(self) -> Path:
        """Path to prompts.yaml file (legacy)."""
        return self.config_dir / "prompts.yaml"

    @property
    def profiles_file(self) -> Path:
        """Path to profiles.yaml file."""
        return self.config_dir / "profiles.yaml"

    @property
    def prompt_library_path(self) -> Path:
        """Path to prompt_library directory with markdown prompts."""
        return self.config_dir / "prompt_library"
</file>

<file path="src/metaagent/orchestrator.py">
"""Orchestrator for the meta-agent refinement pipeline."""

from __future__ import annotations

import json
import logging
import subprocess
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Optional

from .analysis import AnalysisEngine, AnalysisResult, create_analysis_engine
from .codebase_digest import CodebaseDigestRunner, DigestResult
from .config import Config
from .plan_writer import PlanWriter, StageResult
from .prompts import PromptLibrary
from .repomix import RepomixRunner, RepomixResult

logger = logging.getLogger(__name__)


@dataclass
class RunHistory:
    """History of analysis runs for context building."""

    entries: list[dict] = field(default_factory=list)

    def add_entry(self, stage_id: str, summary: str) -> None:
        """Add an entry to the history."""
        self.entries.append(
            {
                "stage": stage_id,
                "summary": summary,
                "timestamp": datetime.now().isoformat(),
            }
        )

    def format_for_prompt(self) -> str:
        """Format history for inclusion in prompts."""
        if not self.entries:
            return "No previous analysis history."

        lines = ["Previous analysis stages:"]
        for entry in self.entries:
            lines.append(f"- [{entry['stage']}]: {entry['summary'][:200]}...")
        return "\n".join(lines)


@dataclass
class TriageResult:
    """Result from the triage step."""

    success: bool
    done: bool = False
    assessment: str = ""
    priority_issues: list[str] = field(default_factory=list)
    selected_prompts: list[str] = field(default_factory=list)
    reasoning: str = ""
    error: Optional[str] = None


@dataclass
class IterationResult:
    """Result from a single iteration of the refinement loop."""

    iteration: int
    prompts_run: list[str]
    changes_made: bool
    committed: bool
    commit_hash: Optional[str] = None
    stage_results: list[StageResult] = field(default_factory=list)


@dataclass
class RefinementResult:
    """Result from a complete refinement run."""

    success: bool
    profile_name: str
    stages_completed: int
    stages_failed: int
    plan_path: Optional[Path] = None
    error: Optional[str] = None
    stage_results: list[StageResult] = field(default_factory=list)
    iterations: list[IterationResult] = field(default_factory=list)


class Orchestrator:
    """Orchestrates the refinement pipeline."""

    def __init__(
        self,
        config: Config,
        prompt_library: Optional[PromptLibrary] = None,
        repomix_runner: Optional[RepomixRunner] = None,
        digest_runner: Optional[CodebaseDigestRunner] = None,
        analysis_engine: Optional[AnalysisEngine] = None,
        plan_writer: Optional[PlanWriter] = None,
    ):
        """Initialize the orchestrator.

        Args:
            config: Configuration settings.
            prompt_library: Optional PromptLibrary instance.
            repomix_runner: Optional RepomixRunner instance.
            digest_runner: Optional CodebaseDigestRunner instance.
            analysis_engine: Optional AnalysisEngine instance.
            plan_writer: Optional PlanWriter instance.
        """
        self.config = config

        # Initialize components with defaults if not provided
        self.prompt_library = prompt_library or PromptLibrary(
            prompts_path=config.prompts_file,
            profiles_path=config.profiles_file,
            prompt_library_path=config.prompt_library_path,
        )

        self.repomix_runner = repomix_runner or RepomixRunner(
            timeout=config.timeout,
            max_chars=config.max_tokens * 4,  # Rough char-to-token ratio
        )

        self.digest_runner = digest_runner or CodebaseDigestRunner(
            max_depth=10,
            output_format="markdown",
            include_content=False,  # We get content from Repomix
        )

        self.analysis_engine = analysis_engine or create_analysis_engine(
            api_key=config.perplexity_api_key,
            mock_mode=config.mock_mode,
            timeout=config.timeout,
        )

        self.plan_writer = plan_writer or PlanWriter(
            output_dir=config.repo_path / "docs",
        )

    def refine(self, max_iterations: int = 10) -> RefinementResult:
        """Run the iterative refinement loop.

        This is the main entry point:
        1. Pack codebase
        2. Triage (AI decides which prompts to run)
        3. Run selected prompts
        4. Implement changes with Claude Code
        5. Commit to GitHub
        6. Repeat until triage says "done" or max iterations reached

        Args:
            max_iterations: Maximum number of iterations to run.

        Returns:
            RefinementResult with the outcome.
        """
        logger.info("Starting iterative refinement loop")

        # Load PRD
        prd_content = self._load_prd()
        if prd_content is None:
            return RefinementResult(
                success=False,
                profile_name="iterative",
                stages_completed=0,
                stages_failed=0,
                error=f"PRD file not found: {self.config.prd_path}",
            )

        history = RunHistory()
        all_stage_results: list[StageResult] = []
        iterations: list[IterationResult] = []
        stages_completed = 0
        stages_failed = 0

        for iteration in range(1, max_iterations + 1):
            logger.info(f"=== Iteration {iteration}/{max_iterations} ===")

            # Step 1: Pack codebase
            logger.info("Step 1: Packing codebase...")
            code_context = self._pack_codebase()

            # Step 2: Triage
            logger.info("Step 2: Running triage...")
            triage_result = self._run_triage(prd_content, code_context, history)

            if not triage_result.success:
                logger.error(f"Triage failed: {triage_result.error}")
                return RefinementResult(
                    success=False,
                    profile_name="iterative",
                    stages_completed=stages_completed,
                    stages_failed=stages_failed,
                    error=f"Triage failed: {triage_result.error}",
                    stage_results=all_stage_results,
                    iterations=iterations,
                )

            if triage_result.done:
                logger.info("Triage says we're done! Codebase meets requirements.")
                break

            logger.info(f"Triage selected prompts: {triage_result.selected_prompts}")
            logger.info(f"Assessment: {triage_result.assessment}")

            # Step 3: Run selected prompts
            logger.info("Step 3: Running selected prompts...")
            iteration_stage_results = []

            for prompt_id in triage_result.selected_prompts:
                prompt = self.prompt_library.get_prompt(prompt_id)
                if not prompt:
                    logger.warning(f"Prompt not found: {prompt_id}")
                    continue

                logger.info(f"Running prompt: {prompt_id}")
                rendered_prompt = prompt.render(
                    prd=prd_content,
                    code_context=code_context,
                    history=history.format_for_prompt(),
                    current_stage=prompt_id,
                )

                analysis_result = self.analysis_engine.analyze(rendered_prompt)

                if analysis_result.success:
                    stages_completed += 1
                    history.add_entry(prompt_id, analysis_result.summary)

                    stage_result = StageResult(
                        stage_id=prompt_id,
                        stage_name=prompt.goal or prompt_id,
                        summary=analysis_result.summary,
                        recommendations=analysis_result.recommendations,
                        tasks=analysis_result.tasks,
                    )
                    iteration_stage_results.append(stage_result)
                    all_stage_results.append(stage_result)
                    logger.info(f"Prompt {prompt_id} completed successfully")
                else:
                    stages_failed += 1
                    logger.error(f"Prompt {prompt_id} failed: {analysis_result.error}")

            # Step 4: Implement changes with Claude Code
            logger.info("Step 4: Implementing changes with Claude Code...")
            changes_made = self._implement_with_claude(iteration_stage_results)

            # Step 5: Commit to GitHub
            commit_hash = None
            if changes_made:
                logger.info("Step 5: Committing changes to GitHub...")
                commit_hash = self._commit_changes(
                    f"Iteration {iteration}: {', '.join(triage_result.selected_prompts)}"
                )

            iterations.append(
                IterationResult(
                    iteration=iteration,
                    prompts_run=triage_result.selected_prompts,
                    changes_made=changes_made,
                    committed=commit_hash is not None,
                    commit_hash=commit_hash,
                    stage_results=iteration_stage_results,
                )
            )

            if not changes_made:
                logger.info("No changes made this iteration, continuing...")

        # Write final plan
        plan_path = None
        if all_stage_results:
            logger.info("Writing final improvement plan...")
            plan_path = self.plan_writer.write_plan(
                prd_content=prd_content,
                profile_name="Iterative Refinement",
                stage_results=all_stage_results,
            )
            logger.info(f"Plan written to: {plan_path}")

        return RefinementResult(
            success=stages_failed == 0 and stages_completed > 0,
            profile_name="iterative",
            stages_completed=stages_completed,
            stages_failed=stages_failed,
            plan_path=plan_path,
            stage_results=all_stage_results,
            iterations=iterations,
        )

    def _pack_codebase(self) -> str:
        """Pack the codebase using both codebase-digest and Repomix.

        Returns:
            Combined code context string.
        """
        # Run codebase-digest for directory tree and metrics
        digest_result = self.digest_runner.analyze(self.config.repo_path)

        if digest_result.success:
            structure_context = self._format_digest_output(digest_result)
        else:
            logger.warning(f"codebase-digest failed: {digest_result.error}")
            structure_context = ""

        # Run Repomix for full file contents
        repomix_result = self.repomix_runner.pack(self.config.repo_path)

        if not repomix_result.success:
            logger.warning(f"Repomix failed: {repomix_result.error}")
            file_contents = f"[Repomix failed: {repomix_result.error}]"
        else:
            file_contents = repomix_result.content
            if repomix_result.truncated:
                logger.warning(
                    f"Codebase was truncated from {repomix_result.original_size} chars"
                )

        return self._build_code_context(structure_context, file_contents)

    def _run_triage(
        self, prd_content: str, code_context: str, history: RunHistory
    ) -> TriageResult:
        """Run triage to determine which prompts to run next.

        Args:
            prd_content: The PRD content.
            code_context: The packed codebase.
            history: Previous analysis history.

        Returns:
            TriageResult with selected prompts or done flag.
        """
        triage_prompt = self.prompt_library.get_prompt("meta_triage")
        if not triage_prompt:
            return TriageResult(
                success=False,
                error="Triage prompt (meta_triage) not found in prompt library",
            )

        rendered_prompt = triage_prompt.render(
            prd=prd_content,
            code_context=code_context,
            history=history.format_for_prompt(),
        )

        analysis_result = self.analysis_engine.analyze(rendered_prompt)

        if not analysis_result.success:
            return TriageResult(success=False, error=analysis_result.error)

        # Parse the triage response
        try:
            # Try to extract JSON from the response
            response_text = analysis_result.summary

            # Look for JSON in the response
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1

            if json_start != -1 and json_end > json_start:
                json_str = response_text[json_start:json_end]
                data = json.loads(json_str)

                return TriageResult(
                    success=True,
                    done=data.get("done", False),
                    assessment=data.get("assessment", ""),
                    priority_issues=data.get("priority_issues", []),
                    selected_prompts=data.get("selected_prompts", []),
                    reasoning=data.get("reasoning", ""),
                )
            else:
                # No JSON found, try to parse as plain text
                # If response contains "done" or similar, mark as done
                if "done" in response_text.lower() and "no further" in response_text.lower():
                    return TriageResult(
                        success=True,
                        done=True,
                        assessment=response_text,
                    )

                return TriageResult(
                    success=False,
                    error=f"Could not parse triage response: {response_text[:200]}",
                )

        except json.JSONDecodeError as e:
            return TriageResult(
                success=False,
                error=f"Failed to parse triage JSON: {e}",
            )

    def _implement_with_claude(self, stage_results: list[StageResult]) -> bool:
        """Implement changes using Claude Code.

        Args:
            stage_results: Results from the analysis stage.

        Returns:
            True if changes were made, False otherwise.
        """
        if not stage_results:
            return False

        # Build implementation prompt for Claude Code
        tasks = []
        for result in stage_results:
            for task in result.tasks:
                tasks.append(task)

        if not tasks:
            logger.info("No tasks to implement")
            return False

        # Create a prompt for Claude Code
        implementation_prompt = "Please implement the following improvements:\n\n"
        for i, task in enumerate(tasks, 1):
            if isinstance(task, dict):
                implementation_prompt += f"{i}. {task.get('description', task)}\n"
            else:
                implementation_prompt += f"{i}. {task}\n"

        # Write the prompt to a file for Claude Code to read
        prompt_file = self.config.repo_path / ".meta-agent-tasks.md"
        prompt_file.write_text(implementation_prompt, encoding="utf-8")

        logger.info(f"Implementation tasks written to: {prompt_file}")
        logger.info("Please run Claude Code to implement these changes.")

        # In a fully automated setup, we would call Claude Code here
        # For now, we just write the tasks and let the user run Claude Code
        # TODO: Integrate with Claude Code CLI when available

        return True

    def _commit_changes(self, message: str) -> Optional[str]:
        """Commit changes to git.

        Args:
            message: Commit message.

        Returns:
            Commit hash if successful, None otherwise.
        """
        try:
            # Check if there are changes to commit
            status_result = subprocess.run(
                ["git", "status", "--porcelain"],
                cwd=self.config.repo_path,
                capture_output=True,
                text=True,
            )

            if not status_result.stdout.strip():
                logger.info("No changes to commit")
                return None

            # Stage all changes
            subprocess.run(
                ["git", "add", "-A"],
                cwd=self.config.repo_path,
                check=True,
            )

            # Commit
            commit_message = f"meta-agent: {message}\n\n🤖 Generated with meta-agent"
            subprocess.run(
                ["git", "commit", "-m", commit_message],
                cwd=self.config.repo_path,
                check=True,
            )

            # Get commit hash
            hash_result = subprocess.run(
                ["git", "rev-parse", "HEAD"],
                cwd=self.config.repo_path,
                capture_output=True,
                text=True,
            )

            commit_hash = hash_result.stdout.strip()[:8]
            logger.info(f"Committed changes: {commit_hash}")

            # Push to remote
            subprocess.run(
                ["git", "push"],
                cwd=self.config.repo_path,
                check=True,
            )
            logger.info("Pushed to remote")

            return commit_hash

        except subprocess.CalledProcessError as e:
            logger.error(f"Git operation failed: {e}")
            return None

    def _load_prd(self) -> Optional[str]:
        """Load the PRD file content.

        Returns:
            PRD content string or None if not found.
        """
        prd_path = self.config.prd_path
        if not prd_path or not prd_path.exists():
            return None

        return prd_path.read_text(encoding="utf-8")

    def _format_digest_output(self, digest_result: DigestResult) -> str:
        """Format codebase-digest output for inclusion in prompts.

        Args:
            digest_result: Result from codebase-digest analysis.

        Returns:
            Formatted string with directory tree and metrics.
        """
        sections = []

        if digest_result.tree:
            sections.append("## Directory Structure")
            sections.append(digest_result.tree)

        if digest_result.metrics:
            sections.append("\n## Codebase Metrics")
            sections.append(digest_result.metrics)

        return "\n".join(sections) if sections else ""

    def _build_code_context(self, structure_context: str, file_contents: str) -> str:
        """Build comprehensive code context from both tools.

        Args:
            structure_context: Directory tree and metrics from codebase-digest.
            file_contents: Full file contents from Repomix.

        Returns:
            Combined code context string.
        """
        sections = []

        if structure_context:
            sections.append("# Codebase Overview (from codebase-digest)")
            sections.append(structure_context)
            sections.append("\n---\n")

        if file_contents and not file_contents.startswith("["):
            sections.append("# File Contents (from Repomix)")
            sections.append(file_contents)
        elif file_contents:
            # Repomix failed, include the error message
            sections.append(file_contents)

        if not sections:
            return "[No codebase context available - both codebase-digest and Repomix failed]"

        return "\n".join(sections)
</file>

<file path="src/metaagent/plan_writer.py">
"""Plan writer for generating improvement plan documents."""

from __future__ import annotations

from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any


@dataclass
class StageResult:
    """Result from a single analysis stage."""

    stage_id: str
    stage_name: str
    summary: str
    recommendations: list[str] = field(default_factory=list)
    tasks: list[dict[str, Any]] = field(default_factory=list)


class PlanWriter:
    """Writes aggregated analysis results to an improvement plan document."""

    def __init__(self, output_dir: Path):
        """Initialize the plan writer.

        Args:
            output_dir: Directory to write the plan file to.
        """
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def write_plan(
        self,
        prd_content: str,
        profile_name: str,
        stage_results: list[StageResult],
        output_filename: str = "mvp_improvement_plan.md",
    ) -> Path:
        """Write the improvement plan to a markdown file.

        Args:
            prd_content: Original PRD content for summary.
            profile_name: Name of the profile used.
            stage_results: List of StageResult from each analysis stage.
            output_filename: Name of the output file.

        Returns:
            Path to the written file.
        """
        output_path = self.output_dir / output_filename

        sections = [
            self._generate_header(profile_name),
            self._generate_prd_summary(prd_content),
            self._generate_stage_summaries(stage_results),
            self._generate_task_list(stage_results),
            self._generate_instructions(),
        ]

        content = "\n\n".join(sections)

        output_path.write_text(content, encoding="utf-8")
        return output_path

    def _generate_header(self, profile_name: str) -> str:
        """Generate the document header."""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return f"""# MVP Improvement Plan

**Generated:** {timestamp}
**Profile:** {profile_name}
**Status:** Ready for implementation

---"""

    def _generate_prd_summary(self, prd_content: str) -> str:
        """Generate a summary of the PRD."""
        summary = self._extract_prd_summary(prd_content)
        return f"""## PRD Summary

{summary}"""

    def _extract_prd_summary(self, prd_content: str, max_lines: int = 20) -> str:
        """Extract a summary from the PRD content.

        Args:
            prd_content: Full PRD content.
            max_lines: Maximum lines to include.

        Returns:
            Summarized PRD content.
        """
        lines = prd_content.strip().split("\n")
        if len(lines) <= max_lines:
            return prd_content.strip()

        # Take first max_lines and add truncation notice
        summary_lines = lines[:max_lines]
        return "\n".join(summary_lines) + "\n\n*[PRD truncated for brevity]*"

    def _generate_stage_summaries(self, stage_results: list[StageResult]) -> str:
        """Generate summaries for each analysis stage."""
        if not stage_results:
            return "## Analysis Stages\n\n*No stages were executed.*"

        sections = ["## Analysis Stages"]

        for result in stage_results:
            section = f"""### {result.stage_name}

{result.summary}"""

            if result.recommendations:
                section += "\n\n**Recommendations:**\n"
                for rec in result.recommendations:
                    section += f"- {rec}\n"

            sections.append(section)

        return "\n\n".join(sections)

    def _generate_task_list(self, stage_results: list[StageResult]) -> str:
        """Generate the aggregated and prioritized task list."""
        all_tasks = self._aggregate_tasks(stage_results)

        if not all_tasks:
            return "## Implementation Tasks\n\n*No tasks were identified.*"

        sections = ["## Implementation Tasks\n"]

        # Group by priority
        priority_order = ["critical", "high", "medium", "low"]
        tasks_by_priority: dict[str, list[dict]] = {p: [] for p in priority_order}

        for task in all_tasks:
            priority = task.get("priority", "medium").lower()
            if priority not in tasks_by_priority:
                priority = "medium"
            tasks_by_priority[priority].append(task)

        for priority in priority_order:
            tasks = tasks_by_priority[priority]
            if not tasks:
                continue

            badge = self._priority_badge(priority)
            sections.append(f"### {badge} {priority.capitalize()} Priority\n")

            for task in tasks:
                title = task.get("title", "Untitled task")
                description = task.get("description", "")
                file_ref = task.get("file", "")

                task_line = f"- [ ] **{title}**"
                if file_ref:
                    task_line += f" (`{file_ref}`)"
                if description:
                    task_line += f"\n  - {description}"

                sections.append(task_line)

            sections.append("")  # Add spacing between priority groups

        return "\n".join(sections)

    def _aggregate_tasks(self, stage_results: list[StageResult]) -> list[dict]:
        """Aggregate tasks from all stages, removing duplicates."""
        all_tasks = []
        seen_titles = set()

        for result in stage_results:
            for task in result.tasks:
                title = task.get("title", "")
                if title and title not in seen_titles:
                    seen_titles.add(title)
                    # Add stage info to task
                    task["stage"] = result.stage_id
                    all_tasks.append(task)

        return all_tasks

    def _priority_badge(self, priority: str) -> str:
        """Get an emoji badge for the priority level."""
        badges = {
            "critical": "[CRITICAL]",
            "high": "[HIGH]",
            "medium": "[MEDIUM]",
            "low": "[LOW]",
        }
        return badges.get(priority.lower(), "[MEDIUM]")

    def _generate_instructions(self) -> str:
        """Generate instructions for using the plan with Claude Code."""
        return """---

## Instructions for Claude Code

To implement this plan, open Claude Code in the repository and use the following prompt:

```
Read docs/mvp_improvement_plan.md and implement the tasks in order of priority.
For each task:
1. Understand the requirement
2. Make the necessary code changes
3. Run relevant tests
4. Mark the checkbox as complete when done

Start with the highest priority tasks first.
```

### Implementation Notes

- Work through tasks systematically, starting with Critical/High priority
- Run tests after each significant change
- Commit changes incrementally with descriptive messages
- If a task is unclear, review the relevant stage summary above for context
"""
</file>

<file path="src/metaagent/prompts.py">
"""Prompt and profile loading for meta-agent."""

from __future__ import annotations

import re
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Optional

import yaml
from jinja2 import Template


@dataclass
class Prompt:
    """A prompt template with metadata."""

    id: str
    goal: str
    template: str
    stage: str
    dependencies: list[str] = field(default_factory=list)
    when_to_use: Optional[str] = None
    category: Optional[str] = None

    def render(
        self,
        prd: str = "",
        code_context: str = "",
        history: str = "",
        current_stage: str = "",
    ) -> str:
        """Render the prompt template with variables.

        Args:
            prd: The PRD content.
            code_context: The packed codebase content.
            history: Previous analysis summaries.
            current_stage: Current stage name.

        Returns:
            Rendered prompt string.
        """
        # Build the full prompt with context
        full_prompt = self.template

        # Add context sections if provided
        context_sections = []
        if prd:
            context_sections.append(f"## Product Requirements Document (PRD)\n\n{prd}")
        if code_context:
            context_sections.append(f"## Codebase\n\n{code_context}")
        if history:
            context_sections.append(f"## Previous Analysis\n\n{history}")

        if context_sections:
            full_prompt = full_prompt + "\n\n---\n\n" + "\n\n---\n\n".join(context_sections)

        return full_prompt


@dataclass
class Profile:
    """A profile defining which stages to run."""

    name: str
    description: str
    stages: list[str]


class PromptLibrary:
    """Manages loading and accessing prompts and profiles."""

    def __init__(
        self,
        prompts_path: Optional[Path] = None,
        profiles_path: Optional[Path] = None,
        prompt_library_path: Optional[Path] = None,
    ):
        """Initialize the prompt library.

        Args:
            prompts_path: Path to prompts.yaml file (legacy, optional).
            profiles_path: Path to profiles.yaml file.
            prompt_library_path: Path to directory containing markdown prompts.
        """
        self.prompts_path = prompts_path
        self.profiles_path = profiles_path
        self.prompt_library_path = prompt_library_path
        self._prompts: dict[str, Prompt] = {}
        self._profiles: dict[str, Profile] = {}
        self._loaded = False

    def load(self) -> None:
        """Load prompts and profiles from files."""
        if self._loaded:
            return

        self._load_prompts()
        self._load_profiles()
        self._loaded = True

    def _load_prompts(self) -> None:
        """Load prompts from markdown files and optional YAML."""
        # Load from markdown prompt library (primary source)
        if self.prompt_library_path and self.prompt_library_path.exists():
            self._load_markdown_prompts()

        # Also load from YAML if provided (for backwards compatibility)
        if self.prompts_path and self.prompts_path.exists():
            self._load_yaml_prompts()

    def _load_markdown_prompts(self) -> None:
        """Load prompts from markdown files in prompt_library directory."""
        if not self.prompt_library_path:
            return

        for md_file in self.prompt_library_path.glob("*.md"):
            prompt = self._parse_markdown_prompt(md_file)
            if prompt:
                self._prompts[prompt.id] = prompt

    def _parse_markdown_prompt(self, file_path: Path) -> Optional[Prompt]:
        """Parse a markdown file into a Prompt object.

        Args:
            file_path: Path to the markdown file.

        Returns:
            Prompt object or None if parsing fails.
        """
        try:
            content = file_path.read_text(encoding="utf-8")
            prompt_id = file_path.stem  # filename without extension

            # Extract title (first heading) as the goal
            title_match = re.search(r"^#\s+(.+)$", content, re.MULTILINE)
            goal = title_match.group(1).strip() if title_match else prompt_id.replace("_", " ").title()

            # Extract category from filename prefix
            category = None
            if "_" in prompt_id:
                category = prompt_id.split("_")[0]

            # Determine stage from category
            stage_mapping = {
                "architecture": "architecture",
                "quality": "quality",
                "performance": "performance",
                "security": "security",
                "testing": "testing",
                "evolution": "evolution",
                "improvement": "improvement",
                "learning": "learning",
                "business": "business",
            }
            stage = stage_mapping.get(category, "analysis")

            return Prompt(
                id=prompt_id,
                goal=goal,
                template=content,
                stage=stage,
                category=category,
            )
        except Exception:
            return None

    def _load_yaml_prompts(self) -> None:
        """Load prompts from YAML file (legacy support)."""
        if not self.prompts_path:
            return

        with open(self.prompts_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}

        prompts_data = data.get("prompts", {})
        for prompt_id, prompt_data in prompts_data.items():
            self._prompts[prompt_id] = Prompt(
                id=prompt_id,
                goal=prompt_data.get("goal", ""),
                template=prompt_data.get("template", ""),
                stage=prompt_data.get("stage", ""),
                dependencies=prompt_data.get("dependencies", []),
                when_to_use=prompt_data.get("when_to_use"),
            )

    def _load_profiles(self) -> None:
        """Load profiles from YAML file."""
        if not self.profiles_path or not self.profiles_path.exists():
            return  # No profiles file, just skip

        with open(self.profiles_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}

        profiles_data = data.get("profiles", {})
        for profile_id, profile_data in profiles_data.items():
            self._profiles[profile_id] = Profile(
                name=profile_data.get("name", profile_id),
                description=profile_data.get("description", ""),
                stages=profile_data.get("stages", []),
            )

    def get_prompt(self, prompt_id: str) -> Optional[Prompt]:
        """Get a prompt by ID.

        Args:
            prompt_id: The prompt identifier.

        Returns:
            Prompt instance or None if not found.
        """
        self.load()
        return self._prompts.get(prompt_id)

    def get_profile(self, profile_id: str) -> Optional[Profile]:
        """Get a profile by ID.

        Args:
            profile_id: The profile identifier.

        Returns:
            Profile instance or None if not found.
        """
        self.load()
        return self._profiles.get(profile_id)

    def list_profiles(self) -> list[Profile]:
        """Get all available profiles.

        Returns:
            List of Profile instances.
        """
        self.load()
        return list(self._profiles.values())

    def list_prompts(self) -> list[Prompt]:
        """Get all available prompts.

        Returns:
            List of Prompt instances.
        """
        self.load()
        return list(self._prompts.values())

    def list_prompts_by_category(self) -> dict[str, list[Prompt]]:
        """Get all prompts organized by category.

        Returns:
            Dict mapping category names to lists of Prompts.
        """
        self.load()
        by_category: dict[str, list[Prompt]] = {}
        for prompt in self._prompts.values():
            cat = prompt.category or "other"
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(prompt)
        return by_category

    def get_prompts_for_profile(self, profile_id: str) -> list[Prompt]:
        """Get all prompts for a profile's stages in order.

        Args:
            profile_id: The profile identifier.

        Returns:
            Ordered list of Prompt instances for the profile's stages.
        """
        self.load()
        profile = self._profiles.get(profile_id)
        if not profile:
            return []

        prompts = []
        for stage in profile.stages:
            prompt = self._prompts.get(stage)
            if prompt:
                prompts.append(prompt)
        return prompts
</file>

<file path="src/metaagent/repomix.py">
"""Repomix integration for codebase packing."""

from __future__ import annotations

import subprocess
import tempfile
from dataclasses import dataclass
from pathlib import Path
from typing import Optional


@dataclass
class RepomixResult:
    """Result from running Repomix on a repository."""

    content: str
    success: bool
    error: Optional[str] = None
    truncated: bool = False
    original_size: int = 0


class RepomixRunner:
    """Runs Repomix to pack a codebase into a single file."""

    def __init__(self, timeout: int = 120, max_chars: int = 400000):
        """Initialize the Repomix runner.

        Args:
            timeout: Timeout in seconds for Repomix execution.
            max_chars: Maximum characters to keep (approximate token budget).
        """
        self.timeout = timeout
        self.max_chars = max_chars

    def pack(self, repo_path: Path) -> RepomixResult:
        """Pack a repository using Repomix.

        Args:
            repo_path: Path to the repository to pack.

        Returns:
            RepomixResult with packed content or error information.
        """
        if not repo_path.exists():
            return RepomixResult(
                content="",
                success=False,
                error=f"Repository path does not exist: {repo_path}",
            )

        try:
            with tempfile.NamedTemporaryFile(
                mode="w", suffix=".md", delete=False
            ) as tmp_file:
                output_path = Path(tmp_file.name)

            try:
                # Try repomix directly first (if installed globally)
                # Fall back to npx repomix if direct call fails
                # On Windows, use shell=True to properly resolve PATH
                import platform
                use_shell = platform.system() == "Windows"

                cmd_options = [
                    "repomix --output {} --style markdown".format(str(output_path)),
                    "npx repomix --output {} --style markdown".format(str(output_path)),
                ]

                result = None
                last_error = None

                for cmd in cmd_options:
                    try:
                        result = subprocess.run(
                            cmd,
                            cwd=repo_path,
                            capture_output=True,
                            text=True,
                            timeout=self.timeout,
                            shell=use_shell,
                        )
                        if result.returncode == 0:
                            break
                        last_error = result.stderr
                    except FileNotFoundError:
                        last_error = f"Command not found: {cmd.split()[0]}"
                        continue

                if result is None or result.returncode != 0:
                    return RepomixResult(
                        content="",
                        success=False,
                        error=f"Repomix failed: {last_error}",
                    )

                content = output_path.read_text(encoding="utf-8")
                original_size = len(content)
                truncated = False

                if len(content) > self.max_chars:
                    content = self._truncate_content(content)
                    truncated = True

                return RepomixResult(
                    content=content,
                    success=True,
                    truncated=truncated,
                    original_size=original_size,
                )

            finally:
                output_path.unlink(missing_ok=True)

        except subprocess.TimeoutExpired:
            return RepomixResult(
                content="",
                success=False,
                error=f"Repomix timed out after {self.timeout} seconds",
            )
        except FileNotFoundError:
            return RepomixResult(
                content="",
                success=False,
                error="Repomix not found. Install with: npm install -g repomix",
            )
        except Exception as e:
            return RepomixResult(
                content="",
                success=False,
                error=f"Unexpected error running Repomix: {e}",
            )

    def _truncate_content(self, content: str) -> str:
        """Truncate content to fit within max_chars while preserving structure.

        Args:
            content: The content to truncate.

        Returns:
            Truncated content with a notice appended.
        """
        # Try to truncate at a sensible boundary
        truncated = content[: self.max_chars]

        # Find the last complete line
        last_newline = truncated.rfind("\n")
        if last_newline > self.max_chars * 0.8:
            truncated = truncated[:last_newline]

        truncated += "\n\n[... content truncated due to size limits ...]"
        return truncated
</file>

<file path="tests/__init__.py">
"""Tests for meta-agent."""
</file>

<file path="tests/conftest.py">
"""Shared test fixtures for meta-agent tests."""

from __future__ import annotations

from pathlib import Path
from typing import Generator

import pytest

from metaagent.config import Config
from metaagent.prompts import PromptLibrary


@pytest.fixture
def temp_repo(tmp_path: Path) -> Path:
    """Create a temporary repository structure for testing."""
    # Create directory structure
    (tmp_path / "src").mkdir()
    (tmp_path / "tests").mkdir()
    (tmp_path / "docs").mkdir()
    (tmp_path / "config").mkdir()

    # Create a sample PRD
    prd_content = """# Sample PRD

## Overview
This is a test PRD for unit testing.

## Requirements
- Requirement 1: Do something
- Requirement 2: Do something else
"""
    (tmp_path / "docs" / "prd.md").write_text(prd_content)

    # Create sample source file
    (tmp_path / "src" / "main.py").write_text('print("Hello, World!")\n')

    return tmp_path


@pytest.fixture
def sample_prompts_yaml(tmp_path: Path) -> Path:
    """Create a sample prompts.yaml file."""
    prompts_content = """prompts:
  test_prompt:
    id: test_prompt
    goal: "Test prompt for unit testing"
    stage: testing
    template: |
      PRD: {{ prd }}
      Code: {{ code_context }}
      History: {{ history }}
      Stage: {{ current_stage }}
"""
    prompts_path = tmp_path / "prompts.yaml"
    prompts_path.write_text(prompts_content)
    return prompts_path


@pytest.fixture
def sample_profiles_yaml(tmp_path: Path) -> Path:
    """Create a sample profiles.yaml file."""
    profiles_content = """profiles:
  test_profile:
    name: "Test Profile"
    description: "A test profile"
    stages:
      - test_prompt
"""
    profiles_path = tmp_path / "profiles.yaml"
    profiles_path.write_text(profiles_content)
    return profiles_path


@pytest.fixture
def prompt_library(sample_prompts_yaml: Path, sample_profiles_yaml: Path) -> PromptLibrary:
    """Create a PromptLibrary with sample data."""
    library = PromptLibrary(sample_prompts_yaml, sample_profiles_yaml)
    library.load()
    return library


@pytest.fixture
def mock_config(temp_repo: Path, sample_prompts_yaml: Path, sample_profiles_yaml: Path) -> Config:
    """Create a mock configuration for testing."""
    # Copy config files to temp repo
    config_dir = temp_repo / "config"
    (config_dir / "prompts.yaml").write_text(sample_prompts_yaml.read_text())
    (config_dir / "profiles.yaml").write_text(sample_profiles_yaml.read_text())

    return Config(
        perplexity_api_key="test-key",
        anthropic_api_key="test-key",
        repo_path=temp_repo,
        config_dir=config_dir,
        prd_path=temp_repo / "docs" / "prd.md",
        mock_mode=True,
    )
</file>

<file path="tests/test_analysis.py">
"""Tests for analysis engine."""

from __future__ import annotations

import json

import pytest

from metaagent.analysis import (
    AnalysisResult,
    MockAnalysisEngine,
    PerplexityAnalysisEngine,
    create_analysis_engine,
)


class TestMockAnalysisEngine:
    """Tests for MockAnalysisEngine."""

    def test_analyze_returns_result(self) -> None:
        """Test that analyze returns a valid result."""
        engine = MockAnalysisEngine()
        result = engine.analyze("Test prompt")

        assert isinstance(result, AnalysisResult)
        assert result.success is True
        assert result.summary != ""
        assert len(result.recommendations) > 0
        assert len(result.tasks) > 0

    def test_tracks_call_count(self) -> None:
        """Test that call count is tracked."""
        engine = MockAnalysisEngine()

        assert engine.call_count == 0

        engine.analyze("First call")
        assert engine.call_count == 1

        engine.analyze("Second call")
        assert engine.call_count == 2

    def test_stores_last_prompt(self) -> None:
        """Test that last prompt is stored."""
        engine = MockAnalysisEngine()
        engine.analyze("My test prompt")

        assert engine.last_prompt == "My test prompt"

    def test_predefined_responses(self) -> None:
        """Test using predefined responses."""
        custom_result = AnalysisResult(
            summary="Custom summary",
            recommendations=["Custom rec"],
            tasks=[{"title": "Custom task"}],
        )

        engine = MockAnalysisEngine(responses={"keyword": custom_result})
        result = engine.analyze("Prompt with keyword in it")

        assert result.summary == "Custom summary"


class TestPerplexityAnalysisEngine:
    """Tests for PerplexityAnalysisEngine."""

    def test_parse_response_json(self) -> None:
        """Test parsing a JSON response."""
        engine = PerplexityAnalysisEngine(api_key="test")

        content = json.dumps(
            {
                "summary": "Test summary",
                "recommendations": ["rec1", "rec2"],
                "tasks": [{"title": "task1"}],
            }
        )

        result = engine._parse_response(content)

        assert result.summary == "Test summary"
        assert result.recommendations == ["rec1", "rec2"]
        assert len(result.tasks) == 1

    def test_parse_response_json_block(self) -> None:
        """Test parsing a JSON block in markdown."""
        engine = PerplexityAnalysisEngine(api_key="test")

        content = """Here's my analysis:

```json
{
    "summary": "Block summary",
    "recommendations": ["rec"],
    "tasks": []
}
```

Additional notes here."""

        result = engine._parse_response(content)

        assert result.summary == "Block summary"

    def test_parse_response_fallback(self) -> None:
        """Test fallback when JSON parsing fails."""
        engine = PerplexityAnalysisEngine(api_key="test")

        content = "This is not JSON, just plain text analysis."

        result = engine._parse_response(content)

        assert result.summary == content
        assert result.recommendations == []
        assert result.tasks == []


class TestCreateAnalysisEngine:
    """Tests for the factory function."""

    def test_create_mock_engine(self) -> None:
        """Test creating a mock engine."""
        engine = create_analysis_engine(mock_mode=True)

        assert isinstance(engine, MockAnalysisEngine)

    def test_create_perplexity_engine(self) -> None:
        """Test creating a Perplexity engine."""
        engine = create_analysis_engine(api_key="test-key", mock_mode=False)

        assert isinstance(engine, PerplexityAnalysisEngine)

    def test_requires_api_key(self) -> None:
        """Test that API key is required when not in mock mode."""
        with pytest.raises(ValueError, match="API key is required"):
            create_analysis_engine(mock_mode=False)
</file>

<file path="tests/test_cli.py">
"""Tests for CLI entrypoint."""

from __future__ import annotations

from pathlib import Path
from unittest.mock import patch

import pytest
from typer.testing import CliRunner

from metaagent.cli import app

runner = CliRunner()


class TestCLI:
    """Tests for CLI commands."""

    def test_version(self) -> None:
        """Test --version flag."""
        result = runner.invoke(app, ["--version"])

        assert result.exit_code == 0
        assert "metaagent version" in result.stdout

    def test_help(self) -> None:
        """Test --help flag."""
        result = runner.invoke(app, ["--help"])

        assert result.exit_code == 0
        assert "Meta-agent for automated codebase refinement" in result.stdout

    def test_refine_help(self) -> None:
        """Test refine command help."""
        result = runner.invoke(app, ["refine", "--help"])

        assert result.exit_code == 0
        assert "--repo" in result.stdout
        assert "--mock" in result.stdout
        assert "--max-iterations" in result.stdout
        assert "iterative refinement" in result.stdout.lower()

    def test_refine_invalid_repo(self, tmp_path: Path) -> None:
        """Test refine command with invalid repository path."""
        result = runner.invoke(
            app,
            ["refine", "--repo", str(tmp_path / "nonexistent")],
        )

        assert result.exit_code != 0
        output = result.stdout + (result.output if hasattr(result, 'output') else "")
        assert "does not exist" in output

    def test_refine_missing_config(self, tmp_path: Path) -> None:
        """Test refine command with missing config directory."""
        result = runner.invoke(
            app,
            ["refine", "--repo", str(tmp_path)],
        )

        assert result.exit_code != 0

    def test_list_profiles_missing_config(self, tmp_path: Path) -> None:
        """Test list-profiles with missing config."""
        result = runner.invoke(
            app,
            ["list-profiles", "--config-dir", str(tmp_path / "nonexistent")],
        )

        assert result.exit_code != 0

    def test_refine_mock_mode(self, mock_config, tmp_path: Path) -> None:
        """Test refine command in mock mode."""
        repo_path = mock_config.repo_path

        # Mock the orchestrator to avoid actual execution
        with patch("metaagent.cli.Orchestrator") as mock_orch:
            from metaagent.orchestrator import RefinementResult
            mock_instance = mock_orch.return_value
            mock_instance.refine.return_value = RefinementResult(
                success=True,
                profile_name="iterative",
                stages_completed=1,
                stages_failed=0,
                iterations=[],
            )

            result = runner.invoke(
                app,
                [
                    "refine",
                    "--repo",
                    str(repo_path),
                    "--mock",
                ],
            )

            # Should succeed with mock mode
            assert result.exit_code == 0 or "completed" in result.stdout.lower()

    def test_list_profiles(self, mock_config) -> None:
        """Test list-profiles command."""
        config_dir = mock_config.config_dir

        result = runner.invoke(
            app,
            ["list-profiles", "--config-dir", str(config_dir)],
        )

        assert result.exit_code == 0
        assert "Test Profile" in result.stdout

    def test_list_prompts(self, mock_config) -> None:
        """Test list-prompts command."""
        config_dir = mock_config.config_dir

        result = runner.invoke(
            app,
            ["list-prompts", "--config-dir", str(config_dir)],
        )

        # Should work even with empty prompt library
        assert result.exit_code == 0
</file>

<file path="tests/test_codebase_digest.py">
"""Tests for codebase-digest integration."""

from __future__ import annotations

from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

from metaagent.codebase_digest import (
    CodebaseDigestRunner,
    DigestResult,
    get_codebase_context,
)


class TestCodebaseDigestRunner:
    """Tests for CodebaseDigestRunner."""

    def test_init_defaults(self) -> None:
        """Test default initialization."""
        runner = CodebaseDigestRunner()

        assert runner.max_depth is None
        assert runner.output_format == "markdown"
        assert runner.include_content is False

    def test_init_custom(self) -> None:
        """Test custom initialization."""
        runner = CodebaseDigestRunner(
            max_depth=5,
            output_format="json",
            include_content=True,
            max_size_kb=1000,
        )

        assert runner.max_depth == 5
        assert runner.output_format == "json"
        assert runner.include_content is True
        assert runner.max_size_kb == 1000

    def test_analyze_nonexistent_path(self, tmp_path: Path) -> None:
        """Test analyzing a non-existent path."""
        runner = CodebaseDigestRunner()
        result = runner.analyze(tmp_path / "nonexistent")

        assert not result.success
        assert "does not exist" in result.error

    @patch("metaagent.codebase_digest.subprocess.run")
    def test_analyze_success(self, mock_run: MagicMock, tmp_path: Path) -> None:
        """Test successful analysis with mocked subprocess."""
        # Create temp dir
        (tmp_path / "src").mkdir()
        (tmp_path / "src" / "main.py").write_text("print('hello')")

        # Mock subprocess result
        mock_run.return_value = MagicMock(
            returncode=0,
            stdout="## Directory Structure\n```\nsrc/\n  main.py\n```",
            stderr="",
        )

        runner = CodebaseDigestRunner()
        result = runner.analyze(tmp_path)

        # Should call subprocess
        assert mock_run.called

    @patch("metaagent.codebase_digest.subprocess.run")
    def test_analyze_failure(self, mock_run: MagicMock, tmp_path: Path) -> None:
        """Test handling of subprocess failure."""
        mock_run.return_value = MagicMock(
            returncode=1,
            stdout="",
            stderr="Error: something went wrong",
        )

        runner = CodebaseDigestRunner()
        result = runner.analyze(tmp_path)

        assert not result.success
        assert "failed" in result.error.lower()

    @patch("metaagent.codebase_digest.subprocess.run")
    def test_analyze_timeout(self, mock_run: MagicMock, tmp_path: Path) -> None:
        """Test handling of subprocess timeout."""
        import subprocess

        mock_run.side_effect = subprocess.TimeoutExpired(cmd="cdigest", timeout=60)

        runner = CodebaseDigestRunner()
        result = runner.analyze(tmp_path)

        assert not result.success
        assert "timed out" in result.error.lower()

    @patch("metaagent.codebase_digest.subprocess.run")
    def test_analyze_not_found(self, mock_run: MagicMock, tmp_path: Path) -> None:
        """Test handling when codebase-digest is not installed."""
        mock_run.side_effect = FileNotFoundError()

        runner = CodebaseDigestRunner()
        result = runner.analyze(tmp_path)

        assert not result.success
        assert "not found" in result.error.lower()

    def test_get_extension(self) -> None:
        """Test file extension mapping."""
        runner = CodebaseDigestRunner(output_format="markdown")
        assert runner._get_extension() == ".md"

        runner = CodebaseDigestRunner(output_format="json")
        assert runner._get_extension() == ".json"

        runner = CodebaseDigestRunner(output_format="text")
        assert runner._get_extension() == ".txt"

    def test_parse_output_markdown(self) -> None:
        """Test parsing markdown output."""
        runner = CodebaseDigestRunner(output_format="markdown")

        content = """## Directory Structure
```
src/
  main.py
```

## Statistics
Files: 10
Lines: 500
"""
        tree, metrics = runner._parse_output(content)

        assert "Directory Structure" in tree or "src/" in tree


class TestGetCodebaseContext:
    """Tests for get_codebase_context helper function."""

    @patch("metaagent.codebase_digest.CodebaseDigestRunner.analyze")
    def test_success(self, mock_analyze: MagicMock, tmp_path: Path) -> None:
        """Test successful context retrieval."""
        mock_analyze.return_value = DigestResult(
            tree="src/\n  main.py",
            metrics="Files: 1",
            content="",
            success=True,
        )

        result = get_codebase_context(tmp_path)

        assert "Directory Structure" in result
        assert "Codebase Metrics" in result

    @patch("metaagent.codebase_digest.CodebaseDigestRunner.analyze")
    def test_failure(self, mock_analyze: MagicMock, tmp_path: Path) -> None:
        """Test handling of analysis failure."""
        mock_analyze.return_value = DigestResult(
            tree="",
            metrics="",
            content="",
            success=False,
            error="Test error",
        )

        result = get_codebase_context(tmp_path)

        assert "failed" in result.lower()
        assert "Test error" in result
</file>

<file path="tests/test_config.py">
"""Tests for configuration management."""

from __future__ import annotations

import os
from pathlib import Path

import pytest

from metaagent.config import Config


class TestConfig:
    """Tests for Config class."""

    def test_from_env_defaults(self, monkeypatch: pytest.MonkeyPatch, tmp_path: Path) -> None:
        """Test Config.from_env with default values."""
        # Clear environment - including any loaded from .env
        monkeypatch.delenv("PERPLEXITY_API_KEY", raising=False)
        monkeypatch.delenv("ANTHROPIC_API_KEY", raising=False)
        monkeypatch.delenv("METAAGENT_TIMEOUT", raising=False)
        monkeypatch.delenv("METAAGENT_LOG_LEVEL", raising=False)
        monkeypatch.delenv("METAAGENT_MOCK_MODE", raising=False)

        # Also patch load_dotenv to prevent loading from .env file
        monkeypatch.setattr("metaagent.config.load_dotenv", lambda: None)

        config = Config.from_env(tmp_path)

        assert config.perplexity_api_key is None
        assert config.anthropic_api_key is None
        assert config.timeout == 120
        assert config.log_level == "INFO"
        assert config.mock_mode is False

    def test_from_env_with_values(self, monkeypatch: pytest.MonkeyPatch, tmp_path: Path) -> None:
        """Test Config.from_env with environment values."""
        monkeypatch.setenv("PERPLEXITY_API_KEY", "test-perplexity-key")
        monkeypatch.setenv("ANTHROPIC_API_KEY", "test-anthropic-key")
        monkeypatch.setenv("METAAGENT_TIMEOUT", "60")
        monkeypatch.setenv("METAAGENT_MOCK_MODE", "true")

        config = Config.from_env(tmp_path)

        assert config.perplexity_api_key == "test-perplexity-key"
        assert config.anthropic_api_key == "test-anthropic-key"
        assert config.timeout == 60
        assert config.mock_mode is True

    def test_validate_mock_mode(self, tmp_path: Path) -> None:
        """Test validation passes in mock mode without API key."""
        (tmp_path / "config").mkdir()

        config = Config(
            repo_path=tmp_path,
            config_dir=tmp_path / "config",
            mock_mode=True,
        )

        errors = config.validate()
        assert not errors

    def test_validate_requires_api_key(self, tmp_path: Path) -> None:
        """Test validation fails without API key in non-mock mode."""
        (tmp_path / "config").mkdir()

        config = Config(
            repo_path=tmp_path,
            config_dir=tmp_path / "config",
            mock_mode=False,
        )

        errors = config.validate()
        assert any("PERPLEXITY_API_KEY" in e for e in errors)

    def test_validate_repo_path_exists(self) -> None:
        """Test validation fails for non-existent repo path."""
        config = Config(
            repo_path=Path("/nonexistent/path"),
            mock_mode=True,
        )

        errors = config.validate()
        assert any("Repository path does not exist" in e for e in errors)

    def test_prompts_file_property(self, tmp_path: Path) -> None:
        """Test prompts_file property returns correct path."""
        config = Config(
            config_dir=tmp_path / "config",
        )

        assert config.prompts_file == tmp_path / "config" / "prompts.yaml"

    def test_profiles_file_property(self, tmp_path: Path) -> None:
        """Test profiles_file property returns correct path."""
        config = Config(
            config_dir=tmp_path / "config",
        )

        assert config.profiles_file == tmp_path / "config" / "profiles.yaml"
</file>

<file path="tests/test_prompts.py">
"""Tests for prompt and profile loading."""

from __future__ import annotations

from pathlib import Path

import pytest

from metaagent.prompts import Prompt, Profile, PromptLibrary


class TestPrompt:
    """Tests for Prompt class."""

    def test_render_basic(self) -> None:
        """Test basic template rendering with context appended."""
        prompt = Prompt(
            id="test",
            goal="Test goal",
            template="Analyze the codebase for issues.",
            stage="testing",
        )

        result = prompt.render(prd="My PRD", code_context="My code")

        # Template content should be present
        assert "Analyze the codebase for issues." in result
        # Context sections should be appended
        assert "## Product Requirements Document (PRD)" in result
        assert "My PRD" in result
        assert "## Codebase" in result
        assert "My code" in result

    def test_render_all_variables(self) -> None:
        """Test rendering with all context variables."""
        prompt = Prompt(
            id="test",
            goal="Test goal",
            template="Check this code.",
            stage="testing",
        )

        result = prompt.render(
            prd="prd_content",
            code_context="code_content",
            history="history_content",
            current_stage="stage",
        )

        # All context sections should be present
        assert "prd_content" in result
        assert "code_content" in result
        assert "history_content" in result
        assert "## Previous Analysis" in result

    def test_render_no_context(self) -> None:
        """Test rendering without context returns just template."""
        prompt = Prompt(
            id="test",
            goal="Test",
            template="Just the template.",
            stage="my_stage",
        )

        result = prompt.render()

        assert result == "Just the template."


class TestPromptLibrary:
    """Tests for PromptLibrary class."""

    def test_load_prompts(self, sample_prompts_yaml: Path, sample_profiles_yaml: Path) -> None:
        """Test loading prompts from YAML."""
        library = PromptLibrary(sample_prompts_yaml, sample_profiles_yaml)
        library.load()

        prompt = library.get_prompt("test_prompt")
        assert prompt is not None
        assert prompt.goal == "Test prompt for unit testing"

    def test_load_profiles(self, sample_prompts_yaml: Path, sample_profiles_yaml: Path) -> None:
        """Test loading profiles from YAML."""
        library = PromptLibrary(sample_prompts_yaml, sample_profiles_yaml)
        library.load()

        profile = library.get_profile("test_profile")
        assert profile is not None
        assert profile.name == "Test Profile"
        assert "test_prompt" in profile.stages

    def test_get_prompts_for_profile(
        self, sample_prompts_yaml: Path, sample_profiles_yaml: Path
    ) -> None:
        """Test getting prompts for a profile."""
        library = PromptLibrary(sample_prompts_yaml, sample_profiles_yaml)
        library.load()

        prompts = library.get_prompts_for_profile("test_profile")
        assert len(prompts) == 1
        assert prompts[0].id == "test_prompt"

    def test_list_profiles(self, sample_prompts_yaml: Path, sample_profiles_yaml: Path) -> None:
        """Test listing all profiles."""
        library = PromptLibrary(sample_prompts_yaml, sample_profiles_yaml)
        library.load()

        profiles = library.list_profiles()
        assert len(profiles) == 1
        assert profiles[0].name == "Test Profile"

    def test_missing_prompts_file_is_ok(self, tmp_path: Path) -> None:
        """Test that missing prompts file is okay (prompts can come from prompt_library)."""
        # Create a profiles file so it doesn't fail on that
        profiles_file = tmp_path / "profiles.yaml"
        profiles_file.write_text("profiles:\n  test: {name: Test, stages: []}")

        library = PromptLibrary(
            prompts_path=tmp_path / "nonexistent.yaml",
            profiles_path=profiles_file,
        )

        # Should not raise - prompts are optional now
        library.load()
        assert library.list_prompts() == []

    def test_get_nonexistent_prompt(
        self, sample_prompts_yaml: Path, sample_profiles_yaml: Path
    ) -> None:
        """Test getting a prompt that doesn't exist."""
        library = PromptLibrary(sample_prompts_yaml, sample_profiles_yaml)
        library.load()

        prompt = library.get_prompt("nonexistent")
        assert prompt is None

    def test_get_nonexistent_profile(
        self, sample_prompts_yaml: Path, sample_profiles_yaml: Path
    ) -> None:
        """Test getting a profile that doesn't exist."""
        library = PromptLibrary(sample_prompts_yaml, sample_profiles_yaml)
        library.load()

        profile = library.get_profile("nonexistent")
        assert profile is None


class TestMarkdownPromptLibrary:
    """Tests for loading prompts from markdown files."""

    def test_load_markdown_prompts(self, tmp_path: Path) -> None:
        """Test loading prompts from markdown files."""
        # Create a prompt library directory
        prompt_lib = tmp_path / "prompt_library"
        prompt_lib.mkdir()

        # Create a sample prompt file
        (prompt_lib / "quality_test_analysis.md").write_text(
            "# Test Quality Analysis\n\n"
            "**Objective:** Analyze test quality.\n\n"
            "**Instructions:**\n1. Check tests\n2. Report issues"
        )

        library = PromptLibrary(
            prompt_library_path=prompt_lib,
            profiles_path=None,
        )
        library.load()

        # Should load the prompt
        prompts = library.list_prompts()
        assert len(prompts) == 1

        prompt = library.get_prompt("quality_test_analysis")
        assert prompt is not None
        assert prompt.goal == "Test Quality Analysis"
        assert prompt.category == "quality"
        assert "Analyze test quality" in prompt.template

    def test_list_prompts_by_category(self, tmp_path: Path) -> None:
        """Test organizing prompts by category."""
        prompt_lib = tmp_path / "prompt_library"
        prompt_lib.mkdir()

        # Create prompts in different categories
        (prompt_lib / "quality_error.md").write_text("# Error Analysis\nCheck errors.")
        (prompt_lib / "quality_style.md").write_text("# Style Analysis\nCheck style.")
        (prompt_lib / "architecture_layers.md").write_text("# Layer Analysis\nCheck layers.")

        library = PromptLibrary(prompt_library_path=prompt_lib)
        library.load()

        by_category = library.list_prompts_by_category()
        assert "quality" in by_category
        assert "architecture" in by_category
        assert len(by_category["quality"]) == 2
        assert len(by_category["architecture"]) == 1
</file>

<file path=".env.example">
# API Keys (Required to enable respective provider)
ANTHROPIC_API_KEY="your_anthropic_api_key_here"       # Required: Format: sk-ant-api03-...
PERPLEXITY_API_KEY="your_perplexity_api_key_here"     # Optional: Format: pplx-...
OPENAI_API_KEY="your_openai_api_key_here"             # Optional, for OpenAI models. Format: sk-proj-...
GOOGLE_API_KEY="your_google_api_key_here"             # Optional, for Google Gemini models.
MISTRAL_API_KEY="your_mistral_key_here"               # Optional, for Mistral AI models.
XAI_API_KEY="YOUR_XAI_KEY_HERE"                       # Optional, for xAI AI models.
GROQ_API_KEY="YOUR_GROQ_KEY_HERE"                     # Optional, for Groq models.
OPENROUTER_API_KEY="YOUR_OPENROUTER_KEY_HERE"         # Optional, for OpenRouter models.
AZURE_OPENAI_API_KEY="your_azure_key_here"            # Optional, for Azure OpenAI models (requires endpoint in .taskmaster/config.json).
OLLAMA_API_KEY="your_ollama_api_key_here"             # Optional: For remote Ollama servers that require authentication.
GITHUB_API_KEY="your_github_api_key_here"             # Optional: For GitHub import/export features. Format: ghp_... or github_pat_...
</file>

<file path=".gitignore">
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
dev-debug.log

# Dependency directories
node_modules/

# Environment variables
.env

# Editor directories and files
.idea
.vscode
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# OS specific
.DS_Store

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
.venv/
venv/
ENV/

# Testing
.pytest_cache/
.coverage
htmlcov/
.tox/
.nox/

# Type checking
.mypy_cache/
.dmypy.json
dmypy.json

# Repomix output
repomix-output.md
repomix-output.xml
</file>

<file path=".mcp.json">
{
	"mcpServers": {
		"task-master-ai": {
			"type": "stdio",
			"command": "npx",
			"args": [
				"-y",
				"task-master-ai"
			],
			"env": {
				"TASK_MASTER_TOOLS": "core",
				"ANTHROPIC_API_KEY": "YOUR_ANTHROPIC_API_KEY_HERE",
				"PERPLEXITY_API_KEY": "YOUR_PERPLEXITY_API_KEY_HERE",
				"OPENAI_API_KEY": "YOUR_OPENAI_KEY_HERE",
				"GOOGLE_API_KEY": "YOUR_GOOGLE_KEY_HERE",
				"XAI_API_KEY": "YOUR_XAI_KEY_HERE",
				"OPENROUTER_API_KEY": "YOUR_OPENROUTER_KEY_HERE",
				"MISTRAL_API_KEY": "YOUR_MISTRAL_KEY_HERE",
				"AZURE_OPENAI_API_KEY": "YOUR_AZURE_KEY_HERE",
				"OLLAMA_API_KEY": "YOUR_OLLAMA_API_KEY_HERE"
			}
		}
	}
}
</file>

<file path="CLAUDE.md">
# Claude Code Instructions

## IMPORTANT: Always Use Taskmaster
**You MUST use Taskmaster for ALL task management in this project. This is MANDATORY.**

### Task Management (Always Use)
- `task-master list` - See all current tasks with status
- `task-master next` - Get the next available task to work on
- `task-master show <id>` - View detailed task information
- `task-master set-status --id=<id> --status=<status>` - Update task status (pending, in-progress, done, deferred, cancelled, blocked)

### Research & Analysis (Use for ALL Research)
- `task-master add-task --prompt="description" --research` - Add new task WITH research
- `task-master expand --id=<id> --research --force` - Break task into subtasks WITH research
- `task-master update-task --id=<id> --prompt="changes" --research` - Update task WITH research
- `task-master update --from=<id> --prompt="changes" --research` - Update multiple tasks WITH research
- `task-master analyze-complexity --research` - Analyze task complexity with AI
- **ALWAYS use `--research` flag when you need to gather information or make informed decisions**

### Progress Tracking
- `task-master update-subtask --id=<id> --prompt="notes"` - Log implementation progress and notes
- `task-master complexity-report` - View complexity analysis report

### Task Organization
- `task-master add-dependency --id=<id> --depends-on=<id>` - Add task dependencies
- `task-master move --from=<id> --to=<id>` - Reorganize task hierarchy
- `task-master validate-dependencies` - Check for dependency issues
- `task-master expand --all --research` - Expand all eligible tasks with research

### Configuration
- `task-master models --setup` - Configure AI models interactively
- `task-master models` - View current model configuration

### Key Rules
1. **NEVER manage tasks manually** - always use Taskmaster commands or MCP tools
2. **ALWAYS use `--research` flag** when doing research, analysis, or making decisions
3. **Mark tasks in-progress** before starting work
4. **Mark tasks complete immediately** after finishing
5. **Use `update-subtask`** to log implementation notes during development

## Task Master AI Instructions
**Import Task Master's development workflow commands and guidelines, treat as if import is in the main CLAUDE.md file.**
@./.taskmaster/CLAUDE.md
</file>

<file path="prd.txt">
**Product Requirements Document (PRD)**  
**Project:** Meta‑Agent for Automated Codebase Refinement  
**Owner:** You  
**Date:** 2025‑12‑14  

***

## 1. Problem Statement

Building an initial v0 from a PRD with Claude Code is now relatively fast, but turning that v0 into a robust, production‑ready MVP still requires a lot of manual review, planning, and refactoring. Human time is spent on repetitive tasks: scanning the codebase, deciding which Codebase‑Digest‑style prompts to use, interpreting results, and translating them into concrete implementation work.

The goal is to create a **meta‑agent system** that can automatically analyze a codebase, choose appropriate prompts from a prompt library (inspired by Codebase Digest), and orchestrate Perplexity + Claude/Claude Code to iteratively refine a project from “v0 that runs” into “viable MVP,” with minimal human intervention.

***

## 2. Goals & Non‑Goals

### 2.1 Goals

- **G1 – Automate post‑v0 refinement:**  
  Given a repository and its PRD, the system should automatically run analysis cycles, generate improvement plans, and invoke Claude Code to apply changes.

- **G2 – Prompt‑driven analysis engine:**  
  Maintain a configurable prompt library (similar to Codebase Digest’s) and allow the system to select and apply prompts in stages (alignment, architecture, hardening, tests).

- **G3 – Tool integration:**  
  Integrate at least these tools into a coherent pipeline:
  - Repomix (codebase packing)
  - Perplexity (analysis/planning)
  - Claude (review/summary)
  - Claude Code (implementation)

- **G4 – Project profiles:**  
  Support multiple “profiles” (e.g., backend service, automation/agent, internal tool) that define which stages and prompts to run.

- **G5 – Transparent plans and diffs:**  
  Every cycle should produce:
  - A human‑readable analysis summary.
  - A prioritized task list / implementation plan.
  - Links or references to actual code changes (diffs) created by Claude Code.

### 2.2 Non‑Goals

- Not trying to fully replace human review for production‑critical code; human sign‑off is still expected.
- Not building a generic LLM platform; this is a **developer‑centric pipeline** optimized for your own workflows.
- Not designing a UI beyond a basic CLI or minimal web dashboard in v1.

***

## 3. High‑Level User Flows

### 3.1 Flow A: PRD → v0 (baseline)

1. User writes a PRD for a new project and saves it as `docs/prd.md` in an empty repo.
2. User opens the repo in Claude Code.
3. User instructs Claude Code:  
   “Use `docs/prd.md` to design and implement an initial v0, with basic tests, so that the main flows run end‑to‑end.”
4. Claude Code builds the initial implementation and passes tests.
5. User commits v0 to version control.

*(This phase is manual and outside this system, but assumed as a prerequisite.)*

### 3.2 Flow B: Meta‑Agent → MVP Refinement

1. User runs the meta‑agent CLI:

   ```bash
   metaagent refine --profile automation_agent --repo /path/to/repo
   ```

2. Meta‑agent:
   - Reads `docs/prd.md`.
   - Runs Repomix on the repo to produce a packed codebase file.
   - Loads the configured profile and its stages.

3. Stage 1 – **PRD Alignment Analysis**:
   - Selects the `alignment_with_prd` prompt.
   - Calls Perplexity with:
     - PRD
     - Repomix output
     - Prompt template
   - Receives:
     - Summary of gaps vs PRD.
     - Task list to close those gaps.

4. Stage 2 – **Architecture / Best Practices**:
   - Selects `architecture_sanity` and/or `best_practices_analysis` prompts (Digest‑style).
   - Calls Perplexity again.
   - Receives: architecture issues, refactor suggestions, and tasks.

5. Stage 3 – **Feature‑specific Hardening**:
   - For example, `core_flow_hardening` (retry logic, error handling).
   - Calls Perplexity with the packed code + PRD + current history.
   - Receives: detailed implementation plan for robustness.

6. Stage 4 – **Test Suite MVP**:
   - Runs a testing prompt (e.g., `test_suite_mvp`).
   - Receives: list of missing tests (file names, test cases).

7. Meta‑agent merges all tasks into a single `mvp_improvement_plan.md`.

8. User opens Claude Code on the repo and feeds in `mvp_improvement_plan.md` with instructions to execute tasks in order, showing diffs and running tests.

9. After implementation:
   - Meta‑agent can re‑run Repomix and a shorter analysis pass to confirm improvements and suggest final tweaks.

***

## 4. Functional Requirements

### 4.1 CLI / Orchestrator

- **FR1:** Provide a CLI command like `metaagent refine --profile <profile> --repo <path>` that starts a refinement run.
- **FR2:** Detect and load:
  - PRD file (default `docs/prd.md`).
  - Prompt library configuration (YAML/JSON).
  - Profile configuration (mapping stages → prompts).

- **FR3:** Run Repomix on the repo to produce a packed code representation in Markdown or XML.

- **FR4:** Maintain a simple “history log” for each run (high‑level summaries of each analysis cycle).

### 4.2 Prompt Library & Profiles

- **FR5:** Store prompt templates and metadata in configuration (e.g., `config/prompts.yaml`), including:
  - `id`
  - `goal`
  - `template`
  - `stage` or `category`
  - Optional `dependencies` or `when_to_use` hints.

- **FR6:** Store profiles in `config/profiles.yaml`, mapping:
  - Profile name → ordered list of stages (e.g., `alignment_with_prd`, `architecture_sanity`, `core_flow_hardening`, `test_suite_mvp`).

- **FR7:** Allow selection of prompts per stage based on profile and current run state (e.g., fixed order for v1).

### 4.3 Analysis Engine (Perplexity)

- **FR8:** For each stage in the profile, construct a Perplexity prompt that includes:
  - PRD text.
  - Truncated Repomix output (to fit context budget).
  - Run history summary (if any).
  - The stage’s prompt template.

- **FR9:** Expect structured responses from Perplexity, including at least:
  - `summary` (what was found).
  - `recommendations`.
  - `tasks` (list of actionable items with file references where possible).

- **FR10:** Aggregate `tasks` from all stages into an ordered improvement plan (e.g., group by priority and file).

### 4.4 Plan & Handoff to Claude / Claude Code

- **FR11:** Write the aggregated plan to `docs/mvp_improvement_plan.md`, including:
  - Short recap of PRD.
  - Stage summaries.
  - Prioritized task list with checkboxes.

- **FR12:** Provide a standard instruction block you can paste into Claude Code, telling it how to interpret and execute the plan (taskmaster style).

- **FR13:** Optionally, provide a separate prompt for Claude (non‑code) to turn the plan + analysis into a polished review document if desired.

### 4.5 Iteration / Re‑analysis

- **FR14:** Support re‑running the refinement after code changes:
  - Regenerate Repomix.
  - Optionally skip certain stages and focus on tests or architecture only.
- **FR15:** Track whether “Must‑fix” tasks (e.g., PRD coverage) are resolved based on subsequent analyses.

***

## 5. Non‑Functional Requirements

- **NFR1:** Implementation language: Python 3.10+.
- **NFR2:** All orchestration should run via CLI; no GUI required for v1.
- **NFR3:** Configurable timeouts and max token sizes for LLM calls.
- **NFR4:** Keep secrets (API keys) in environment variables or a separate secrets file, not in repo.
- **NFR5:** Make it easy to extend the prompt library and profiles without changing Python code.

***

## 6. Integrations & Dependencies

- **Repomix:** Installed CLI accessible on PATH; used to pack codebase.[1][2]
- **Perplexity API:** Used for analysis stages and plan generation.
- **Claude (chat) API:** Optional, for generating narrative reviews.
- **Claude Code:** Used via its IDE / environment for executing the plan (file edits + tests).
- **Optional:** Codebase‑Digest could be added later as an alternative or complement to Repomix if you want its directory tree and stats.

***

## 7. Milestones

1. **M1 – Minimal orchestrator**  
   - CLI command.  
   - Repomix integration.  
   - Single profile with 2 stages: `alignment_with_prd`, `core_flow_hardening`.  
   - Generates a basic `mvp_improvement_plan.md`.

2. **M2 – Full profile + prompt library**  
   - Add `architecture_sanity` and `test_suite_mvp` stages.  
   - Config‑driven prompts and profiles.  
   - Basic run history logging.

3. **M3 – Iteration support**  
   - Ability to re‑run refinement after changes and detect remaining gaps.  
   - Simple rule‑based logic for skipping or repeating stages.

4. **M4 – Optional review generation**  
   - Claude integration to create polished review docs from the plan and analysis logs.

If you want, the next step can be turning this PRD into a skeleton repo layout (e.g., `metaagent/cli.py`, `metaagent/prompts.py`, `metaagent/analysis.py`, `config/prompts.yaml`, `config/profiles.yaml`).

[1](https://github.com/yamadashy/repomix)
[2](https://repomix.com/guide/usage)
</file>

<file path="pyproject.toml">
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "metaagent"
version = "0.1.0"
description = "Meta-agent for automated codebase refinement from v0 to MVP"
readme = "README.md"
license = "MIT"
requires-python = ">=3.10"
authors = [
    { name = "Developer" }
]
keywords = ["cli", "automation", "code-analysis", "llm", "refactoring"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Environment :: Console",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]

dependencies = [
    "typer>=0.9.0",
    "rich>=13.0.0",
    "pyyaml>=6.0",
    "httpx>=0.25.0",
    "python-dotenv>=1.0.0",
    "jinja2>=3.1.0",
    "codebase-digest>=0.1.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "pytest-asyncio>=0.21.0",
    "ruff>=0.1.0",
    "mypy>=1.5.0",
]

[project.scripts]
metaagent = "metaagent.cli:main"

[tool.hatch.build.targets.wheel]
packages = ["src/metaagent"]

[tool.hatch.build.targets.sdist]
include = [
    "/src",
    "/config",
]

[tool.pytest.ini_options]
testpaths = ["tests"]
pythonpath = ["src"]
addopts = "-v --tb=short"

[tool.ruff]
line-length = 100
target-version = "py310"

[tool.ruff.lint]
select = ["E", "F", "I", "N", "W"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_ignores = true
</file>

<file path="README.md">
# Meta-Agent

A Python CLI tool for automated codebase refinement from v0 to MVP.

Meta-agent analyzes your codebase against its PRD (Product Requirements Document) using LLM-powered analysis and generates a prioritized improvement plan that can be executed by Claude Code or similar AI coding assistants.

## Features

- **PRD Alignment Analysis**: Identifies gaps between your implementation and requirements
- **Architecture Review**: Checks code organization and best practices
- **Robustness Hardening**: Finds opportunities to improve error handling and edge cases
- **Test Coverage Analysis**: Identifies missing tests for MVP quality
- **Multiple Profiles**: Different analysis profiles for different project types
- **Mock Mode**: Test the workflow without API calls

## Installation

```bash
# Clone the repository
git clone <repo-url>
cd meta-agent

# Install in development mode
pip install -e ".[dev]"

# Or with uv
uv pip install -e ".[dev]"
```

## Configuration

### Environment Variables

Create a `.env` file in your project root:

```bash
# Required for real analysis (not needed in mock mode)
PERPLEXITY_API_KEY=your_perplexity_api_key

# Optional
ANTHROPIC_API_KEY=your_anthropic_api_key
METAAGENT_TIMEOUT=120
METAAGENT_MAX_TOKENS=100000
METAAGENT_LOG_LEVEL=INFO
```

### Prerequisites

- **Python 3.10+**
- **Repomix** (for codebase packing): `npm install -g repomix`
- **codebase-digest** (for directory tree & metrics): `pip install codebase-digest` (installed automatically)
- **Perplexity API key** (for analysis, or use `--mock` mode)

### Codebase Analysis Tools

Meta-agent uses two complementary tools for comprehensive codebase analysis:

| Tool | Purpose | Output |
|------|---------|--------|
| **codebase-digest** | Directory structure & metrics | Tree view, file counts, token estimates |
| **Repomix** | Full file contents | All source code packed into one file |

Both tools run automatically during refinement, providing the LLM with both high-level structure and detailed code content.

## Usage

### Basic Usage

```bash
# Run refinement analysis on current directory
metaagent refine --profile automation_agent

# Run on a specific repository
metaagent refine --profile automation_agent --repo /path/to/repo

# Run in mock mode (no API calls)
metaagent refine --profile automation_agent --mock
```

### Available Commands

```bash
# Show version
metaagent --version

# Show help
metaagent --help

# List available profiles
metaagent list-profiles

# Run refinement with verbose output
metaagent refine --profile automation_agent --verbose
```

### Available Profiles

| Profile | Description |
|---------|-------------|
| `automation_agent` | For CLI tools and automation agents |
| `backend_service` | For API backends (includes security review) |
| `internal_tool` | Lighter profile for internal tools |
| `quick_review` | Fast PRD alignment check only |
| `full_review` | Comprehensive analysis with all stages |

### Output

After running, meta-agent generates `docs/mvp_improvement_plan.md` containing:

1. **PRD Summary**: Brief recap of requirements
2. **Stage Summaries**: Results from each analysis stage
3. **Task List**: Prioritized checklist of improvements
4. **Instructions**: How to use the plan with Claude Code

## Project Structure

```
meta-agent/
├── src/metaagent/
│   ├── __init__.py
│   ├── cli.py           # CLI entrypoint
│   ├── config.py        # Configuration management
│   ├── orchestrator.py  # Main pipeline orchestration
│   ├── prompts.py       # Prompt/profile loading
│   ├── repomix.py       # Repomix integration
│   ├── analysis.py      # LLM analysis engine
│   └── plan_writer.py   # Plan file generation
├── config/
│   ├── prompts.yaml     # Prompt templates
│   └── profiles.yaml    # Profile definitions
├── tests/
│   ├── conftest.py      # Test fixtures
│   ├── test_*.py        # Test modules
└── docs/
    └── prd.md           # Project PRD
```

## Adding Custom Prompts

Add new prompts to `config/prompts.yaml`:

```yaml
prompts:
  my_custom_prompt:
    id: my_custom_prompt
    goal: "Description of what this prompt analyzes"
    stage: custom
    template: |
      Your prompt template here.
      Use {{ prd }}, {{ code_context }}, {{ history }}, {{ current_stage }}.

      Respond with JSON:
      {
        "summary": "...",
        "recommendations": [...],
        "tasks": [...]
      }
```

## Adding Custom Profiles

Add new profiles to `config/profiles.yaml`:

```yaml
profiles:
  my_profile:
    name: "My Custom Profile"
    description: "Description of when to use this profile"
    stages:
      - alignment_with_prd
      - my_custom_prompt
```

## Development

```bash
# Install dev dependencies
pip install -e ".[dev]"

# Run tests
pytest

# Run tests with coverage
pytest --cov=metaagent

# Type checking
mypy src/metaagent

# Linting
ruff check src tests
```

## Workflow

1. **Build v0**: Use Claude Code to build initial implementation from PRD
2. **Run Meta-Agent**: `metaagent refine --profile automation_agent`
3. **Review Plan**: Check `docs/mvp_improvement_plan.md`
4. **Implement**: Feed the plan to Claude Code to implement improvements
5. **Iterate**: Re-run meta-agent to find remaining gaps

## License

MIT
</file>

</files>
